Main Idea:
    We need to use ranking losses instead of other methods for the purpose of HPO algorithms.


Documentation (Agenda):
    First we need to Implement/create the infrastructure to learn ranking loss functions.
    So we need to read and understand / revise the ranking losses concept.

    We decided to model a ranking function using a deep neural network.
        Reason: reprentation capacity.
        For now defined a [input_dim, (32, RELU) , (32, RELU), 1]

    Creating a toy example to build and run our ranking function
        Can our scoring function learn to sort a list of numbers? 

    Obervations after completing the implementation of toy example with list wise ranking function.
        1. We do not have any output range of values as a reference. Hence the model can learn score values arbitrarily large
        2. We must control the output domain so that the training does not lead to underflow or overflow.
            This is because we use the exp function for getting the strict positive increasing function.
            Perhaps using other increasing positive function helps? (More reading/research required on this topic)
        3. This may make the output domain restrictive. Works for now, however, we may need a better solution for this.
        4. The learnt model is extremely sensitive to the learning rate and the number of epochs

Advantages of the proposed Idea:
    The amount of data instances for training is exponential in number. Which is very good for a deep learning model
    For example if we have 100 observation set and we use a list size of 15 to train our model, we will have 100C15
    unique instances to train.


Observed disadvantages:
    The learnt model is extremely sensitive to the learning rate and the number of epochs


Applying this to HPOB... (With or without query)
    First learning from first search space.
    Even with this the loss curves are very smooth if we use 2 * tanh(0.01 * x)


Possible extensions:
    Top ranked enhanced loss can be used as it is simple to understand and implement
    Section 4 in https://arxiv.org/pdf/1707.05438.pdf
    This is because it is more important to get the correct top rankings than later rankings
    In fact the top1 rank is what is most important to us in any optimization step of an HPO algorithm.
