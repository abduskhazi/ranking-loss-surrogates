\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\title {Ranking Loss Surrogates \\[1ex] \large MSc Thesis}
% Commenting out another way to write the title.
% \title{MSc Thesis - Ranking Loss Surrogates}

\author{
        Abdus Salam Khazi [
        \href{mailto:abdus.khazi@students.uni-freiburg.de}
                {Email} ]\\ \\
        \href{https://github.com/abduskhazi/ranking-loss-surrogates.git}
                {Github Repository} \cite{github_repository} \\ \\
        Supervisors:
        \begin{tabular}{ll}
             JProf. Josif Grabocka \&
			Sebastian Pineda
		\end{tabular}
       }

\begin{document}

\maketitle
\date{}
\tableofcontents
\newpage

\begin{abstract}

Abstract goes here

\end{abstract}

\newpage

\section{Introduction}
The performance of any machine learning model is sensitive to the hyper-parameters used during the model training. 
Instead of using a new model type, it is more helpful to tune the hyper-parameters of an existing model to improve its performance.
Learning the best hyper-parameter for an ML model is called, Hyperparameter optimization (HPO in short).
This thesis studies various existing approaches to HPO and proposes a new idea for the same using the concept of ranking.
The proposed idea in this thesis is called \textbf{HPO using Ranking Loss Surrogates}. 
The results obtained using this model are compared against the state-of-the-art results obtained using models like FSBO,  RGPE,  TAF, and others. 

\subsection{Problem Definition}
To find out the best hyper-parameter for any machine learning model $m$, we must first quantify a given hyper-parameter configuration $\textbf{x}$ by a real-valued number $v \in \mathbb{R}$.
If we define that
$$
\textbf{x}_1 \succ  \textbf{x}_2 \iff v_{\textbf{x}_1} < v_{\textbf{x}_2}
$$
then HPO can be defined mathematically by an abstract function, say,  $f(\textbf{x}) \mapsto \mathbb{R}$ as
$$
     \underset{\rm \textbf{x}}{\rm argmin}  f(\textbf{x}) \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$
where $\mathbb{S}$ is the hyper-parameter search space.

This function $f(\textbf{x}) \mapsto \mathbb{R}$ is evaluated in the following chronological steps:
\begin{enumerate}
\item Using a given hyper-parameter configuration $\textbf{x}$,  we train our model $m$ to obtain the model $m^{trained}_\textbf{x}$. It consists of learning the parameters of our model, E.g. learning the weights and biases of a Deep Neural Network.
We use the training data to learn this model.
\item The validation data is passed through $m^{trained}_\textbf{x}$ to obtain the required results.
These results are evaluated based on an evaluation criterion '$\textrm{eval}$'. This criterion is different for different problems, e.g. Regression, Classification, etc.
The result of this evaluation is a real-value that gives a score for the configuration $\textbf{x}$.
\end{enumerate}

Hence the function $f(\textbf{x}) \mapsto \mathbb{R}$ can be written as 
$$
\textrm{eval}(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}
$$

Finally, the HPO problem can be defined using the following equation:
$$
\underset{\rm \textbf{x}}{\rm argmin} \;\; \textrm{eval}(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}   \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$

\label{ProblemOverviewlabel}
\subsection{Hyper-Parameter Optimization Constraints}

Hyper-parameter optimization is different from other optimization methods because it has different constraints.
It is because of the peculiar properties of the hyper-parameter search spaces.
Finding out the correct hyper-parameter setting is generally not feasible using a brute-force approach (trying out all possible combinations of hyper-parameters) because the search space itself has many dimensions, and the search space may be continous.
More specifically,  some of the important constraints of this optimization problem are:

\begin{enumerate}
\item The evaluation of a given HPO configuration is computationally expensive.
\item It is a non-convex optimization problem.
\item The process of getting $m^{trained}_\textbf{x}$ from $m$ is stochastic hence the value $v_{\textbf{x}}$ is noisy.
\item Some dimensions have conditional constraints. The values of some dimensions may depend on the values of others. For example, the number of neurons in layer 3 only makes sense if we have 3 or more layers in a neural network.
\item The search space is hybrid in terms of continuity. Some of the dimensions (variables) may be continuous, while others may be discrete.
Using a gradient method is hence not trivial.
\end{enumerate}



\section{Literature Review}
To deal with the constraints of HPO problems, researchers have used different strategies for developing HPO algorithms and models.
Some of the straightforward methods include
\begin{itemize}
\item Manual Search
\item Grid Search
\item Random Search
\end{itemize}


Manual Search in the HPO search space is feasible when we have expert knowledge of the problem and the given model. 
The idea is to select configurations step by step by observing the results obtained
so that we do not waste computation time evaluating similar configurations through intuition.
This approach may be helpful for small models with lesser constraints.
However, as the HPO search space becomes very large or conditional constraints become too complex, the selection of configurations becomes more and more difficult.
Hence a more systematic and automated approach is more practical.

Grid search is a more systematic approach in which we divide the search space into grid points similar to ones in a Euclidean graph.
Let there be $m$ dimensions in the search space $\mathbb{S}$. Let the selected number of values for each dimension be $n_1, n_2, ... n_m$. In the case of a discrete variable, the selected values will be a subset of the possible values, whereas, in the case of a continuous variable, we need to select the values based on a selected granularity.
The cross-product of the selected subsets gives us the configurations to be evaluated. Hence, the number of configurations to evaluate will be $n_1 * n_2 * ... n_m$.
The number of configurations we need to evaluate in this approach becomes a big issue for this method as the dimensions of the search spaces increase.
Hence this approach becomes intractability for large search spaces.

One issue with the Grid Search approach is that we assume that all dimensions in the HPO search space are equally important. It is not the case in many HPO problems. For example, the learning rate in deep neural networks is much more important than many other parameters. If dimension $p$ is the most important in the search space, then it makes sense to evaluate more values of $p$. Random Search helps us solve this problem. Hence Random Search can be used as a trivial baseline for comparing other HPO models.

One advantage of these methods is that there are no restrictions on the HPO search spaces. Hence, they are suitable for any HPO problem at hand.
On the other hand, these methods are non-probabilistic.
Hence they cannot deal with noisy evaluations of the HPO configuration well.
Moreover, these methods are computationally expensive. The reason is that they do not use surrogate evaluators and hence train and evaluate the whole model.
Also, these search methods give us optimal HPO configurations only by chance.

In the remainder of this section, we discuss some sophisticated probabilistic models for doing HPO that use surrogates to reduce computational costs.

\subsection{Bayesian Optimization}
Bayesian optimization tries to solve both computational costs and noisy evaluations of our objective function (section 1).
It does this by building a model of the HPO objective function. This model is called a surrogate function.
Bayesian optimization uses known evaluations as its data to build the surrogate model. The data is of the form {x, f(x)} pairs.
The surrogate model is a probabilistic model. Hence, it also learns about the noise in the evaluations of the objective function.

The core procedure of the optimization process is the following:
\begin{itemize}
\item From known data $D = {(x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3)) .... }$, build a probabilistic model that learns the mean and variance of the objective function
\item Use the surrogate to sample the next best HPO configuration x'. Evaluate f(x')
\item Append (x', f(x')) to D and repeat the process.
\end{itemize}

The above process repeats till the computational resources are finished (here time) or we find an acceptable HPO configuration.
This procedure is also called SMBO (Sequential model-based Optimization).
The procedure alternates between collecting data and fitting the model with the 
collected data. \cite{SMBOPaper}

There are two essential components of Bayesian optimization:

\begin{itemize}
\item Probabilistic model of the objective function
\item The acquisition function
\end{itemize}

There are many probabilistic models such as Random Forests, Gaussian Processes, Tree parson Estimators, etc.
But for the purpose of this thesis, we briefly mention Random forests and discuss in-depth the Gaussian processes.


\subsubsection{Random (Regression) Forest}
     The core idea of this model is to train a Random Regression Forest, using the known data as in any SMBO procedure \cite{SMBOPaper}.
Random regression forests are an ensemble of regression trees. 
This property is used to our advantage to predict the mean and the variance. 
The mean of the prediction of all the trees is the mean of the surrogate model.
The variance in the prediction of all trees is the variance of the surrogate model.

   The advantages of this model are
   \begin{itemize}
   \item It can handle both continuous and discrete variables trivially without any modifications to the model.
The data splitting during training is done using any variable be it discrete or continuous.
\item It can handle conditional variables, unlike Gaussian processes, by making sure that no variable is split till it is guaranteed that the conditionality does not break by doing the split.
   \end{itemize}


\subsubsection{Gaussian Processes}

\subsection{Deep Ensembles}
One way to do Hyper Parameter Optimization is by using 
Uncertainty prediction is the key in the paper. \cite{DeepEnsemblePaper}

\subsection{Few Shot BO: Deep Kernel Learning}


\section{Existing Methods}

\section{Dataset}
\subsection{HPO\_B Dataset}

\section{Proposed Idea: Ranking Loss Surrogates}

\subsection{Understanding Ranking Losses}



\section{Evaluation and Ablation Study}
\subsection{Testing}
\subsection{Ablation}

\section{Results}

\bibliographystyle{plain}
\bibliography{references}

\section{Appendix}
\subsection{More information}
This thesis was completed in the representation learning lab of Albert-Ludwig-Universität Freiburg.  (Figure~\ref{fig:UniLogo})

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.35]{images/logo}
    \caption{Logo: Albert-Ludwig-Universität Freiburg}
    \label{fig:UniLogo}
\end{figure}

\end{document}

