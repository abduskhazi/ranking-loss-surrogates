\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\title {Ranking Loss Surrogates \\[1ex] \large MSc Thesis}
% Commenting out another way to write the title.
% \title{MSc Thesis - Ranking Loss Surrogates}

\author{
        Abdus Salam Khazi [
        \href{mailto:abdus.khazi@students.uni-freiburg.de}
                {Email} ]\\ \\
        \href{https://github.com/abduskhazi/ranking-loss-surrogates.git}
                {Github Repository} \cite{github_repository} \\ \\
        Supervisors:
        \begin{tabular}{ll}
             JProf. Josif Grabocka \&
			Sebastian Pineda
		\end{tabular}
       }

\begin{document}

\maketitle
\date{}
\tableofcontents
\newpage

\begin{abstract}

Abstract goes here

\end{abstract}

\newpage

\section{Introduction}
The performance of any machine learning model is very sensitive to the hyper-parameters used during the training of the model. 
Many a time, instead of using a new model type, it is more helpful to tune the hyper-parameters of an existing model that performs well on a given problem (or dataset).
Learning the best hyper-parameter for an ML model is called Hyper parameter optimization (HPO in short).
This thesis studies various existing approaches to HPO and proposes a new idea for the same using the concept of ranking.
The proposed idea is called, \textbf{HPO using Ranking Loss Surrogates}. 
The results obtained using this model are benchmarked against the results of state of the art models like FSBO,  RGPE,  TAF and others. 

\subsection{Problem Definition}
To find out the best hyper-parameter for any machine learning model $m$ , we must first quantify a given hyper-parameter configuration $\textbf{x}$ by a real valued number $v \in \mathbb{R}$.  If we define that
$$
\textbf{x}_1 \succ  \textbf{x}_2 \iff v_{\textbf{x}_1} < v_{\textbf{x}_2}
$$
then HPO can be defined mathematically by an abstract function, say,  $f(\textbf{x}) \mapsto \mathbb{R}$ as
$$
     \underset{\rm \textbf{x}}{\rm argmin}  f(\textbf{x}) \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$
where $\mathbb{S}$ is the hyper-parameter search space.

This function $f(\textbf{x}) \mapsto \mathbb{R}$ is evaluated in the following chronological steps:
\begin{enumerate}
\item Using a given hyper-parameter configuration $\textbf{x}$,  we train our model $m$ to obtain the model $m^{trained}_\textbf{x}$.   This consists of learning the parameters of our model, E.g.  learning the weights and biases of a Deep Neural Network.
We use the training data to learn this model.
\item The validation data is passed through $m^{trained}_\textbf{x}$ to obtain the required results.
These results are evaluated based on a required evaluation criteria, '$\textrm{eval}$', which is different for different types of problems e.g. Regression,  Classification etc.
The result of this evaluation is a real value that quantifies a given configuration $\textbf{x}$.
\end{enumerate}

Hence the function $f(\textbf{x}) \mapsto \mathbb{R}$ can be written as 
$$
\textrm{eval}(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}
$$

Finally,  our complete problem can be defined as
$$
\underset{\rm \textbf{x}}{\rm argmin} \;\; \textrm{eval}(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}   \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$

\label{ProblemOverviewlabel}
\subsection{Hyper-Parameter Optimization Constraints}

Hyper-parameter optimisation is different from other optimisation methods because it has different constraints.
This is because of the peculiar properties of the hyper-parameter search spaces.
Finding out the correct hyper-parameter setting is generally not feasible using a brute-force approach (i.e trying out the all possible combinations of hyper-parameters) because the search space itself has a lot of dimensions. 
This makes the possible number of configurations exponentially large.
More specifically,  some of the important constraints of this optimisation problem are:

\begin{enumerate}
\item By definition the evaluation of a given HPO configuration is computationally expensive.
\item It is a non-convex optimization problem.
\item The process of getting $m^{trained}_\textbf{x}$ from $m$ is stochastic hence the value $v_{\textbf{x}}$ is noisy.
\item Some dimensions have conditional constrains i.e the values of some dimensions may depend on values of other dimensions e.g.  the number of neurons in layer 3 only makes sense if we have 3 or more layers in a neural network.
\item The search space is hybrid in terms of continuity i.e some of the dimensions (variables) may be continuous and others may be discrete.
Using a gradient method is hence not trivial.
\end{enumerate}



\section{Literature Review}
To deal with the constraints of HPO problems, researchers have used different strategies for developing HPO algorithms and models.
Some of the straightforward methods include
\begin{itemize}
\item Manual Search
\item Grid Search
\item Random Search
\end{itemize}


Manual Search in the HPO search space is feasible when we have expert knowledge of the problem and the given model. 
The idea is to select configurations step by step by observing the results obtained
so that we do not waste computation time evaluating similar configurations through intuition.
This approach may be helpful for small models with lesser constraints.
However, as the HPO search space becomes very large or conditional constraints become too complex, the selection of configurations becomes more and more difficult.
Hence a more systematic and automated approach is more practical.

Grid search is a more systematic approach in which we divide the search space into grid points similar to ones in a Euclidean graph.
Let there be $m$ dimensions in the search space $\mathbb{S}$. Let the selected number of values for each dimension be $n_1, n_2, ... n_m$. In the case of a discrete variable, the selected values will be a subset of the possible values, whereas, in the case of a continuous variable, we need to select the values based on a selected granularity.
The cross-product of the selected subsets gives us the configurations to be evaluated. Hence, the number of configurations to evaluate will be $n_1 * n_2 * ... n_m$.
The number of configurations we need to evaluate in this approach becomes a big issue for this method as the dimensions of the search spaces increase.
Hence this approach becomes intractability for large search spaces.

One issue with the Grid Search approach is that we assume that all dimensions in the HPO search space are equally important. It is not the case in many HPO problems. For example, the learning rate in deep neural networks is much more important than many other parameters. If dimension $p$ is the most important in the search space, then it makes sense to evaluate more values of $p$. Random Search helps us solve this problem. Hence Random Search can be used as a trivial baseline for comparing other HPO models.

One advantage of these methods is that there are no restrictions on the HPO search spaces. Hence, they are suitable for any HPO problem at hand.
On the other hand, these methods are non-probabilistic.
Hence they cannot deal with noisy evaluations of the HPO configuration well.
Moreover, these methods are computationally expensive. The reason is that they do not use surrogate evaluators and hence train and evaluate the whole model.
Also, these search methods give us optimal HPO configurations only by chance.

In the remainder of this section, we discuss some sophisticated probabilistic models for doing HPO that use surrogates to reduce computational costs.


\subsection{Random Search}

\subsection{Gaussian Processes}
\subsection{Deep Ensembles}
One way to do Hyper Parameter Optimization is by using 
Uncertainty prediction is the key in the paper. \cite{DeepEnsemblePaper}

\subsection{Few Shot BO : Deep Kernel Learning}


\section{Existing Methods}

\section{Dataset}
\subsection{HPO\_B Dataset}

\section{Proposed Idea : Ranking Loss Surrogates}

\subsection{Understanding Ranking Losses}



\section{Evaluation and Ablation Study}
\subsection{Testing}
\subsection{Ablation}

\section{Results}

\bibliographystyle{plain}
\bibliography{references}

\section{Appendix}
\subsection{More information}
This thesis was completed in the representation learning lab of Albert-Ludwig-Universität Freiburg.  (Figure~\ref{fig:UniLogo})

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.35]{images/logo}
    \caption{Logo: Albert-Ludwig-Universität Freiburg}
    \label{fig:UniLogo}
\end{figure}

\end{document}
