%\documentclass[11pt]{report}
\documentclass[12pt, twoside, ngerman]{report} 
%\documentclass[12pt,twoside]{report}
% \documentclass{article}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage{tikz}
\usetikzlibrary{positioning}
% \usetikzlibrary{graphdrawing.trees}

\usepackage{booktabs}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

% Commenting out another way to write the title.
% \title{MSc Thesis - Ranking Loss Surrogates}

\iffalse    
\author{
        Abdus Salam Khazi [
        \href{mailto:abdus.khazi@students.uni-freiburg.de}
                {Email} ]\\ \\
        \href{https://github.com/abduskhazi/ranking-loss-surrogates.git}
                {Github Repository} \cite{github_repository} \\ \\
        Supervisors:
        \begin{tabular}{ll}
             JProf. Josif Grabocka \&
			Sebastian Pineda
		\end{tabular}
       }
\fi


\begin{document}

\newgeometry{
    top=1.5in,
    bottom=1.5in,
    outer=1.75in,
    inner=1.25in,
    }

\title{
{\LARGE Master's Thesis}
\vspace{20pt}
\hrule
\vspace{20pt}
\textbf{Hyperparameter Optimization using Ranking Loss Surrogates}
\vspace{20pt}
\hrule
\vspace{20pt}
\author{{\LARGE Abdus Salam Khazi}}
\date{\LARGE \today}
\begin{figure}[h]
    \centering
    \includegraphics[width=50mm]{images/Logo.png}
\end{figure}
\large{Albert-Ludwigs-University Freiburg
\\Representation Learning Department}
}
    
\pagenumbering{gobble}

\maketitle
    

% \date{}

\chapter*{Declaration \thispagestyle{empty}}

I hereby declare, that I am the sole author and composer of my Thesis and that no
other sources or learning aids, other than those listed, have been used. Furthermore,
I declare that I have acknowledged the work of others by providing detailed
references of said work.

I hereby also declare, that my Thesis has not been prepared for another examination
or assignment, either wholly or excerpts thereof.

\vspace{100pt}

\begin{minipage}{2in}
\textbf{\underline{\hspace{100pt}}} \\
Place, Date
\end{minipage}
\hfill
\begin{minipage}{1.3in}
\textbf{\underline{\hspace{100pt}}} \\
Signature
\end{minipage}

\newpage
\pagenumbering{roman}
\begin{abstract}

Hyperparameter Optimization (HPO) is vital in training any Machine Learning (ML) model. The hyperparameter configuration obtained from the optimization directly impacts the ML model's performance. Hence various HPO methods have been proposed in the past decade. Among these methods, Sequential Model-Based Optimization (SMBO) methods have successfully predicted suitable configurations at a relatively lesser computational cost. Moreover, using transfer learning has furthered their performance. However, the learning and design of surrogates used in SMBO is still an active area of research. This thesis proposes a different perspective on the learning and usage of surrogates in the SMBO algorithm. It formulates the selection of configurations to evaluate in the SMBO algorithm as a ranking problem. The surrogates are learned using ranking losses and work in the ranking space rather than the output space of the hyperparameter objective function. This thesis shows that using transfer learning surrogates in ranking space is more advantageous in a problem domain like HPO, where metadata is very scarce. The surrogate learning using listwise ranking losses gave the best performance among all ranking losses.
Moreover, the thesis proposes incorporating a weighting strategy that is position-dependent in the listwise ranking loss to improve its performance. It also proposes making this surrogate context-aware by integrating Deep Sets into its architecture. It concludes that using a surrogate with Deep Sets learned with a weighted listwise ranking loss gave HPO results on par with state-of-the-art transfer hyperparameter methods on the HPO-B benchmark.


\end{abstract}

\newpage

\tableofcontents
\newpage
\newpage

\pagenumbering{arabic}

\chapter{Introduction}

Machine learning has become a ubiquitous tool in the modern economy. 
Due to the availability of extensive data at a meager cost, applying machine learning methods has become very accessible.
Computer vision, marketing, customer service, and cybersecurity are some of the many domains that use machine learning. A critical factor in its success is that it gives a general framework for learning the models. One only needs to use the data and apply the formulated optimization to train the models. In other words, complex models are created using this framework without needing to program them explicitly.

Even though most machine learning models can work out of the box, their performance does not come for free. Their performance is sensitive to the hyperparameters used during the model training. Some examples of hyperparameters are the learning rate, the type of optimizer, the architecture of the neural networks, and the regularization parameter.
One needs to tune the model's hyperparameters to get the most performance. Tuning the hyperparameters to get the best performing machine learning model is called Hyperparameter Optimization (HPO in short).

A brute force method for an HPO would be to try out all possible configurations. This method is intractable because the space of hyperparameter configurations may be continuous. Even if the space is discrete, the number of combinations may be prohibitively large. Moreover, evaluating even one hyperparameter configuration may take many days. This task of evaluating an HP configuration is called the HPO objective function. Considering the cost of the HPO objective function, more sophisticated methods have been developed over the past decades. Sequential Model-Based Optimization (SMBO)~\cite{NIPS2011_86e8f7ab} is one method that has been used successfully for HPO. This method uses cheap approximations of the actual HPO's objective function to predict good HP configurations. These cheap approximations are called surrogates which need to be learned using the available hyperparameters configurations and their evaluations. This thesis proposes using the concept of ranking to learn the surrogates in SMBO. It proves that using this ranking surrogate in SMBO is a viable alternative to other methods.
Hence, the proposed idea in this thesis is called \textbf{Hyperparamter Optimization using Ranking Loss Surrogates}.


\section{Objective}
\label{sec:HPODefinition}

To define the objective and scope of this thesis, let us first understand how one does HPO.
To find the best hyperparameter for any machine learning model $m$, we quantify a hyperparameter configuration $\textbf{x}$ by a real-valued number $v \in \mathbb{R}$. If we define that
$$
\textbf{x}_1 \succ  \textbf{x}_2 \iff v_{\textbf{x}_1} < v_{\textbf{x}_2}
$$
then HPO can be defined mathematically by an abstract function, say,  $f(\textbf{x}) \mapsto \mathbb{R}$ as
$$
     \underset{\rm \textbf{x}}{\rm argmin}  f(\textbf{x}) \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$
where $\mathbb{S}$ is the space containing the set of all hyperparameters, also called hyperparameter search space.
The function $f$ is evaluated in the following chronological steps:
\begin{enumerate}
\item Using a given hyperparameter configuration $\textbf{x}$,  we train our model $m$ to obtain the model $m^{trained}_\textbf{x}$. It consists of learning the parameters of our model, for example, learning the weights and biases of a Deep Neural Network. We use any given training data to learn this model.
\item Validation split of the data is passed through $m^{trained}_\textbf{x}$ to obtain the validation prediction results. These results are evaluated based on an evaluation criterion '$\textrm{eval}$'. The result of this evaluation is a real-value that gives a score for the configuration $\textbf{x}$.
\end{enumerate}

The function $f$ can hence be written as 
$$
f(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}
$$

Finally, the HPO problem can be defined using the following equation:
\begin{equation}
\label{eq:hpoobjectivefunc}
\underset{\rm \textbf{x}}{\rm argmin} \;\; f(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}   \;\;\;  \forall \textbf{x} \in \mathbb{S}
\end{equation}


\iffalse
\subsubsection{HPO Constraints}
\fi
\label{sec:hpoConstraints}

One way to view this objective function non-mathematically is that we are trying to select a hyper parameter setting of the given model to obtain the best (lowest) validation error~\cite{fsbopaper}.  Apart from being computationally expensive,  HPO has the following constraints:

\begin{itemize}
\item It is a non-convex optimization problem.
\item The process of getting $m^{trained}_\textbf{x}$ from $m$ is stochastic hence the value $v_{\textbf{x}}$ is noisy.
\item Some dimensions have conditional constraints. The values of some dimensions may depend on the values of others. For example, the number of neurons in layer three only makes sense if we have three or more layers in a neural network.
\item The search space is hybrid in terms of continuity. Some dimensions (variables) may be continuous, while others may be discrete.
Using a gradient method is hence not trivial.
\end{itemize}

This thesis aims to improve the HPO by learning good surrogates of the HPO objective function. It uses the concept of ranking to select the best HP configuration in the SMBO algorithm. Since the surrogate should behave like a ranking, ranking losses are used to learn the surrogate of the function $f$. The thesis only targets the discrete HPO problem. This is because ranking requires sets with discrete objects. In order to improve the performance of the learned surrogate, the thesis also tries to make it context-aware.

\label{ProblemOverviewlabel}

\section{Overview}

This report is divided into six chapters. The introductory chapter (i.e., this chapter) first introduces the concept of Hyperparameter optimization - this thesis's area of research. Then it describes the objective or primary aim of the thesis. In chapter 2, we discuss the basic concepts related to HPO optimization. We first discuss different types of HPO. We then discuss the Bayesian optimization and its subcomponents in detail, as understanding these concepts are a prerequisite to understanding the research presented in this report. We also talk about designing a context-aware surrogate. In chapter 3, we dive deep into the concept of ranking and ranking losses. We discuss in detail the mathematical workings behind the loss functions. We also discuss the weighting extension of the loss functions to make it better for the HPO problem. In addition, we describe the primary baselines this thesis implements. Chapter 4 describes how the concepts of ranking and Bayesian Optimization are integrated to build the proposed ranking loss surrogate. We also talk about designing our surrogate to make it context-aware. In chapter 5, we do an ablation study of the implemented baselines and the proposed model's various components. Finally, in chapter 6, we conclude the report by describing some advantages and limitations. We also propose a few exciting future research topics.


\chapter{Related Work}\label{chap:relatedWork}

This chapter discusses the concepts already found in previous literature relevant to the HPO problem. It can be divided into three parts.
In the first part, we describe different classifications of HPO solutions. These include Blackbox HPO, Online HPO, Multi-fidelity HPO, and Transfer Learning HPO. Classifying the HPO solutions based on these criteria helps us quickly identify their advantages and drawbacks. However, it is essential to note that these classifications are not exclusive to one another. 
The second part is dedicated to Bayesian Optimization, a method that can be used for the HPO problem. We discuss in detail Bayesian optimization as this is what we use in the proposed model. We also discuss the various components of Bayesian Optimization, namely surrogates, acquisition functions, and losses.
In the final part of this chapter, we discuss how to use sets to inject context into an ML model.


\section{Black-box HPO}
In a Black-Box HPO solution, the objective function $f$ in equation~\ref{eq:hpoobjectivefunc}
is treated as a blackbox.
The problem is generalized to finding a global optimum of $f$.
Some straightforward black-box HPO methods include Manual Search,  Grid Search, and Random Search.

Manual Search in the HPO search space is feasible when we have expert knowledge of the problem and the given model. 
The idea is to select configurations step by step by observing the results obtained so we do not waste computation time evaluating similar configurations through intuition.
This approach may be helpful for small models with lesser constraints.
However, as the HPO search space becomes very large or conditional constraints become too complex, the selection of configurations becomes more and more difficult.
Hence a more systematic and automated approach is more practical.

Grid search is a more systematic approach to dividing the search space into grid points similar to those in a Euclidean graph.
Let there be $m$ dimensions in the search space $\mathbb{S}$. Let the selected number of values for each dimension be $n_1, n_2, ... n_m$. In the case of a discrete variable, the selected values will be a subset of the possible values, whereas, in the case of a continuous variable, we need to select the values based on a selected granularity.
The cross-product of the selected subsets gives us the configurations to be evaluated. Hence, the number of configurations to evaluate will be $n_1 * n_2 * ... n_m$.
The number of configurations we need to evaluate in this approach becomes a big issue for this method as the dimensions of the search spaces increase.
Hence this approach becomes intractability for large search spaces.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.8]{images/rsgsexample}
    \caption{Illustrates Grid search and Random search in the case where 2 parameters are not equally important.  Adpated from~\cite{rshpoarticle}.}
    \label{fig:rshpofig}
\end{figure}

One issue with the Grid Search approach is that we assume that all dimensions in the HPO search space are equally important. It is not the case in many HPO problems. The Grid layout in Figure~\ref{fig:rshpofig}(left) illustrates this. For example, the learning rate in deep neural networks is much more important than many other parameters. If dimension $p$ is the most important in the search space, then it makes sense to evaluate more values of $p$. Random Search helps us solve this problem. 
The Random layout in Figure~\ref{fig:rshpofig}(right) illustrates this. 
Hence Random Search can be used as a trivial baseline for comparing HPO solutions.

One advantage of these methods is that there are no restrictions on the HPO search spaces. Hence, they are suitable for any HPO problem at hand.
On the other hand, these methods are non-probabilistic.
Hence they cannot deal with noisy evaluations of the HPO configurations well.
Moreover, these methods are computationally expensive. The reason is that they do not use surrogate evaluators and train and evaluate the whole model.
Also, these search methods give us optimal HPO configurations only by chance.

\section{Online HPO}
The idea of an online HPO approach is that it tries to evaluate and update the HP configuration during the ML model's training. That is to say, the model's parameters and the hyper-parameters are updated jointly.
If the hyper-parameters and the model parameters are learned disjointly in an HPO solution, such a solution is called an offline HPO.

Online HPO can be used to learn a single hyperparameter or all the hyperparameters. 
For example,  Baydin et al.~\cite{onlineLearningRateUpdate} propose the online learning of only the learning rate. They target this hyper parameter because it is the single most important one. On the other hand, Luketina et al. ~\cite{gradientbasedHPOtuning} propose the interleaved updating of all the hyperparameters with the ML model's parameters.

An example of offline HPO is the solution proposed by Franceschi et al.~\cite{HPOAsBilevelOptimization}. They formulate the whole HPO problem as a bi-level optimization problem~\cite{hutterneuripstutorial}.
A similar approach is used by Maclaurin et al.~\cite{hypergradient}. They use a relatively cheap method to obtain hyper gradients, i.e., gradients of hyper parameters for the whole objective function $f$. They use these gradients to update the hyperparameters, similar to any gradient-based optimization method. Notice that we must completely optimize the ML parameters in both these cases to update the hyperparameters once. This property of disjoint updating of the parameters and hyperparameters makes these approaches offline.


\section{Multi-fidelity HPO}
If we treat our HPO objective function $f$ as a black box function, we would need to evaluate equation~\ref{eq:hpoobjectivefunc} fully.
This evaluation is prohibitively expensive as evaluating a single HP configuration may take days~\cite{hutter2019automated}.
Multi-fidelity hyperparameter optimization tries to solve this problem by evaluating the HP candidates on cheap functions $f_{approx}$ that approximate the objective function $f$.

Here, $f_{approx}$  is called fidelity as it reproduces the actual objective function to some degree.
For example, a fidelity could be an evaluation of the HP configuration on only a subset of data, the evaluation of the HP on downscaled image data, or learning only for a few epochs.
The idea is to trade-off between the performance of the approximate function and its optimization accuracy such that we get the best selection of HP using the least compute power.
Since we do not do the actual evaluation of HP configuration using $f$, it is an approximate optimization technique.

The technique does not belong to the black box HPO domain.
It can be understood if we consider the "few epochs" fidelity.
The optimization algorithm looks into the training process and learns the objective function to exit it earlier.
It thus gets feedback from within the "black box" of the HPO objective function.

This technique is named multi-fidelity because it may use different fidelities within the optimization process to get the best HP configuration.
An example of the multi-fidelity technique is successive halving, described by Jamieson et al.~\cite{successivehalving}. While using successive halving, one can start with a given "budget fidelity," for example - a defined number of epochs or a defined amount of training time.
The budget doubles at each step of the optimization process, and we eliminate the 50\% of the worst-performing HP configurations for each step.
Hence it uses "multiple" fidelities during the optimization process.


\section{Transfer-learning for HPO}

The HPO types discussed so far do every HP optimization from scratch. These types have no prior knowledge before starting any HP optimization.
In addition to being computationally inefficient,  it is contrary to how humans learn.
Humans use prior experience that they have accumulated and condition their actions on this knowledge.
Any machine learning method that uses this concept of conditioning its output on previously seen data is called a transfer learning method~\cite{Weiss2016}.

The concept of transfer learning can be utilized to accelerate HPO.
For example,  Gomes et al.~\cite{svmhpmetalearnt} propose to meta-learn a set of exemplary HP configurations for the SVM.
They propose that if we learn HP configurations that previously worked well, there is a high chance that a new SVM model also works well with these parameters.
Reif et al.~\cite{metalearningwarmstartpaper} also proposed a similar concept for the HP optimization of genetic algorithms.
In this thesis, we compare the proposed model's performance with the performance of state-of-the-art transfer HPO methods like Two-Stage Transfer Surrogate (TST)~\cite{tstpaper}, Ranking-Weighted Gaussian Process Ensemble (RGPE)~\cite{Feurer2018ScalableMF}, Transfer Acquisition Function Framework (TAF)~\cite{Wistuba2017ScalableGP}, and Few Shot Bayesian Optimization (FSBO)~\cite{fsbopaper}.
The TST method comprises two stages. In the first stage, multiple surrogates are learned for each metadata (or task). The outputs of the learned surrogates are combined in the second stage to rank the HP configurations. These two stages are used with the SMBO algorithm to do HP optimizations. We have discussed Bayesian Optimizations and SMBOs in detail in section~\ref{sec:BayesianOptimization}.
RGPE is very similar to TST because it also uses multiple models (Gaussian Processes in this case) learned for each task. One significant difference  between the two is that TST uses Nadaraya-Watson kernel-weighting to combine the surrogate outputs, whereas RGPE uses a linear combination of Gaussians instead. Wistuba et al. use a different method to do transfer HPO in their proposed model called TAF. They use the knowledge transfer mechanism during the acquisition phase of the SMBO process instead of using surrogates to transfer knowledge.
FSBO is a transfer HPO solution that gives a state-of-the-art performance.  Hence, we discuss this in detail in the following chapters.

On the other hand, any method that does not have prior knowledge before starting an HP optimization is called a non-transfer HPO method.
We also compare our proposed method with some state-of-the-art non-transfer techniques in this thesis. These include Gaussian Processes (GP), Deep Networks for Global Optimization (DNGO)~\cite{dngopaper}, Bayesian Optimization with Hamiltonian Monte Carlo Artificial Neural Networks (BOHAMIANN)~\cite{bohamiannpaper}, and Deep Kernel GP (DGP)~\cite{fsbopaper}. GPs are standard models used in Bayesian Optimizations for the HPO problem. In addition to being used in a silo, they are also used as sub-components in many different models. We have discussed the working of GPs in detail in section~\ref{sec:GaussianProcesses}. In DNGO, Snoek et al. propose using neural networks instead of GPs as surrogates in the Bayesian optimization. In BOHAMIANN, Springenberg et al. propose using neural networks as surrogates like in the case of DNGO. However, they employ the Bayesian Neural Network Regression Concept for doing the Bayesian optimization.
DGP is a non-transfer version of FSBO where the Deep kernel surrogate is not meta-learned but directly used in the HP optimization.
No meta-training or knowledge transfer is done in any of these methods. Hence they are classified as non-transfer HPO methods.

There are two broad ways to transfer knowledge for HP optimization: doing warm starting of initial configurations or learning surrogates.

\subsection{Warm starting}

Before running any HPO method, typically, experts study the data used to train the given model.
They suggest a few initial HP configurations that have worked well for similar datasets, according to their experience.
These initial configurations are evaluated for the given model, and the evaluated values act as a starting point (or hinge) for the HPO method.
The idea of warm starting is to automate the selection of the initial configurations before the HPO optimization runs.

Warm starting was used by Gomes et al.~\cite{svmhpmetalearnt}, and Reif et al.~\cite{metalearningwarmstartpaper}. In their respective papers, the authors used the meta-learned HP configurations as starting points for finding the optima in the HP response surface.
This idea was also proposed for the SMBO optimization algorithm by Feurer et al.~\cite{Feurer2014UsingMT}


\subsection{Meta-learning of surrogates}

The idea of meta-learning a surrogate is to meta-train it using meta-data from previously optimized similar tasks.
After its training, the surrogate can be used in an algorithm like SMBO to predict good-performing HP configurations.
This surrogate's prediction of good-performing HP configurations is conditioned on the training meta-data. This pre-conditioning makes it a transfer HPO.
The methods proposed by Schilling et al.~\cite{Schilling2016ScalableHO} and by Feurer et al.~\cite{Feurer2018ScalableMF} are a couple of examples among many others that use meta-learning of surrogates to do knowledge transfer.

The meta-learned surrogate can be used in two ways during the HPO optimization procedure.
One could use it without modification during the HPO.
On the other hand, the surrogate can be further meta-trained (finetuned) using the available metadata of the target task.
This thesis proposes a transfer HPO method that uses meta-learning of surrogates for knowledge transfer. We study both the finetuned and non-finetuned versions of our model.

\section{Bayesian Optimization}
\label{sec:BayesianOptimization}

Bayesian Optimization (BO) is a technique used to find an optimum of any computationally costly and noisy objective function.
It does this by building a model of the objective function that is cheap to evaluate computationally. This cheap model is called a surrogate function.
When BO is used in the HPO domain, the objective function would be the HPO objective function $f$. The BO surrogate is referred to as an HPO surrogate.

BO uses known objective function evaluations as data to build the surrogate model. The data consists of a list of pairs of the form $(x, f(x))$. In the context of HPO, $x$ refers to an HP configuration, and $f(x)$ refers to the true evaluations of the configuration.
The surrogate model used is a probabilistic model. Hence, it also learns about the noise in the evaluations of the objective function.
The core procedure of the BO in the HPO domain is the following:
\begin{enumerate}
\item From known data $D = {(x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3)) .... }$, build a probabilistic model that learns the mean and variance of the objective function
\item Use the surrogate to sample the next best HP configuration $x'$ using a function known as acquisition function. Evaluate $f(x')$.
\item Append $(x', f(x'))$ to $D$ and repeat the process.
\end{enumerate}

The above process repeats till the computational resources are finished, or we find an acceptable HP configuration.
This procedure is also called SMBO (Sequential model-based Optimization).
The procedure alternates between collecting data and fitting the surrogate model with the collected data~\cite{SMBOPaper}.
Hence, there are two essential components of Bayesian Optimization:
\begin{itemize}
\item A probabilistic surrogate model of the objective function.
\item An acquisition function.
\end{itemize}
Both these components are discussed in the following sections.


\subsection{Surrogate models for Bayesian Optimization}
\label{sec:SurrogateModelsForBO}
HPO using Sequential Model Based Optimization (SMBO)~\cite{NIPS2011_86e8f7ab} is a powerful and convenient method proposed in the literature.
However,  the performance of this method is heavily reliant on how well the surrogate models the true HPO objective function.
In this section, we discuss some of the powerful models that may be used as surrogates.

\subsubsection{Gaussian Processes}
\label{sec:GaussianProcesses}
Gaussian processes~\cite{GPTutorial} are predictive machine learning models that work well with few data points (or data pairs). 
They are inherently capable of modeling uncertainty.
Hence, they are used widely in HPO problem settings, where uncertainty estimation is essential. 
Due to the robustness of the Gaussian processes, we use it as one of the baseline HPO surrogates in our thesis.
In addition, many state-of-the-art HPO methods like FSBO, TST, DGP, and RGPE use Gaussian processes in one or more components.
In this section, we explain the Gaussian process intuitively.

In order to completely grasp the concept of Gaussian processes,  we need to understand normal (Gaussian) distributions. 
Consider a scalar random variable $X$ distributed normally around a mean $\mu$ with a variance of $\sigma^2$.
The following equation defines the probability density function (PDF) of $X$: 
$$
P_X(x) = \frac{1}{\sqrt[2]{2\pi}\sigma}\exp\left(- \frac{(x - \mu)^2}{2\sigma^2}\right)
$$
Here, $X$ represents the random variable, and $x$ represents an instance of the variable~\cite{GPTutorial}.
In this case,  the mean $\mu$,  variance $\sigma^2$, and any sample $x$ are all scalars.

If the random variable $\textbf{X}$ is a vector in $\mathbb{R}^d$ where $d \in I^{+}$,  then each component of the vector can be considered as a random variable.
In this case the mean $\boldsymbol{\mu} \in \mathbb{R}^d$ whereas the variance is a matrix $\Sigma \in R^{d \times d}$.
The variance is a matrix because the variance of any vector-valued random variable $\textbf{X}$ should contain the following two types of values:
\begin{itemize}
\item Variance of a vector component w.r.t itself. This is represented by
the $d$ diagonal values of the matrix.
\item Variance of each vector component w.r.t all other components. These variances are represented by the upper/lower triangular values of the matrix.
\end{itemize}
This matrix $\Sigma$, also known as the covariance matrix, has all values necessary to represent the variance of any vector-valued random variable.


The Probability Density Function (PDF) of a vector valued variable $\textbf{X} \in \mathbb{R}^d$ with a mean $\boldsymbol{\mu}$ and covariance matrix $\Sigma$ is given by~\cite{MITMLBook}:

$$
\mathcal{N}(\textbf{x} | \boldsymbol{\mu},  \Sigma) = 
\frac{1}{(2\pi)^{\frac{d}{2}} |\Sigma|^{\frac{d}{2}}}
\exp\left( - \frac{1}{2} (\textbf{x} - \boldsymbol{\mu})^T  \Sigma^{-1}   (\textbf{x} - \boldsymbol{\mu}) \right)
$$
This equation defines the PDF of a multivariate normal distribution.

The core idea in the Gaussian processes is that functions can be considered vectors of infinite dimensions.
Consider any function $f$ that has a domain $\mathbb{R}$.
If $f$ is considered to be a vector in $\mathbb{R}^{\infty}$,
then each point $i \in \mathbb{R}$  can be represented by a component $f_i$ of the function $f$.
Hence, a function is nothing but a sample from $\mathbb{R}^{\infty}$.

\iffalse
Unfortunately, functions sampled from $\mathbb{R}^{\infty}$ are too general and not useful by themselves.
\fi

The idea of Gaussian processes is to sample smooth functions from $\mathbb{R}^{\infty}$.
In any smooth function $f$, if any point $g$ is close to $x$ in the domain of $f$, then $f(g) \approx f(x)$.
The following equation mathematically represents this idea:
$$
\lim_{\delta x \to 0} f_{x + \delta x} \approx f^{+}_x  \;\; \textrm{and} \;\; 
\lim_{\delta x \to 0} f_{x - \delta x} \approx f^{-}_x 
$$
$$\;\; \textrm{where} \;\; \delta x > 0 \;\; \textrm{and} \;\; x, \delta x \in \mathbb{R}
$$
The above definition is nothing but the definition of a smooth function in terms of vector notation. 
Moreover, nearby components of $f$ "vary" similarly w.r.t each other.
These properties can be naturally encoded using a covariance matrix.
Hence, we obtain smooth functions if we sample them from a multivariate normal distribution with the required covariance matrix.
The Gaussian process restricts the function sample space to a multivariate normal distribution.

The similarity between 2 points in a domain is defined by a function called \textbf{kernel} in Gaussian processes.
The values in the required covariance matrix are populated using this kernel function.
The Gaussian process kernel controls the sampled function's smoothness $f$.
Formally kernel $k$ is defined as,
$$
k(\textbf{x}, \textbf{x'}) \mapsto \mathbb{R}
$$
Here, $\textbf{x}, \textbf{x'}$ belong to a domain in the most abstract sense.
For example,  when the input domain is a euclidean space,  $\textbf{x} \in \mathbb{R}^{\mathbb{I}^+}$.
Some well-known kernels are Radial Basis Function Kernel,  Matern Kernel,  and Periodic Kernel.

Finally,  a Gaussian Process specifies that any new observation $y^*$ for input $\textbf{x}^*$,  is jointly normally distributed with known observations $\textbf{y}$ such that
\begin{align}
    Pr\left( \begin{bmatrix}
           \textbf{y} \\
           y^*
         \end{bmatrix}
         \right)
         &=  \mathcal{N}\left(m(\textbf{X}), \mathbf{\Sigma}\right)
\end{align}
Here, $m(\textbf{X})$ is the mean of the vectors which is commonly taken as $\textbf{0}$.
$\mathbf{\Sigma}$ is the covariance matrix defined as
$$
\mathbf{\Sigma} = \begin{bmatrix}
           \textbf{K} & \textbf{K}_* \\
           \textbf{K}_*^T & \textbf{K}_{**}
         \end{bmatrix}
$$
  Where $\textbf{K} = k(\textbf{X}, \textbf{X})$,  $\textbf{K}_*  =  k(\textbf{X}, \textbf{x}_*)$ and $\textbf{K}_{**} = k(\textbf{x}_*,  \textbf{x}_*)$ for any given kernel $k$~\cite{GPTutorial}.

\subsubsection{Random Regression Forest}
Random regression forests are an ensemble of regression trees.
When it is used as a surrogate in the SMBO procedure~\cite{SMBOPaper},  its training is done using the known HP configurations and their evaluations.
Since this model is an ensemble, predicting the mean and variance of the surrogate is trivial. The mean of the surrogate is the mean of the predictions of all the trained trees. Similarly, the variance of the surrogate is the variance of their predictions.

The following are the two main advantages of using this model.
\begin{itemize}
\item It can handle both continuous and discrete variables trivially without any modifications to the model. This means that the HP search space need not be restricted.
\item It can handle conditional variables more efficiently as compared to the Gaussian surrogate.
\end{itemize}

Both these advantages are limitations of the Gaussian surrogates. Hence the random regression forests can be used as surrogates where the Gaussian process cannot.

\iffalse
1. The data splitting during training is done using any variable be it discrete or continuous.

2. unlike Gaussian processes, by making sure that data is not split based on a variable till it is guaranteed that the split breaks no conditionality.
\fi

\subsubsection{Deep Neural Networks}
Deep Neural Networks (DNNs) are machine learning models consisting of a network of multiple neuron layers.
In its most basic form, the DNN network has every neuron of one layer connected to every neuron of the following layer.
Hence each layer is called a fully connected layer (FC for short), and the neural network is called a fully connected deep neural network.
Figure~\ref{fig:DeepNeuralNetwork} illustrates an example of a fully connected deep neural network.
DNNs can be viewed as models that pass inputs sequentially through multiple computation functions before getting the final output.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.5]{images/DeepNeuralNetwork}
    \caption{A sample of a fully connected deep neural network~\cite{deepNeuralNetworksImage}.}
    \label{fig:DeepNeuralNetwork}
\end{figure}

Let us denote $I$ as the input to the fully connected DNN and $O$ as its output.
Let the interacting neuron layers, i.e., the input layer, the hidden layer, and the output layer, be represented by functions $f_i$, $f_h$, and $f_o$, respectively.
Then the computation of this DNN can be abstractly represented by
\begin{equation}
O = f_o(f_h(f_i(I)))
\end{equation}
And each function $f$ can be represented by the equation
\begin{equation}
f(I) = \textsc{NonLinearFunction}(\textbf{W} * I + \textbf{b})
\end{equation}
where the matrix $\textbf{W}$  holds the weights of the connections to the neurons of the current layer and $\textbf{b}$ represents some bias that is added to the outputs of the neurons.

The use of \textsc{NonLinearFunction} makes it possible for the neural network to represent non-linear functions.
The overall capacity of DNNs is very high. It can be further increased by increasing the depth of the network or by increasing the neurons at each layer.
One disadvantage of DNNs is that they cannot model uncertainty trivially.
One method to represent uncertainty in DNNs is to use an ensemble of neural networks or a specialized loss function with two outputs.
Since uncertainty modeling is essential for this thesis, it is discussed in more detail in section~\ref{sec:uncertaintyDeepEnsembles}.
In addition to fully connected DNNs, there are neural networks with sophisticated architectures like convolutional neural networks and recurrent neural networks. However, we do not use these in our thesis; hence we do not describe them here.

\subsubsection{Bayesian Neural Networks}
Bayesian Neural Networks are a particular type of neural network in which the output of any layer in the neural network is stochastic and not a point estimate like in the case of classical neural networks.
These types of neural networks are called Bayesian Neural Networks (BNN) because they are trained using Bayesian inference mechanism~\cite{BayesianNeuralNetworks}.
BNNs can be used to quantify uncertainty directly because of their stochastic property.

In BNNs, stochasticity is added either by using a stochastic activation function or by using stochastic weights and biases.
Consider the case where we model a BNN using stochastic weights and biases.
Let us represent all weights and biases by a parameter vector $\mathbf{\theta}$.
First, we define a probability distribution of $\mathbf{\theta}$ given by  $p(\theta)$.
This is the prior or a hypothesis in the Bayesian jargon.
For example, one prior could be a multivariate Gaussian distribution with a defined mean and a covariance matrix.
Given the training data $D$,  we calculate the new probability distribution to get the posterior distribution $p(\theta | D)$ using Bayes theorem.
Here $D$ is also referred to as evidence in Bayesian terms.
We do not mention the intricacies of the BNN training procedure as it is out of scope for this thesis.

Let a non-stochastic neural network be represented by $f_{\theta}$ for a definite parameter vector $\theta$. 
The uncertainty calculation in BNNs is done using the following equations
\begin{align*}
\mathbf{\theta}_i \sim p(\mathbf{\theta}| D)  \\
y_i = f_{\mathbf{\theta}_i}(x) \\
\textit{where} \quad i \in \mathbb{I}^+
\end{align*}

First, we sample a set of parameters from the posterior probability distribution $p(\theta | D)$.
Using this set of parameters, we calculate multiple values of $y$ for the given input, say $x$.
Hence we obtain multiple output values for a single input $x$.
One can use these multiple output values to calculate the output's mean and variance.
Note that if the output of the Bayesian Neural Network is a vector, we would need to calculate the covariance matrix to get the multivariate variance.

Even though BNNs are theoretically very robust, they are practically difficult to train.
Firstly,  the exact estimation of the posterior is computationally intractable.
Hence approximate methods like Monte Carlo Markov chains and variational inference methods are used to train the BNNs.
Even these approximations are computationally more expensive than the classical artificial neural network.
Moreover,  it is unclear from the beginning what prior probability distribution should be used for the stochastic parameters.
Because of these disadvantages, we do not use them as surrogates in our thesis.

\subsection{Learning the surrogates}
In section~\ref{sec:SurrogateModelsForBO} we discussed the details of various surrogate models that can be used in the Bayesian Optimization process. These surrogates must be learned using available $(x, f(x))$  data pairs regardless of using them in a transfer or non-transfer HPO method. Surrogate learning is formulated as an optimization problem in which a loss function is minimized to get the trained surrogate.
The type of loss function used will differ based on the problem formulation. For example, if we formulate the HPO problem as a regression problem in which the surrogate would learn the HPO objective function's output, we could use regression losses like Root Mean Squared Error (RMSE).

In this thesis, we study and compare the performance of different loss functions. We use the negative log-likelihood loss function to learn the Deep Ensemble surrogate~\cite{DeepEnsemblePaper}. This loss $L_{de}$ is given by:
\begin{equation}
L_{de} = -\log p(y|x)
\end{equation}
where $x$ is the input HP configuration and $y$ signifies the HPO objective function's output.
In FSBO~\cite{fsbopaper}, the loss function is similar, with the only distinction being that it uses a negative log \textbf{marginal} likelihood.
Our proposed method sees the surrogate as a ranker in the context of SMBO. Hence, we use ranking losses to learn the surrogate in our thesis. In the next section, we discuss the ranking losses in more detail.

\subsubsection{Ranking Losses}
\label{sec:ranklearning}

Ranking losses can be broadly classified into the following types~\cite{RankingLossFirstPaperRead}:
\begin{itemize}
\item  Pointwise ranking losses
\item  Pairwise ranking losses
\item  Listwise ranking losses
\end{itemize}

In pointwise ranking losses,  the loss function views the ranking problem as a problem of assigning a label to each queried data point.
Hence,  each learning instance is a single object.
For example, Li et al.~\cite{McRank} use pointwise ranking losses for their proposed model called McRank.
They formulate the ranking problem as a multilevel classification problem.
Each data point is classified independently using soft classification.
The score of an object then is its expected rank.
Similarly, Cossock et al.~\cite{subsetregressionpaper} use pointwise ranking losses for subset regression. 
\iffalse
Therefore, the complete scoring function comprises a multilevel classifier and an external expectation calculation.
\fi

In pairwise ranking losses,  the loss function's input is a pair of objects.
The loss function tries to separate the input data points as much as possible in the output space by minimizing the pairwise classification error~\cite{pairwisepreferencespaper}.
Examples of models built using pairwise loss functions are Rankboost~\cite{rankboostpaper} and RankNet~\cite{ranknetpaper}.

In listwise ranking losses,  a single learning instance is the whole input set, i.e., all the objects in the input set.
This makes an intuitive sense because we cannot rank the objects independently as an object's rank is relative to the ranks of other objects in a set.
This is the fundamental advantage of listwise ranking losses compared to the pointwise and pairwise losses.
Moreover,  it has been shown by Cao et al.~\cite{listwisebetter} that training a model with listwise losses gives us a superior model compared to training with pointwise or pairwise losses.
The 2 most prominent listwise loss functions are ListNet~\cite{listwisebetter} and ListMLE~\cite{listmlepaper}.
We discuss and analyze these loss functions in detail in chapter~\ref{chap:Background}.

\iffalse
We hence use the listwise approach to ranking.
Ranking loss could be the answer in modeling hp optima better.
\fi
 

\subsection{Acquisition functions}
Acquisition functions are used in each step of BO to "acquire" a good-performing HP configuration.
During BO, they need to balance exploiting information from the evaluated HP configurations and exploring unknown configurations in the HP search space.
The following functions are some of the most prominent acquisition functions found in the literature~\cite{GPTutorial}
\begin{itemize}
\item \textbf{Upper Confidence Bound (UCB)}: It returns the best possible hyperparameter configuration using a linear combination of the mean and the standard deviation.
\item \textbf{Probability of Improvement}: It returns the probability of getting a better hyperparameter configuration than the incumbent best configuration.
\item \textbf{Expected Improvement}: Given a Gaussian distribution at a new input point, it finds the expectation of improvement i.e ($f(x) - f_{max}$) over the part of normal that is greater than $f_{max}$.  This acquisition function is used with all the baselines in this thesis. However, it is not used with the proposed method. The reason for this decision is explained in more detail in section~\ref{sec:AcquisitionFunctionInRankingSpace}.
\end{itemize}


\section{Set Modelling with Neural Networks}

In the previous sections, we discussed various types of surrogates that can transfer knowledge in transfer HPO methods.
However, training on data from known tasks and using this trained model for unknown tasks has a conceptual shortcoming.
Humans do not pre-condition their actions on new tasks only on previous experiences.
Their pre-conditioning also includes the knowledge of the current task (however slight it may be).

To model this concept, one has to learn to represent and pre-condition knowledge from known tasks in a model.
Doing this makes the model context-aware.
If we consider an HP configuration and its evaluation ($\textbf{x}$, $y$) as a single object, then the group of all the known pairs can be represented as a set.
Here, $\textbf{x}$ is a feature vector, and $y$ is a scalar.
Now our problem is a 2 stage process that includes
\begin{itemize}
\item Representation of a set.
\item Conditioning of our model on this representation.
\end{itemize}


We use Deep Neural networks to represent a set in this thesis because it is a model with high representational capacity and is easy to train.
Deep Sets by Zaheer et al.~\cite{deepSets} and Set Transformers by Lee et al.~\cite{setTransformer} are two of the exciting researches that we found in the literature.
Since the sophisticated attention mechanisms used in Set Transformers were unnecessary,  we chose to use the simpler architecture used in Deep Sets.
The following section discusses these Deep Sets.

% \subsection{Set-transformers}
\subsection{Deep Sets}
\label{sec:DeepSets}

Machine learning models learn functions of the following format:
$$
f : \mathbb{R}^d \mapsto \mathbb{R}^k \quad d,k \in \mathbb{I}^+ \quad \textrm{For Regression}
$$
$$
f : \mathbb{R}^d \mapsto \{c_1, c_2, ... c_n\}  \quad \textrm{For Multi-Classification}
$$

The function $f$ transforms objects from an input space to an output space.
For the problem of set latent representation, the input space is a space containing Sets (i.e., each object in the space is a Set by itself).
The output space, however, remains similar to the regression case for regression tasks and the classification case for classification tasks.
Let $\mathbb{X}$ be the union of all possible sets in the inputs space.
Then the set representation problem can be defined as:
$$
g : 2^{\mathbb{X}} \mapsto R^k   \quad k \in \mathbb{I}^+ \quad \textrm{For Regression}
$$
$$
g : 2^{\mathbb{X}} \mapsto \{c_1, c_2, ... c_n\}  \quad \textrm{For Multi-Classification}
$$
Here, $2^{\mathbb{X}}$ is the power set of $\mathbb{X}$.

Two critical constraints of this problem are:
\begin{itemize}
\item Permutation-Invariance constraint: The permutation of the objects within an input set should be irrelevant for the model $g$.
\item Set cardinality invariance constraint: The cardinality of the set can be variable.
Hence our model $g$ should be invariant to the number of elements in the input set.
\end{itemize}

\iffalse
Permutation equivariance is another type of constraint that the deep set paper deals with.
In this constraint the latent space should be equivariant or symmetric to the permutation of the objects in the input set.
This creates issues for our problem because we don't care if our latent space is some symmetric mapping of the input space.
We want the whole set to be mapped definitively to a latent embedding.
\fi

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X1)                             {$\textbf{X}_1, y_1$};
\node[roundnode]      (X2)                            [below=of X1]     {$\textbf{X}_2, y_2$};
\node[roundnode]      (X3)                            [below=of X2]     {$\textbf{X}_3, y_3$};
\node[squarednode]      (phi1)                      [right=of X1]        {DNN$_{\theta}$};
\node[squarednode]      (phi2)                     [right=of X2]        {DNN$_{\theta}$};
\node[squarednode]      (phi3)                     [right=of X3]        {DNN$_{\theta}$};
\node[squarednode]      (Pool)                     [right=of phi2]     {Pool};
\node[squarednode]      (pho)                      [right=of Pool]     {DNN$_{\phi}$};
\node[roundnodey]      (LatentOutput)         [right=of pho]     {Latent Output};

%Lines
\draw[->] (X1.east) -- (phi1.west);
\draw[->] (X2.east) -- (phi2.west);
\draw[->] (X3.east) -- (phi3.west);
\draw[->] (phi1.east) -- (Pool.west)    node[midway, right] {$I_1$};
\draw[->] (phi2.east) -- (Pool.west)   node[midway, above] {$I_2$};
\draw[->] (phi3.east) -- (Pool.west)   node[midway, right] {$I_3$};
\draw[->] (Pool.east) -- (pho.west);
\draw[->] (pho.east) -- (LatentOutput.west);
\end{tikzpicture}
\caption{Skeletal of the architecture proposed in deep sets}
\label{fig:DeepSetArchitecture}
\end{figure}

Figure~\ref{fig:DeepSetArchitecture} illustrates the architecture proposed by the Zaheer et al.~\cite{deepSets}.
We use a set of only three objects in the Figure for simplicity.
The architecture independently passes all the three inputs through the deep neural network to get intermediate outputs $I_1$, $I_2$, and $I_3$.
To solve the permutation invariance constraint,  any mathematical operation that is commutative and associative is applied to the intermediate outputs.
As long as the pooling operator is permutation invariant, the proposed architecture is also permutation invariant.
Examples of such operators are sum, mean, and max.
The mean pooling operator is better in our case because it also meets the Set cardinality constraint.
Hence the pooling operation becomes:
$$
\textrm{Pool}(I_1, I_2, I_3) = \frac{ I_1 + I_2 + I_3}{3}
$$
The output of the pool operation is then passed through a different Deep Neural Network to get a latent output.

\iffalse
\section{RGPE}
Read the FSBO paper and write the summary like that or read  the RGPE paper
This section is necessary because our model performs similar to this

\section{TAG}
Read the TAG paper.
Some other model if also necessary and you mention this in your report.
\fi

% \section{Deep Kernel Learning}

\iffalse
Paper: https://arxiv.org/pdf/2101.07667.pdf

Problem Domain: Hyperparameter searching.
Keyword: Use Deep Kernel surrogates

Key points:
    1. Use transfer learning surrogates to get faster convergence
    2. Treat HPO as a few shot learning problem (In the context of transfer learning)
    3. Use kernel k(phi(x), phi(x')) where phi is a neural network that transforms x to a latent vector
    4. Learn parameters of phi(W) and kernel(theta) together based on historical meta data
    5. Finetune both W and theta based on the given task. If possible use warm start for initialization.
    6. Learn output range variance by creating augumented tasks.

Conceptual points:
    1. The deep kernel learnt does not have any task dependent parameters.
       The task dependent parameters are marginalized out.
       Hence finetuning during test time (after pretraining) should be complete.

Quick note on expected improvement.
    First the mean and variance is calculated for the target Hyperparameter Setting.
    Considering the mean and variance of a particular HP, we calculate the following:
%        Remove y_max from the set of all values of f(x) and lowerbound it by 0.
 %       Multiply the above value with the probability of the gaussian curve N(predicted_mean, predicted_variance)
        For a continous curve we need to integrate.
    Then we get the Expected Improvement for this HP setting.


Gaussian Processes:
    The output is considered to be a random variable.
    When have more than 1 data point, the output becomes a multivariate normal
        The output is a joint gaussian distribution.

Personal points (Probability vs Likelihood)
    Probability and likelihood are reverse in nature.
    Probability starts with a given set of parameters and caculates the probability of a given outcome to occur
    Likelihood starts with a given outcome, and we would like to determine the parameters.

    Marginal likelihood (Likelihood after marginalization of some parameters)
        integrates the effects of parameters that are not of your interest.

Note:
    Warm start is not essential for us. (It is not the main contribution of the paper)

Main motive of deep kernel learning:
    Learn the kernel function in the gaussian process.

How to do implementation with FSBO with HPO-B Metadataset?
There are 16 search spaces (model optmizations here)
    The train test split is already done metatraindata has the training data and metatest data has the test data.
    What about validation?? Really required?
    The search space id can be used for identifying correct search space for test and training data by using:
%        hpobhander.meta_test_data[search_space_id][dataset_id]["X"]
  %      hpobhander.meta_train_data[search_space_id][dataset_id]["X"]

1 ML model optimization
    1 search space (with a unique searchspace id)
        Dataset 1 (with unique dataset id)
        Dataset 2
        Dataset 3

Pretrain the FSBO model with the train split of the dataset. M'
Finetune it on 1 dataset and evaluate using Expected Improvement
    i.e run loop like the training with the given data.

Did not use strictGP --> Is it a problem?
\fi



\iffalse
This concept is heavily used in domains like document ranking,  web search results  (i.e information retrieval)(and other areas) to rank documents that were retrieved for a given query.
\fi

\iffalse
Main Idea:
    Given a set of objects, we need to rank the objects. (E.g. Ranking documents based on relevance to a query)
    

3 types of ranking functions:


Evalution of learnt RF = Ranking measures.
Relationship of Ranking measures and RLs is unkown.


Main AIM:
    Find relationship between Ranking Measuress and Pointwise/Listwise Ranking Losses.
    Pointsize relationship already clear
    Goal to do the same for pair wize and listwise case.

Proposed IDEA:
    Use an essential loss. (Need more reading)

Loss function understanding:
Pointwise - Try to get the label as per data
Pairwise - Try to separate the labels as much as possible. (Because of -z in all forms of phi).
           It is a classification of 2 objects with a boundary --- Hence the effort to separate things.
Listwise - The anology of this is that of 2 oppposing forces. One the rank of the object. Secondly
           the number of objects below it in the list.
           Loss ===> -rank + number of objects below it.
                If the rank is low and more number of objects are below it ==> Loss > 0 which is not desirable
                On the contrary, if the rank is high it can bear more objects below it as Loss would not be so high.
           Permutation invariance is obtained by using random valid (best case)
%     Doubt. We talk about permutations. It is only possible if #Lables << #DataPoints.
        ==> This is true as we are doing K-Layer classfication.


Ranking Losses:
    Point               Pair            List
    Subset regression   Ranking SVM     ListNET
    McRank              RankBoost       ListMLE
                        RankNet

Brief note on Pairwise approach: from the introduction of https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf
    The data is created by creating (x1, x2) -> label tuples for all possible x1, x2 in the ranked list. The label can be -1, 0, or 1
    ,for instance, if x1 is having lower, equal or higher rank to x2 respectively.

Ranking Measures
    NDCG = K level ratings
    MAP = 2 level ratings

    Read: https://faculty.cc.gatech.edu/~zha/CS8803WST/dcg.pdf
    Read: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/letor3.pdf (Page 15 for a clearer picture)
        CG = Cumulated gain (of information)
        DCG = Discounted cumulated gain (of information)
        NDCG = Normalized DCG i.e Divide each position of the DCG by Ideal DCG for the results.
               You get a list of values in [0,1] which lenght of list = number of rankings considered.
        NDCG@k = required real valued function of the ranking measure.

    Question: Only 2 level rankings necessary for our case?

Understanding listwise loss function:
    Queries       Ranking(f(Q, D))      Ground Truth scores
    q1        [d1, d2, d3 ... d10]      [y1, y2, y3 ... y10]
    q2        [d1, d2, d3 ... d15]      [y1, y2, y3 ... y15]
    q3        [d1, d2, d3 ... d7]       [y1, y2, y3 ... y7]

    D = {Set of Documents}
    Q = {Set of Queries}
    f : QxD -> R [Note: Here D is conditioned on Q]. The function is defined for 1 (query, document) pair.

    Point to note - Each feature input = a concatenation of the query vector and the document vector.

    One instance of our training data is (X, Y) where X = {Set of all documents returned by query} Y = {Set of the respective ground
    truth scores}. Basically 1 query is one instance. Hence our loss function has to take in vector of outputs from f.

    Loss = L(Xi , Yi) where Xi and Yi are refer to 1 query qi.
    Full Batch Loss = mean (L(Xi, Yi) for all elements i in the training objects)

    Here f(Q, D) itself is the scoring function that is the main model to be learnt in our framework.

    *** Complete explanation found in Section 3 and Section 4 of paper:
        *** https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf

    * ListNet
    * We need to make sure that f returns scores that are SIMILAR IN RELAVANCE/ORDER to the ground truth scores.
    * This would make the Ranking(f(Q, D)) equal to the ground truth and we would have learnt our ranking function.
    * Note that we do not need to get the exact ground truth scores. Which increases the space of acceptable functions in the
      function space F (Here f belongs to F) we are searching from 1 to INF. i.e It becomes easier to search if we only want a subspace and not the exact function.
    * RANKING is nothing but sorting the results based on their respective scores/relavance (decreasing order)
    * Since the sorting function is non differentiable, the loss function in question should not be composed of the RANKING function.
    * Moreover, leaving the sorting function makes our loss permutation independent by making use of the + permutation invariant operator
      [Check the final step for more clarity on this. Cross entropy uses + operator]
    * This leaves us with 2 lists -
        a. List of scores given by our ranking function f
        b. List of relavance scores given to us by ground truth
    * The loss function finds the distance of these 2 lists.
    * A probabilitic approach is taken so as to take into account for any uncertainities.
%    * The probability of selecting any document can be taken as score_of_document / sum(all document scores) since higher score means
      a more relavant document
    * However the score of the document can negative as well. Hence a strictly positive and increasing funciton phi is taken
%        which makes prob(d) = phi(d) / sum (phi(d_i) for i in Documentsof(QueryGiven))
    * The probability of a permutation is nothing but the probability of selecting one document after another without replacement
      (Reminder: Discrete probability calculation){Which itself becomes a probability distribution i.e sigma = 1}
    * One possible way to find the distance betwen the 2 lists a and b is to find the probabilities of all permutations for a and b
      and compare both the distriputions. 
    * Complexity of this O(n!) ==> Intractible
    * Instead take the probabilities of selecting every document first in any possible permutation. {Which also is a probability 
      distribution i.e sigma = 1}
    * The first selection probability (Top 1 probability in the research paper) for a and b are calculated separately using their
      respective scores.
    * The final Loss for the list = Cross entropy of probability distribution (a) w.r.t that of b.
    * This loss is backpropogated through the network to set the parameters.
%
    ListMLE: http://icml2008.cs.helsinki.fi/papers/167.pdf
    * Main difference is that the loss function used is different.
    * Loss function is intuitive in that they would want to raise the probability of getting the ground truth permutation.
    * They increase the probability of getting the exact (ground truth) permutation using the scores given by f.

    Both the papers use linear network model for some simplicity. But we can use the non-linearity due to available library
        implementations
    Very nice summary of both loss functions section 3.2.1 : https://arxiv.org/pdf/1707.05438.pdf 

Advantages:
    Can be used to select best n configs and then evaluated at random based on the ranks. This is not as trivial to get in other
    places. So there is an amount of parallelism that can be built it.

Best Model for training and ft giving auful resuts - Reason can be that we are selecting a very unfitted model due to too much variation in the validation losses.
Hence we are start only ft best model. Ih the hope that we will get better resutls.
only finetune best fit is not giving good results. Investigation in progress. 

\fi


\chapter{Background}
\label{chap:Background}

This chapter discusses the fundamental concepts one must grasp to understand the thesis work. 
We first talk about the concept of ranking.
Then, we take the next step to define the ranking loss functions abstractly.
After that, we discuss the concrete ranking loss functions studied in this thesis.
This ranking concept and hence ranking losses are used to learn HPO surrogates of the SMBO process in our proposed model.
One of the most fundamental advantages of ranking surrogates in the SMBO process is that the surrogate works in the ranking space rather than the HPO objective function's output space. This is because we can completely ignore the output range of the HPO objective function, which gives much freedom to the surrogates to model the objective function.

After discussing the intricacies of ranking losses, we discuss how uncertainty is modeled using Deep Neural Network Ensembles.
Lastly,  we discuss the details of the baselines - Deep Ensembles and FSBO that we studied in this thesis.

\section{Understanding Ranking}

Consider a set $\mathbb{A} = \{a_1,  a_2,  a_3, ... ,  a_n\}$ where each object $a_i \in \mathbb{D}$ for some domain $\mathbb{D}$.
Let us consider that an object with a lower rank is preferable to an object with a higher rank.
Ranking the objects of set $\mathbb{A}$ is defined as ordering all objects $a_i \in \mathbb{A}$ such that
$$
\texttt{Rank}(a_i) < \texttt{Rank}(a_j) \iff a_i  \succ a_j
$$

Here \texttt{Rank} of an object $a_i$ is analogous to its position in the ordered list.
An ML model can be used to model this task.
In the most general case, the cardinality of the set $\mathbb{A}$ is not fixed.
Practically building an ML model that takes a whole set of an unknown cardinality as input is not trivial.
Hence, to keep the model practical, the task of ranking is divided into the following components:~\cite{procedureforrankinginintro}
\begin{itemize}
\item Calculation of relevance scores of each $a_i \in \mathbb{A}$.
\item Ordering or sorting of objects based on their relevance scores. 
\end{itemize}

Doing a gradient-based optimization for an ML model that models both the above tasks is not possible.
This is because sorting is non-differentiable, and we cannot get a gradient for the sorting functionality.
Hence, to do a gradient-based optimization, we only model the calculation of relevance scores.
We represent this model by $s$.
We use the letter "s" to signify that we are calculating a "score" for an object. $s$ is defined as
$$
s : \mathbb{A} \mapsto \mathbb{R}
$$
Given a set of objects to rank, first, the relevant scores of the objects are predicted by $s$. After that, using these relevant scores,  the objects are sorted.
Hence, one can use $s$ to rank any newly given set of objects.

Learning a machine learning model requires optimizing criteria on the model's output.
This output, in our case, is the sorted list of objects.
The optimizing criteria in machine learning jargon are called a loss function.
Hence,  in our case, the loss function can be referred to as a \textit{Ranking Loss}.

\section{Understanding Ranking Loss functions}

\iffalse
\begin{table}[ht]
\centering
% spacing in table
% \ra{1.3}  % Commenting this could not get this function right
\begin{tabular}{@{}lr@{}}
  \toprule
  Epochs listsize & Accuracy(within range) Accuracy outside range \\ \midrule
  A    & 82.47 $\pm$ 3.21 \\
  B    & 78.47 $\pm$ 2.43 \\
  C    & 84.30 $\pm$ 2.35 \\
  D    & 86.81 $\pm$ 3.01 \\
  \bottomrule
\end{tabular}

    \caption[Table caption]{\textbf{Sorting accuracy.} Obtained after sorting the numbers based on the scorer's results\\}
    \label{tab:accuracy}
\end{table}
\fi

Consider data in the format shown in table~\ref{tab:dataformat} is given to us.

\begin{table} [ht]
\centering
\begin{tabular}{ | c | c | c | }
  \toprule
  Instance & Object Set & Ground Truth \\ \midrule
% \hline \hline
  1 & $\{a_1, a_2, a_3, ... , a_{10}\}$  & $\{y_1, y_2, y_3, ... , y_{10}\}$  \\
  2 & $\{a'_1, a'_2, a'_3, ... , a'_{15}\}$ & $\{y'_1, y'_2, y'_3, ... , y'_{15}\}$  \\
  3 & $\{a''_1, a''_2, a''_3, ... , a''_{7}\}$ & $\{y''_1, y''_2, y''_3, ... , y''_{7}\}$  \\
  ... & $\{...\}$ & $\{...\}$ \\
  \bottomrule
\end{tabular}
\caption{Data used to train a scoring function.}
\label {tab:dataformat}
\end{table}

Let each data point $a_i$ be a sample/element from the set $\mathbb{A}$ and let $y_i$ be their respective ground truth preferences.
Normally the ground truth scores are given as real-valued numbers. Hence we assume that $y_i \in \mathbb{R}$.
These preference scores are relative to the objects within $\mathbb{A}$.
This means that the preference scores of two objects $a_i$ and $b_i$ belonging to 2 different sets $\mathbb{A}$ and $\mathbb{B}$ cannot be compared.

The goal is to learn the function $s$ to predict the relevance score of any new object sampled from the set $\mathbb{A}$.
If we parameterize function $s$ with $\theta$, we can use typical optimization techniques like gradient descent to obtain an optimum $\theta^*$ by minimizing the loss function. 
Hence, our goal is obtaining the function $s_{\theta^*}$.

There are various types of ranking loss functions. The type of ranking loss defines its signature, i.e., what type of input it takes.
This is because the learning instance (or an input instance) to a loss function is different for different ranking types.
The output of any loss function studied in this thesis remains a scalar irrespective of its type.

We already know from Section~\ref{sec:ranklearning} that ranking losses can be either pointwise, pairwise, or listwise.
For pointwise loss functions, each learning instance is a single object.
Its format hence can be defined as:
\begin{equation}
L_{\texttt{pointwise}} : s(\mathbb{A}) \times \mathbb{Y} \mapsto \mathbb{R}
\end{equation}

Similarly,  the pairwise loss function has each instance as a pair of objects and their corresponding ground truths. Its format can hence be defined as:
\begin{equation}
L_{\texttt{pairwise}} : s_1(\mathbb{A}) \times s_2(\mathbb{A}) \times \mathbb{Y}_1 \times \mathbb{Y}_2 \mapsto \mathbb{R}
\end{equation}

One learning instance for listwise losses is a set of objects with corresponding ground truths.
If we take any positive integer $n \leq |\mathbb{A}|$,  the declaration of the listwise loss is given by
\begin{equation}
L_{\texttt{listwise}} : s_1(\mathbb{A}) \times s_2(\mathbb{A}) ...  s_{n}(\mathbb{A}) \times \mathbb{Y}_1 \times \mathbb{Y}_2 \times ...  \mathbb{Y}_{n} \mapsto \mathbb{R}
\end{equation}

In the following sections, we analyze the loss functions studied in this thesis.
As listwise loss functions are more advantageous conceptually than others, we discuss them in more detail.

\section{Pointwise Loss Function: Subset Regression}

We take the subset regression loss function proposed by Cossock et al. ~\cite{subsetregressionpaper} as a reference for analyzing pointwise loss functions.
We reused the implementation written by Pobrotyn et al. ~\cite{Pobrotyn2020ContextAwareLT} for our analysis.


The basic idea of subset regression is to learn the rank of the objects directly and not have an extra step of using the output scores to determine the rank of objects.
The subset regression loss function is similar to the RMSE loss function.
Given a batch of size $N$ containing objects $a_i$ and their corresponding ground truth values $y_i$ such that $0 \leq i \leq N$,  the loss function is of the form
\begin{equation}
L_{\texttt{SubsetRegression}} = \frac{1}{N} \sum\limits_{i=1}^{N} (s(a_i) - \texttt{level}(y_i))^2
\end{equation}

The $\texttt{level}$ is given by the rank of $y_i$ in the current batch.
For example,  consider we have 3 ground truth values $\{y_1 = 0.8, y_2 = 0.9, y_3 = 0.1\}$.
If we consider that a higher $y$ value is better and a lower rank/level is better then 
$\texttt{level}(y_1) = 2,  \texttt{level}(y_2) = 1,  \texttt{level}(y_3) = 3$.

To keep the implementation such that the score value is in the same given range as the regression value,  Pobrotyn et al. ~\cite{Pobrotyn2020ContextAwareLT} use a slightly modified version of the loss function, which is conceptually the same.
This loss function is given by
\begin{equation}
L_{\texttt{SubsetRegression}} = \frac{1}{N} \sum\limits_{i=1}^{N} (\texttt{distinct\_levels} * s(a_i) - y_i)^2
\end{equation}

Where $\texttt{distinct\_levels}$ is the total number of distinct ground truth values in the given batch.
As we can see, the loss function does not directly make the scoring
function learn the output regression value.
This property is shared across all ranking loss functions.

\section{Pairwise Loss function : RankNet}

To analyze how pairwise loss functions are helpful for the HPO problem,
we study the RankNet loss function proposed by Burges
 et al ~\cite{ranknetpaper}.
We reused the implementation written by Pobrotyn et al. ~\cite{Pobrotyn2020ContextAwareLT} for our analysis.

As we know from table~\ref{tab:dataformat} we are always given a set of values and their corresponding relevance scores to train.
Hence, we have $\{a_1, a_2, a_3..., a_n\}$ as inputs and $\{y_1, y_2, y_3..., y_n\}$ as their corresponding relevance scores.
As the $L_{\texttt{pairwise}}$ takes only pairs of values we obtain first the scores of all objects to get $\{s(a_1), s(a_2), s(a_3)..., s(a_n)\}$.
Then,  we form all possible pairs of inputs to the loss function from the given scores of objects.
For example,  after forming the pairs, one instance of the loss function is of the form $\{s(a_1), s(a_2), y_1, y_2\}$.

Let the actual probability of $a_1 \succ a_2$ is given by $P^*$,  and the predicted value of the same is given by $P$.
Then the pairwise loss is nothing but the cross entropy loss given by
\begin{equation}
L_{\texttt{RankNet}} = \texttt{C.E.Loss} = -P^*\log P - (1 - P^*)\log(1-P)
\end{equation}

Where,

\begin{equation}
  P^*(a_1 \succ a_2) =
    \begin{cases}
      1 & y_1 \geq y_2 \\
      0 & \text{otherwise}
    \end{cases}       
\end{equation}

\begin{equation}
\label{eq:pairwiseprobability}
P(a_1 \succ a_2) = \frac{e^{s(a_1) - s(a_2)}}{1 + e^{s(a_1) - s(a_2)} }
\end{equation}


The pairwise loss function only tries to classify the input objects.
This means we only need to know from relevance scores if one object is better than the other.
Hence a binary cross-entropy loss function is used instead of a general one in the implementation.
For the loss function, the order of the objects in the pair is irrelevant.
So,  instead of taking two pairs containing the same objects (but in a different order), only one pair is taken such that $s(a_1) - s(a_2) > 0$.
This makes the implementation easier.

\section{Listwise Loss function: ListMLE}

In this thesis, we used the listwise loss function called ListMLE for our analysis. To understand ListMLE in-depth, we first talk about a listwise loss function called ListNET, which is its predecessor. After this, we will define and analyze the ListMLE loss function itself.

\subsection{ListNET}
The ListNet idea was proposed by Cao et al. ~\cite{listwisebetter}. In this section, We intuitively explain the mathematical equations that define this loss function.

Our objective is to learn the scoring function $s$ such that it returns scores that are similar in relevance (or) order when compared to the ground truth scores.
Let us assume we learn the scoring function $s$ such that
$$
y_3 < y_{12} < y_1 \implies s(a_3) < s(a_{12}) < s(a_1)
$$
The ranking of objects is obtained by sorting them based on their respective scores. Here, sorting the objects based on the scores returned by $s$ would return the same ranking compared to the ranking obtained by sorting the objects using the ground truth values.
Note that we do not need to get the exact ground truth scores.
This increases the target function space, and more than one function gives the correct results.
A bigger target function space makes learning a suitable scoring function easier.
Another notable point is that the sort functionality is non-differentiable; hence it is not a part of the ranking loss function.

Our loss function needs to be constructed using the following two lists:
\begin{itemize}
\item List of scores given by scoring function $s$.
\item List of scores given to us by the ground truth.
\end{itemize}
If the loss function returns a distance between the two given lists, learning the function $s$ amounts to reducing this distance. Hence finding an optimum is to find the minimum distance between the two lists by changing the parameters of the scoring function $s$.

In ListNET, a probabilistic approach is adopted to calculate the distance between the two lists. This accounts for any uncertainties in the ground truth values.
The probability of selecting an object from the input set is given by
\begin{equation}
\label{eq:basicProbEquation}
P = \frac{s(a)}{\Sigma_i s(a_i)} \;\;\; \forall i \in \{1, 2, 3, ...,  n\}
\end{equation}
where $n$ is the list size.
Equation~\ref{eq:basicProbEquation} makes intuitive sense because the probability of selecting an object should be higher if it is more relevant and vise-versa.
Note that the score of any object given by the scoring function can also be negative.
Therefore the score is passed through a strictly positive and increasing function $\phi$.
This changes the probability to
\begin{equation}\label{eq:objselection}
P = \frac{\phi(s(a))}{\Sigma_i \phi(s(a_i))} \;\;\; \forall i \in \{1, 2, 3, ...,  n\}
\end{equation}

Equation~\ref{eq:objselection} is also referred to as top 1 probability of an object by Cao et al. ~\cite{listwisebetter}.
This is because this gives the probability of ranking the object first when we are calculating the permutation probability of the given list.

The proposed method to find the distance between 2 lists in ListNet is
\begin{enumerate}
\item Find the top 1 probabilities of each object using the scores given by the scoring function.
\item Using the ground truth values find similar top 1 probabilities.
\item Since the list of top 1 probabilities forms a probability distribution, the cross-entropy between the two top 1 probability lists is taken as the "distance" metric.
\end{enumerate}

Let $P_{s(a)}$ represent the top 1 probability of an object using the scores given by the scoring function.
Similarly,  let $P_y$ represent the top 1 probability using its ground truth value.
The cross entropy used as a loss in ListNet is given by
\begin{equation}
L(\textbf{y},  {s(\textbf{a})}) = - \Sigma_i P_{s(a_i)} \log P_{y_i}
\end{equation}
Where $1 \leq i \leq n; i \in \mathbb{I}^+$ and $\textbf{y}$ \& $s(\textbf{a})$ represent the ground truth values and the scores given by the scoring function respectively.

\iffalse

\fi

\subsection{ListMLE}\label{sec:listMLE}

The ListMLE loss stands for "List Maximum Likelihood Estimation" loss.
As in ListNet, the probability of selecting an object from the list is the same as given in equation~\ref{eq:objselection}.
However,  the final loss used in ListMLE is not cross-entropy.
Instead, it maximizes a likelihood estimation, as the name suggests.

Let $\pi$ define any permutation of a list.
The probability of a permutation is nothing but the probability of selecting objects from the list without replacement.
In our case, the permutation probability of selecting one permutation using the selection probabilities given by equation~\ref{eq:objselection}  is
\begin{equation}\label{eq:firstMLEequation}
P_{\pi} = \prod\limits_{j=1}^{k} \frac{\phi(s(\pi_j))}{ \sum\limits_{t=j}^k \phi(s(\pi_k))}
\end{equation}
where $\pi_i$ is the object at position $i$ in the permutation $\pi$.

Applying log to the above equation gives us
\begin{equation}
\log P_{\pi} = \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi_j))}{ \sum\limits_{t=j}^k \phi(s(\pi_k))}
\end{equation}

However, the question remains which permutation to use?
The best permutation for the given set of objects would be according to the actual relevance scores of the objects.
More precisely,  it would be the objects ordered in the descending order of their relevance scores.
Let this permutation be represented by $\pi^*$.
Hence our probability equation becomes
\begin{equation}
\log P_{\pi^*} = \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}

The ListMLE loss function maximizes this probability.
Since we generally minimize the objective function, the loss function is given by
\begin{equation}
L_{mle} = - \log P_{\pi^*}
\end{equation}
Expanding the right-hand side of the equation gives us the final loss function that must be minimized.
This is the general form of the loss function proposed by Xia et al. ~\cite{listmlepaper}.
\begin{equation}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}


Notice that the objects' ground truth score values are unused in the loss calculation.
This means that the absolute predicted scores of the objects do not matter.
The only constraint is that the scores must have the correct relative values and that using these values to sort the list should result in the correct ordering of objects.
Even non-linearly scaling the actual scores does not affect the output as long as the constraint is maintained.

In the case of ListNet,  however,  the actual scores do matter.
The probability of selecting objects depends on their ground truth scores.
Hence the result is invariant only to linear scaling.
This advantage makes the ListMLE loss function superior to the ListNet loss. The ListMLE function has a larger target function space, making convergence easier during optimization.
Because of this advantage, we use ListMLE as a list loss function in our thesis.


\section{Position Enhanced Ranking}\label{sec:positionEnhancedRanking}

In many problem domains that use the ranking concept,  it may not be essential that each object be placed at an exact location as induced by its relevance.
For example,  when a search engine ranks its search results, it is more important to find the most important results and rank them correctly than order the least important results correctly.
This is also the case for the problem of ranking HP configurations in the SMBO process.
During the SMBO process, the ranking surrogate is only needed to obtain the most important HP configuration at each step in the optimization.

Lan et al. ~\cite{positionawarerankinglistmle} discuss this problem in their paper,  "Position-Aware ListMLE: A Sequential Learning Process for Ranking".
However,  they reformulate the problem as a sequential learning process.
A more accessible approach is to weight each object component in our ListMLE by any decreasing function $c$ as proposed by Chen
 et al. ~\cite{TRLWO}.
This is possible because the ListMLE loss function is in the form of a summation.

Hence the weighted ListMLE function is given by:
\begin{equation}
L_{mle} = -  \sum\limits_{j=1}^{k} c(j) \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}
Where $c(j)$ gives the weight of the rank j in the ordered list.
This is the approach used in our model to improve our ranking loss function.
The type of decreasing function used is discussed in more detail in chapter 4.
Note that it is also possible to use the weighting in the ListNet case as ListNET and ListMLE have similar forms.

\section{Uncertainty modelling using Deep Ensembles}\label{sec:uncertaintyDeepEnsembles}

Deep Neural Networks (DNNs) are machine learning models with very high representational capacity~\cite{Goodfellow-et-al-2016}.
Due to this property,  one can use them as surrogates for HPO objective functions.
However, the issue is that DNNs do not quantify uncertainty trivially.
They predict results overconfidently, and the results may be wrong.
If any HPO optimization algorithm used a DNN as a surrogate,  its overconfident wrong predictions could cause much computational overhead by predicting inefficient HP configurations.
This becomes even more crucial in an optimization technique like SMBO because one inefficient configuration selection might have a cascading effect of selecting further inefficient configurations.
As DNNs are used as scoring functions in our proposed method, and we use the SMBO algorithm for the HP optimization, we must study how to model uncertainty efficiently using them.

In the current literature,  uncertainty quantification methods using deep neural networks can be broadly classified into the following methods:
\begin{itemize}
\item Bayesian Neural networks~\cite{Goan-2020}.
\item Ensemble Approach using Monte Carlo drop out~\cite{JMLR:v15:srivastava14a}.
\item Ensemble approaches using multiple neural networks ~\cite{DeepEnsemblePaper}.
\end{itemize}

In a Bayesian neural network(BNN), a prior over the network parameters (Weights and Biases) is specified during the initialization of the BNN.
Given the data,  a posterior predictive distribution is calculated for all the network parameters.
One issue with this approach is that BNNs are very complex and challenging to train. 

Monte Carlo dropout is a regularization technique used during the training of DNNs.
With a certain probability,  connections between neurons are dropped.
Using this dropping mechanism, one could obtain possibly $2^N$ neural networks where $N$ is the number of connections in the neural network.
We thus get an ensemble of high-capacity models for free.
If we drop the connections during training, our model can be regularized well.
However, using Monte Carlo dropout during the evaluation,  we can get multiple results from the same input.
Lets say we are given an input $x$,  we can obtain an output $y = \textrm{DNN}(x)$.  If we have $m$ neural networks obtained using Monte Carlo dropout,  we get $\{y_1, y_2... y_m\}$ outputs.  We can calculate the mean and variance of the outputs by using the following equations
\begin{equation}\label{eq:simpleDNNensemble}
y_\textrm{mean} = \frac{ \sum\limits_{i=1}^{m} y_i}{m}  \;\;\;\;\;  y_\textrm{variance} =\frac{ \sum\limits_{i=1}^{m} (y_i - y_{mean})^2}{m-1}
\end{equation}
Please note that the $m-1$ in the denominator is due to Bessel's Correction~\cite{besselcorrection} to reduce the bias in estimation.

In an ensemble of multiple neural networks, we have separate neural networks instead of using a mechanism to get new neural networks from one network (like in the case of Monte Carlo).
These neural networks are trained separately or together, depending on the architecture of the ensemble.
If we have $m$ separate neural networks, we would have $m$ outputs similar to the Monte Carlo approach discussed above.
Hence we could use the Equation~\ref{eq:simpleDNNensemble} to calculate the ensemble's mean and variance for this approach.

Lakshminarayana et al.~\cite{DeepEnsemblePaper} propose another method to predict uncertainty using deep neural networks.
They propose that the uncertainty prediction can be made directly using a single neural network.
This is possible if we assume that the underlying uncertainty is a Gaussian distribution.
With this assumption, the neural network would have two outputs instead of one. One output for the mean of the prediction, say $\mu$, and the other for its variance,  say $\sigma^2$.
One important point to note is that the variance cannot be negative.
The authors ensure this by passing the neural network's output through a strictly positive "soft plus" function.

The authors use the following loss function for the optimization.
$$
L_{de} = \frac{\log \sigma^2 }{2} + \frac{(y - \mu)^2}{2\sigma^2} + k
$$
Where both the mean and the variance is given by $\mu = g(\theta, \textbf{x})$  and $\sigma^2 = f(\theta,  \textbf{x})$ respectively.
As there are multiple neural networks, each predicting its Gaussian distribution,
there needs to be a mechanism to integrate the results.
This is done using a mixture of Gaussian Distributions.
If there are $m$ neural networks in the ensemble (each with its own mean and variance), the final mean and variance are given by
$$
\mu_{final} =  \frac{\sum\limits_{i=1}^{m} \mu_i}{m} \quad \textrm{and} \quad \sigma_{final}^2 = \frac{\sum\limits_{i=1}^{m} (\sigma_i^2 + \mu_i^2) - \mu_{final}^2}{m}
$$

We use the loss function proposed by Lakshminarayana et al.~\cite{DeepEnsemblePaper} to build a deep ensemble baseline surrogate in this thesis. 
However, the prediction of uncertainty using the proposed ranking loss surrogates is made using the simple ensemble approach given in equation~\ref{eq:simpleDNNensemble}.
This is because the integration of a loss function that learns both the mean and variance with the ranking loss functions is non-trivial.
To integrate these two concepts, one must do a thorough theoretical analysis which is out of this thesis's scope.

\section{Baselines}

In this thesis,  2 HPO techniques were implemented before studying the proposed model - Deep Ensembles (proposed by Lakshminarayana et al.~\cite{DeepEnsemblePaper}) and Few Shot Bayesian Optimization (FSBO).

Deep Ensembles (DE) were studied with two main objectives in mind.
First, to study how uncertainty is estimated using Deep Neural Networks.
This was a prerequisite to understanding how to implement uncertainty in our proposed model as we use deep neural networks as a scorer in our model. 
The second objective was to understand how a non-transfer technique would work for the HP optimization problem.
It is also possible to make the Deep Ensemble surrogate a transfer technique by meta-training it before using it in the HP optimization. However, we did not do this in our thesis.

The second technique implemented was FSBO.
We chose this because this gave the state-of-the-art results on the HPO-B benchmark. The HPO-B benchmark is discussed later in Chapter~\ref{chap:HPOBExplanation}.
Studying FSBO also gave us an idea of implementing the transfer HPO mechanism in our proposed ranking loss surrogate model.
In addition, we used Random search and GP as standard baselines for result comparison.

Both these FSBO and DE baselines have built-in capability for uncertainty estimation.
Hence we use an acquisition function called Expected Improvement when using these baselines in the SMBO algorithm.
This is because it uses uncertainty to its advantage to suggest the best HP configurations.
Furthermore, it has advantages over other acquisition functions ~\cite{Jones1998}.
The following two sections discuss the details of the baselines methods used in our thesis.

\subsection{Deep Ensemble}

As previously mentioned, Deep Ensembles were implemented as non-transfer HPO surrogates.
Hence, there was no meta-training done for them.
Consequently, the usage of DE as a surrogate was quite similar to that of Gaussian processes.
Algorithm~\ref{alg:deepEnsembleFinetuning} shows how they were used in the SMBO process. Pineda et al. ~\cite{pineda2021hpob} implemented the infrastructure for manipulating HP configurations. We used this infrastructure to test the baseline and the proposed surrogates with their respective acquisition functions.
In Algorithm~\ref{alg:deepEnsembleFinetuning}  each time a new or best HP configuration is selected and evaluated,  an HP optimization step (or optimization cycle) completes.
In the rest of this report,  we refer to this as an optimization step.

\begin{algorithm}[htb]
\caption{SMBO with Deep Ensemble surrogate}
\label{alg:deepEnsembleFinetuning}
\begin{algorithmic}
    \State $X_{known},  Y_{known} \gets$ Initial configurations and their evaluations.
    \State $X_{pending} \gets$ Configurations to evaluate.
    \State $k \gets$ Number of optimization steps.
    \For{$i < k$}
        \State DE $\gets$ Randomly initialize a list Neural Networks. 
        \For {$nn \in$ DE}  \Comment{Can be trained in parallel}
            \State train($nn$) with $X_{known},  Y_{known}$
        \EndFor
        \State $EI_{scores} \gets$ EI( $X_{pending}$ ) \Comment{Expected Improvement scores}
        \State $x^* \gets $ best $(EI_{scores})$
        \State $y^* \gets f(x^*)$ \Comment{HP objective function evaluation}
        \State $X_{known} \gets X_{known} \cup \{x^*\}$
        \State $Y_{known} \gets Y_{known} \cup \{y^*\}$
        \State $X_{pending} \gets X_{pending} \setminus \{x^*\}$ 
        \State $i \gets i + 1$
    \EndFor
    
\end{algorithmic}
\end{algorithm}

We use similar procedures like Algorithm~\ref{alg:deepEnsembleFinetuning} for other models, i.e., FSBO and the proposed Ranking Loss surrogate model.
The only difference is that the surrogate and its training differ with different methods.

A few points are worth noting here.
First, the algorithm evaluates a set of discrete HP configurations in the HP search space.
This is the same approach we take when applying this algorithm to our model because the ranking concept requires a set of discrete objects.
The advantage of using this approach is that there is no restriction on the type of search space we optimize.
It may be discrete or continuous.
If it is continuous,  we only have to discretize it to a required granularity based on our computational resources.

Secondly, neural networks are reinitialized before training at each optimization step.
The old trained neural networks are discarded.
The rationale behind this is discussed in Section~\ref{sec:restart}.
We also use this reinitialization in FSBO and the proposed ranking loss surrogate.
Finally, as the neural networks are independent,  they can be trained in parallel.
This makes DE surrogates scalable.

One question that remains to be answered is what sort of architecture our neural networks use.
For this, two architectures were analyzed - an undivided neural network as shown in Figure~\ref{fig:undividedarchitecture} and a neural network divided at its tail as shown in Figure~\ref{fig:dividedarchitecture}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$X$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCn)                      [right=of FCmiddle]        {$FC_n$};
\node[roundnodey]      (mean)                             [right=of FCn] {$\mu$};
\node[roundnodey]      (variance)                             [below=of mean] {$\sigma^2$};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCn.west);
\draw[->] (FCn.east) -- (mean.west);
\draw[->] (FCn.east) -- (variance.west);
\end{tikzpicture}
\caption{Example of an undivided Neural Network architecture.}
\label{fig:undividedarchitecture}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$X$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCpenaltimate)             [right=of FCmiddle]        {$FC_{n-1}$};
\node[squarednode]      (FCn1)                      [right=of FCpenaltimate]        {$FC_{n_1}$};
\node[squarednode]      (FCn2)                      [below=of FCn1]        {$FC_{n_2}$};
\node[roundnodey]      (mean)                             [right=of FCn1] {$\mu$};
\node[roundnodey]      (variance)                             [right=of FCn2] {$\sigma^2$};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCpenaltimate.west);
\draw[->] (FCpenaltimate.east) -- (FCn1.west);
\draw[->] (FCpenaltimate.east) -- (FCn2.west);
\draw[->] (FCn1.east) -- (mean.west);
\draw[->] (FCn2.east) -- (variance.west);
\end{tikzpicture}
\caption{Example of a divided Neural Network architecture.}
\label{fig:dividedarchitecture}
\end{figure}

In both these figures,  FC stands for fully connected layers.
The ranking loss surrogate uses an undivided architecture similar to an architecture shown in Figure~\ref{fig:undividedarchitecture}. Hence, we used this undivided architecture in the Deep Ensemble baseline implementation to compare the surrogates' results consistently.

\subsection{FSBO}
\label{sec:FSBOBackground}
Few Shot Bayesian Optimization (FSBO) is a transfer HPO method that utilizes meta-learned surrogates for the knowledge transfer mechanism.
It reformulates the HPO problem into a few-shot learning task.
In FSBO, the surrogate meta-learns common characteristics of some given tasks by training on their respective metadata. After that, it adapts to the target task during HP optimization by finetuning for a few training epochs using the known HP configurations and their evaluations.
Therefore, the following two steps are required to be done in chronological order:
\begin{enumerate}
\item Meta training - For knowledge transfer.
\item Finetuning - For few-shot learning.
\end{enumerate}

The authors of the FSBO paper (Wistuba et al.~\cite{fsbopaper}) make use of a deep kernel surrogate proposed by Wilson et al.~\cite{pmlr-v51-wilson16}.
Here, a neural network transforms points in the HP search space into a latent space.
Kernels are then applied to this latent space in a Gaussian process to obtain a probabilistic evaluation.
The deep kernel can is represented as:~\cite{fsbopaper}
$$
k(\phi(\textbf{x}, \textbf{w})  ,   \phi(\textbf{x'}, \textbf{w}) |  \mathbf{\theta})
$$
where $\textbf{x}$ and $\textbf{x'}$ are configurations in the original HP search space. $\mathbf{\theta}$ and $\textbf{w}$ are parameters of kernel $k$ and the neural network $\phi$ respectively.
We used the implementation of deep kernels provided by Patacchiola et al.~\cite{patacchiola2020bayesian} in this thesis.
The parameters $\mathbf{\theta}$ and $\textbf{w}$  are learned by maximizing the log marginal likelihood.
If we represent $\textbf{X}^{(1)}$... $\textbf{X}^{(T)}$ as the evaluated HP configurations of $T$ tasks and $\textbf{y}^{(1)}$...$\textbf{y}^{(T)}$ as their evaluations, the loss function maximises the following log probability
$$
\log p( \textbf{y}^{(1)}...\textbf{y}^{(T)} | \textbf{X}^{(1)}...\textbf{X}^{(T)}, \mathbf{\theta}, \textbf{w})
$$

During meta-training, we train our surrogate by learning the parameters $\mathbf{\theta}$ and $\textbf{w}$.
We used the Matern $\frac{5}{2}$ kernel in our implementation.
A fully connected neural network was used to obtain the latent space representation.
A New FSBO model and, consequently, new surrogate parameters must be learned for every new search space.
This is the case even if the input dimensions of the HP search space are the same.
This is because every HP search space represents a different ML model.

If an assumption is made that the knowledge from the metadata is enough to predict the best HP configuration for all future tasks, the finetuning step may be skipped.
However,  this is rarely the case. There are always variations in the new tasks.
Therefore, finetuning was used with the FSBO model in every HP optimization procedure.

The optimization algorithm in the case of FSBO is similar to Algorithm~\ref{alg:deepEnsembleFinetuning}.
The acquisition function used in this model is also Expected Improvement.
The FSBO implementation also uses restarting the finetuning before each HPO optimization step.
However, the meta-trained FSBO surrogate is loaded anew instead of randomly initializing it, as in the case of DE surrogates.

We used an early stopping mechanism for meta-training to avoid substantial computation costs and overfitting of the surrogate.
We took advantage of the split of meta-training and meta-validation data to do this early stopping.
The training was stopped if the training error was consistently higher than the validation error for a set number of epochs (we used ten epochs in our case).
 
\iffalse
\subsubsection{Implementation issues}
After completing the own implementation,  we found that the results obtained were not in par with the paper.
For this reason, the results are used only for the first stage.
The results already present in the benchmarking data of~\cite{pineda2021hpob} were used for comparing this result with our implementation.
\fi

% \section{Evaluation Strategy}



\chapter{Proposed Method: Deep Ranker}\label{chap:ProposedIdea}

% \section{Proposed Idea: : Ranking Loss Surrogates}

The choice of the surrogate used in a model-based optimization algorithm is crucial. This is because the selected surrogate directly impacts the optimization performance.
Moreover, we also have to answer critical questions prior to its selection. Some of them are - Does the surrogate have enough representative capacity?
Does it have the capability of representing uncertainty?
How is the surrogate learned?

In the quest to improve HPO surrogates,  we propose and analyze a new type of surrogate model based on the concept of ranking:
There are two primary components of our proposed idea

\begin{itemize}
\item The learning mechanism of the surrogate model.
\item The surrogate model itself.
\end{itemize}

This chapter discusses in detail both these components.
We initially use a model with sufficient representational capacity and a capability to represent uncertainty.
An excellent model with these properties is a Deep ensemble; hence, we use it as a model.
We then analyze and implement the proposed learning algorithm that uses the concept of ranking.
Finally, we make the proposed ranking loss surrogate context-aware by integrating deep sets into the Deep ensemble architecture.
We use SMBO as a reference HPO algorithm for our study.



\section{Deep Ranker}
\label{sec:BasicScoringModelDNN}

In order to discuss the implementation details of ranking loss functions,  we need to understand how a basic ranker works.
A basic ranker consists of a Deep Neural Network that outputs real-valued scores $\textbf{s}$ for a batch of inputs, say $\textbf{X}$.
These scores are used to rank the batch of inputs, with the highest score getting the lowest rank.
Our thesis assumes that a lower rank is better than a higher one.
Figure~\ref{fig:basicScoringModel} depicts the architecture of a basic ranker.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$\textbf{X}$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCn)                      [right=of FCmiddle]        {$FC_n$};
\node[roundnodey]      (y)                             [right=of FCn] {$\textbf{s}$};
\node[squarednode]      (Rank)                      [right=of y]        {Rank};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCn.west);
\draw[->] (FCn.east) -- (y.west);
\draw[->] (y.east) -- (Rank.west);
\end{tikzpicture}
\caption{Basic ranker.}
\label{fig:basicScoringModel}
\end{figure}

This DNN architecture is very similar to the DNNs used in the Deep Ensemble baseline implementation (Figure~\ref{fig:undividedarchitecture}).
The difference between the 2 is that Figure~\ref{fig:undividedarchitecture} had 2 outputs whereas Figure~\ref{fig:basicScoringModel} has one output.
In addition, there is one extra but crucial step of ranking in the basic ranker.
Since we use a Deep Neural Network to get the scores before ranking the inputs,  we may also refer to this architecture as a Deep Ranker.

Let  \{\textbf{X}, \textbf{y}\} be the training data used for training a generic machine learning model.
Most loss functions utilize the values present in \textbf{y} as a reference for training the ML model.
After the training completes, the range of the outputs of the learned ML model is not vastly different from the range of the actual output values.
However, when we use a ranking loss function to train the ML model, we cannot guarantee this. This is because the ranking loss calculates the loss in the ranking space instead of the output space.
The only thing that matters for ranking loss functions is that the objects should be ranked in the correct order.
Hence the score range of the basic ranker can be arbitrary.
The only exception is a point-loss function called Subset Regression.
This is because subset regression directly learns the rank instead of using the output score $s$ to rank the batch of inputs.

There may be occasions where the score range of the basic ranker needs to be controlled.
For further discussion on this please refer Appendix~\ref{chap:OutputRangeControl}.


\subsection{Uncertainty implementation using Deep Ensembles}\label{sec:UncertaintyImplementation}

We know from Section~\ref{sec:hpoConstraints} that the evaluation of the target objective function in HPO is noisy.
Hence it is essential for any surrogate that models this objective function to have the capability of estimating uncertainty.
The uncertainty estimation becomes more critical in a sequential HPO algorithm like SMBO.
In the first steps of SMBO, we do not have enough data to build or finetune the surrogate.
Using a surrogate without uncertainty in the initial steps may give wrong results with high confidence.
This is not good in a sequential process because selecting subsequent HP configurations depends on the previously selected HP configurations.
For this reason, a model with uncertainty (if estimated correctly) is superior to other models.


We now integrate multiple basic rankers (or Deep Rankers) to build an ensemble of rankers.
Using this ensemble, we can estimate uncertainty in the rank prediction.
Figure~\ref{fig:proposeModelUncertainty} shows the architecture of the ensemble of basic rankers.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]          (X)                                           {$\textbf{X}$};
\node[squarednode]      (DNN2)                      [right=of X]        {DNN$_2$};
\node[squarednode]      (DNN1)                      [above=of DNN2]                    {DNN$_1$};
\node[squarednode]      (DNN3)                      [below=of DNN2]                    {DNN$_3$};
\node[roundnodey]        (y1)                             [right=of DNN1] {$\textbf{s}_1$};
\node[roundnodey]        (y2)                             [right=of DNN2] {$\textbf{s}_2$};
\node[roundnodey]        (y3)                             [right=of DNN3] {$\textbf{s}_3$};
\node[squarednode]      (Rank1)                             [right=of y1] {Rank$_1$};
\node[squarednode]      (Rank2)                             [right=of y2] {Rank$_2$};
\node[squarednode]      (Rank3)                             [right=of y3] {Rank$_3$};
\node[squarednode]      (UncertaintyEstimation)                             [right=of Rank2] {Uncertainty Estimation};
\node[roundnodey]        (mus)                             [right=of UncertaintyEstimation] {$\mu_r$};
\node[roundnodey]        (sigmas)                             [below=of mus] {$\sigma^2_r$};

%Lines
\draw[->] (X.east) -- (DNN1.west);
\draw[->] (X.east) -- (DNN2.west);
\draw[->] (X.east) -- (DNN3.west);
\draw[->] (DNN1.east) -- (y1.west);
\draw[->] (DNN2.east) -- (y2.west);
\draw[->] (DNN3.east) -- (y3.west);
\draw[->] (y1.east) -- (Rank1.west);
\draw[->] (y2.east) -- (Rank2.west);
\draw[->] (y3.east) -- (Rank3.west);
\draw[->] (Rank1.east) -- (UncertaintyEstimation.west);
\draw[->] (Rank2.east) -- (UncertaintyEstimation.west);
\draw[->] (Rank3.east) -- (UncertaintyEstimation.west);
\draw[->] (UncertaintyEstimation.east) -- (mus.west);
\draw[->] (UncertaintyEstimation.east) -- (sigmas.west);

\end{tikzpicture}
\caption{Ensemble of Basic Rankers.}
\label{fig:proposeModelUncertainty}
\end{figure}


In Section~\ref{sec:uncertaintyDeepEnsembles}, uncertainty is estimated by calculating the mean and the variance of the DDN's output scores. This calculation of uncertainty is in the output space of the ensemble.
In the case of ranking loss surrogates, uncertainty estimation is a  little different.
As we are primarily interested in ranking the HP configurations, individual scores and their distributions have less meaning for us.
Instead, we are concerned about the uncertainty in the predicted rank.
In other words, we want to calculate the uncertainty in the rank space.

We first calculate the rank of each HP configuration in the batch as per the rankers' output scores.
Using these ranks, we calculate the mean rank $\mu_r$ and variance in the rank $\sigma^2_r$.
Any acquisition function that uses a ranking model needs to use this "ranking distribution" instead of the output score distribution.

\iffalse
After adding Deep Sets, we train all of them together due to the architectural requirement.
\fi

\iffalse
Hence for example if we are using ListMLE,  our loss function would be given by
\begin{equation}\label{eq:overflowissue}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\exp(\mu(\pi^*_j))}{ \sum\limits_{t=j}^k \exp(\mu(\pi^*_k))}
\end{equation}
\fi

\subsection{Acquisition function in the Ranking space}
\label{sec:AcquisitionFunctionInRankingSpace}
When doing HP optimization using ranking loss surrogates, it is essential to note that the ranking of an HP configuration is relative.
A configuration can have different ranks based on the batch in which it is located.
The expected rank of a configuration is not defined in absolute terms. It is always conditioned on other configurations in the ranking batch.
Due to this property, using the predicted uncertainty in the ranking space is tricky.

Moreover, using well-defined acquisition functions like Expected Improvement (EI) is not trivial. EI requires us to maintain an incumbent evaluated value. The incumbent is the best-evaluated HP objective function's value in the output space. However, it is always 1 in the ranking space because the best possible rank in any ranking batch is always 1. Hence, if we used EI with ranking loss surrogates, it would be impossible to select the following HP configuration efficiently.
It is also not possible to directly use the output scores of the ensemble. Each neural network in the ensemble gives separate scores for the input batch of configurations. These scores of each neural network may not be in the same range.

Due to these intricacies, we believe a detailed study of the different acquisition functions in the ranking space is necessary. Such a study is out of scope for this thesis. Therefore, we leave this study for future research.
Instead, throughout our thesis, we use an elementary acquisition function with ranking loss surrogates. This acquisition function selects the HP configuration with the best mean rank predicted by the ensemble of rankers.


\iffalse
One problem that may come up during the training of these DNNs together is that the ranges of the DNNs may be different.
This is taken care by the fact that we use a range controller in our scoring function as shown in Figure~\ref{fig:basicScoringModel}.
Hence, the ranges of all the DNNs are the same. 
The variance within the ensemble is still present due to the random initialization of the neural networks.
By using the range controlling mechanism we both make the model numerically stable and make the training more efficient.
\fi

\subsection{ListMLE Implementation}

This thesis implemented the listwise loss function ListMLE to train the rankers. The deep learning library PyTorch~\cite{PyTorch} was used for the implementation.
Algorithm~\ref{alg:listMLEAlgorithm} shows the steps to obtain the real-valued loss given the actual output and the predicted output.
After calculating this loss, it is backpropagated through the ranker using the Autograd functionality of PyTorch.
Please note that the implementation is a little more sophisticated than this due to the use of multi-dimensional tensors.

Before calculating the loss, the predicted scores are reduced by a constant factor for numerical safety and accuracy. Here we use the max of the predicted scores as a constant factor. This can be seen in Step 2 of \textsc{ListMLE} procedure in Algorithm~\ref{alg:listMLEAlgorithm}.
This reduction of the constant factor does not affect the loss because the strictly increasing positive function we use is exponentiation. This is illustrated in Equation~\ref{eq:ExpConstantFactor} using sample numbers $a_i \in  \mathbb{R}$.
In this equation, $k$ represents the total number of elements that are ranked.

\begin{equation}
\label{eq:ExpConstantFactor}
\frac{e^{a_1}}{\sum\limits_{i} e^{a_i}} = \frac{e^{a_1 + k}}{\sum\limits_{i} e^{a_i + k}} 
\end{equation}

\begin{algorithm}[h]
\caption{ListMLE Algorithm}
\label{alg:listMLEAlgorithm}
\hspace*{\algorithmicindent} \textbf{Input} : $l_{predicted} \in \mathbb{R}^k$ \Comment{Scores predicted by the ranker}\\
\hspace*{\algorithmicindent} \textbf{Input} : $l_{actual} \in \mathbb{R}^k$ \Comment{Actual scores}\\
\hspace*{\algorithmicindent} \textbf{Output} : Loss $\in \mathbb{R}$
\begin{algorithmic}[1]
    \Procedure{ListMLE}{$l_{predicted}$,  $l_{actual}$}
    \State $l_{predicted} \gets l_{predicted} - \max(l_{predicted})$
    \State $l_{predicted} \gets \exp(l_{predicted})$   \Comment{For Numerical Stability}
    \State $sum \gets 0$
    \For{$i < k$} \Comment{$k$ is list size here}
        \State $sum \gets sum$ + \textsc{Top1LogProb}($l_{predicted}$,  $l_{actual}$)
        \State $l_{predicted},  l_{actual} \gets $ \textsc{RemoveTop1}($l_{predicted}$,  $l_{actual}$)
        \State $i \gets i + 1$
    \EndFor
    \State  Return $-1 * sum$     \Comment{Negating loss as we are doing gradient descent}
    \EndProcedure
     \Procedure{Top1LogProb}{$l_{predicted}$,  $l_{actual}$}
    \State $j \gets \textrm{argmax}(l_{actual})$
    \State $prob \gets \frac{l_{predicted}[j]}{\sum l_{predicted} }$
    \State  Return $\log(prob)$
    \EndProcedure
         \Procedure{RemoveTop1}{$l_{predicted}$,  $l_{actual}$}
    \State $j \gets \textrm{argmax}(l_{actual})$
    \State $l_{predicted} \gets$ \textsc{RemoveElementAtIndex}($l_{predicted}$, $j$)
    \State $l_{actual} \gets$ \textsc{RemoveElementAtIndex}($l_{actual}$, $j$)
    \State  Return $l_{predicted}$,  $l_{predicted}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:listMLEAlgorithm} calculates the permutation probability of the objects in the list as described in Section~\ref{sec:listMLE}.
However, this implementation is not efficient.
The algorithm modifies and removes elements in predicted and actual lists.

Consider a case where the list elements are stored in a data structure that stores objects in contiguous memory locations (like an array).
The time complexity of removing an element from an array is O(n), where $n$ is the list size.
The removal operation is done for all list elements one by one resulting in the worst-case time complexity of $O(n^2)$.
Even if we store the list in a data structure that stores its elements in a non-contiguous memory location (for example, a linked list), accessing the elements in the list would have $O(n)$ time complexity. This would also result in time complexity of $O(n^2)$.

One way to solve this problem is to sort the lists before calculating the permutation probability.
This approach is followed by Pobrotyn
 et. al~\cite{Pobrotyn2020ContextAwareLT} in their implementation.
After sorting the lists, calculating the permutation probability can be done with linear time complexity. Since sorting has a time complexity of $O(n \log n)$, the worst-case complexity remains $O(n \log n)$.
For this reason, we use the implementation given by Pobrotyn
 et. al~\cite{Pobrotyn2020ContextAwareLT} in this thesis.
Algorithm~\ref{alg:listMLESorted} depicts this approach.


More precisely, the ListMLE loss is re-written in the following form:

\begin{equation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log \sum\limits_{t=j}^k \exp(s(\pi^*_k)) - \log \exp(s(\pi^*_j)) \right)
\end{equation}

Since the list values are sorted in the correct order, we can further synthesize the equation as:

\begin{equation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log \sum\limits_{t=j}^k \exp(s^*(k)) - \log \exp(s^*(j)) \right)
\end{equation}

Now, $\sum\limits_{t=j}^k \exp(s^*(k))$ is nothing but the reverse cumulative sum which is the sum of all list values starting from position $j$.
The reverse cumulative sum of all positions can be calculated and stored in linear time using iterative dynamic programming.
The expression $\log \exp(s^*(j))$ can be directly written as $s^*(j)$ if we use a natural logorithm.
Hence, if the reverse cumulative sum at position j is represented by $Q(j)$,  the equation implemented in Algorithm~\ref{alg:listMLESorted} is given by:
\begin{equation}\label{eq:sortedListMLEequation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log Q(j) - s^*(j) \right)
\end{equation}

\begin{algorithm}[h]
\caption{ListMLE Algorithm (sorted)}
\label{alg:listMLESorted}
\hspace*{\algorithmicindent} \textbf{Input} : $l_{predicted} \in \mathbb{R}^k$ \Comment{Scores predicted by ranker}\\
\hspace*{\algorithmicindent} \textbf{Input} : $l_{actual} \in \mathbb{R}^k$ \Comment{Actual scores}\\
\hspace*{\algorithmicindent} \textbf{Output} : Loss $\in \mathbb{R}$
\begin{algorithmic}[1]
\Procedure{ListMLESorted}{$l_{predicted}$,  $l_{actual}$}
\State $l_{actual}, $ IndexOrder $\gets$ \textsc{Sort}($l_{actual}$)
\State $l_{predicted} \gets$ \textsc{SortWithIndexOrder}($l_{predicted}$,  IndexOrder)
\State $l_{predicted} \gets l_{predicted} - \max(l_{predicted})$ \Comment{For Numerical Stability}
\State $prob \gets 0$    
    \For{$i < k$}
        \State $prob \gets prob + \log Q[i] - l_{predicted}[j]$
        \State $i \gets i + 1$
    \EndFor
\State Return $prob$
\EndProcedure
\end{algorithmic}
\end{algorithm}

To test this implementation, we did a case study to learn the inverse mapping of real-valued numbers in a given range.
For more details on the case study, please refer to Appendix~\ref{chap:caseStudy}.

\subsection{Weighted Loss}
As discussed in Section~\ref{sec:positionEnhancedRanking},  the ranking problem we have in the HPO domain is not the general ranking problem.
It is more important for our ranking function to order the top part of the list than the bottom part.
This constraint is evident from Algorithm~\ref{alg:deepEnsembleFinetuning} where we are only concerned about selecting the best available configuration for evaluating the HPO objective function at every optimization cycle.
Keeping this constraint in mind,  we need to use a weighting function $c(j)$ such that$ c(j) \propto \frac{1}{j} $.
Here, $j$ is the ranking of the object.

This section discusses some strategies to do this weighting for our problem.
The concept of biased ranking loss is discussed in "Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation" by  Chen et al. ~\cite{TRLWO}.
Chen et. al propose a position based weighting of the ranking function such that:
\begin{equation}
w_j = \frac{k - j + 1}{\Sigma_{t=1}^k t}
\end{equation}
where $w_j$ represents weight of the object at position $j$ in the ordered list.
$k$ represents the total number of elements that are ranked.
This strategy decreases the weights linearly across the ranks.

Linear weighting has an issue. It makes the weight of the middle object in the list $0.5$ times the weight of the object at the top.
This can be seen in the position dependent attention graph of figure~\ref{fig:weightingfunctions} (The weights of the position dependent attention graph are scaled by a factor of 50 to make a fair comparison between the weights).
This, however, is not what we want for our case.
We need a sharper decrease in weights across the ranks.
We could use 2 alternative strategies of weighting the ordered list
\begin{itemize}
\item Inverse linear weighting given by $w_j = \frac{1}{j}$.
\item Inverse logarithmic weighting given by $w_j = \frac{1}{\log (j+1)}$.
\end{itemize}

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.65]{images/weightingfunctions}
    \caption{Different Weighting Functions.}
    \label{fig:weightingfunctions}
\end{figure}

Figure~\ref{fig:weightingfunctions} shows all 3 weighting strategies.
In the case of inverse linear strategy,  we see that the weight becomes extremely small after only a few ranks in the list.
It has an unwanted consequence for our problem.
Because the weights of objects at the end of the list are too small,  the ranking function may not learn to rank optimally.
On the other hand, inverse log weighting is the most suitable because it neither completely ignores the objects at the end of the list nor gives them too high an importance.
The weights at the bottom of the list are very similar, which is what we want - all objects at the tail of the list are equally "unimportant" in our case.

Another advantage of using inverse weighting is that we can employ parallelism to enhance the HPO during the optimization cycle.
For example, the top $n$ configurations can be selected at every step,  and they can be tried in parallel for optimization.
This overcomes any uncertainties that may occur while ranking in the top part of the list. 
The parallelism is ineffective when using the inverse linear weighting or weighting proposed by Chen et al.


\iffalse
Applying this to HPOB... (With or without query)
    First learning from first search space.
    Even with this the loss curves are very smooth if we use 2 * tanh(0.01 * x)
\fi

\section{Method training and optimization}

The proposed method in this thesis, "Ranking loss surrogate," is a transfer HPO solution.
For this reason, its working principle is similar to that of the FSBO baseline model.
Hence, the two main parts of using the optimization process are
\begin{itemize}
\item Meta-learning the ranking loss surrogate.
\item Using the trained surrogate in the optimization cycle (with or without finetuning).
\end{itemize}

\subsection{Meta training the surrogate}\label{sec:rlmetatraining}

To understand the meta-training of the proposed surrogate, we need to understand the search space in question. The space of hyper parameter configurations in which we try to get an optimum during any HPO is called HP search space.
This search space is different for different ML models.
Hence we need to meta-train different ranking loss surrogates for different HP search spaces.
However, the same ML model can be used to fit different data sets or tasks.
We get different HP response surfaces in the same search space if we train the ML model for a different dataset.
Hence we obtain different metadata for the same search space, each for a particular dataset used during the training.
The goal of meta-training the surrogate is to use different metadata sets (or tasks) from a single search space to learn all the typical characteristics and transfer this knowledge to a new task.

After understanding the goal of meta-training, we need to define its mechanism. Since we use an ensemble of Deep Neural Networks as a surrogate in our proposed idea, we can employ two strategies for meta-training:
\begin{itemize}
\item Meta-train each network separately.
\item Use the mean rank of the entire architecture to train the networks together.
\end{itemize}
A combined meta-training method is more efficient than meta-training each neural network separately.
However, it is theoretically unnecessary to do this.
Moreover, the combined meta-training also reduces the variance in the ensemble. This is because we must tie up each neural network's losses before backpropagating the loss through the networks. This reduces the independence of the networks in the ensemble, which is a disadvantage.
Therefore we train each network separately.

We use stochastic gradient descent to meta-train every network.
To do this, we must sample a batch of data from various metadata in a search space.
There are two ways to accomplish this:
\begin{itemize}
\item Double sampling: Sample the task, then sample the metadata within the task.
\item Sample metadata from all tasks together.
\end{itemize}

We use the first method of sampling.
The reason for doing this is due to have a higher level of randomness in the sampling process. If we use a double sampling mechanism, there is also a good regularization and a possibility to use a lower learning rate due to a higher number of gradient descent steps in the meta-training process.
We use Adam as our surrogate's optimizer with a learning rate of 0.001.
We do the meta-training for 5000 epochs, and in each epoch, we take 100 steps in which each step does one double sampling.
While sampling data points from within a metadata set, we sample without replacement.
We use the number of lists (batch size) as 100 and the size of each list also as 100.
Algorithm~\ref{alg:RLSMetaTraining} illustrates to us a brief skeleton of the meta-training procedure.
After the meta-training, every model is saved to a persistent location (e.g., hard disk) so that it can be loaded when necessary.

\begin{algorithm}[h]
\caption{Ranking Loss surrogate Meta-training}
\label{alg:RLSMetaTraining}
\hspace*{\algorithmicindent} \textbf{Input} : $epochs \in \mathbb{I}$ \\
 \hspace*{\algorithmicindent} \textbf{Input} : $X_{train}$,  $y_{train}$  \Comment{Meta data used to train from all Search Spaces.} \\
\hspace*{\algorithmicindent} \textbf{Input} : $m_{\theta}$ \Comment{Surrogate model to train (parameterized by $\theta$)}
\begin{algorithmic}[1]
\Procedure{MetaTrain}{$m_{\theta}$,  $X_{train}$,  $y_{train}$, $epochs$}
    \For{$i < $ epochs $*$ batch size}
                \State $B_{X}, B_{y} \gets$ \textsc{DoubleSample}($X_{train}$) \Comment{Get the training batch}
                \State $y_{pred} \gets m_{\theta}(B_{X})$
                \State $\textrm{loss} \gets L_{mle}(y_{pred},  B_{y}) $
                \State $g \gets \frac{\partial \textrm{loss}}{\partial \theta}$ \Comment{$g$ stands for gradient}
                \State Use $g$ to update $\theta$ using the Adam optimizer
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Finetuning}\label{sec:rlfinetune}
Finetuning is the second major part of the optimization process.
It may be considered optional for transfer HPO surrogates like ours.
However, we found that it gave improved results.

We take the SMBO optimization of Deep Ensembles in Algorithm~\ref{alg:deepEnsembleFinetuning} as an example to understand the finetuning process.
In this algorithm, the deep ensembles are retrained at every optimization step with the seen evaluations of the true HPO objective function.
We need to modify Algorithm~\ref{alg:deepEnsembleFinetuning} by replacing Deep Ensemble training with the finetuning of our model.
To avoid duplication, we do not rewrite the complete algorithm again.

The finetuning process is similar to the \textsc{MetaTrain} procedure in Algorithm~\ref{alg:RLSMetaTraining}.
The difference is that we use seen evaluations instead of the train split of meta-data. Moreover, sampling is not required during the finetuning process as we use all of the available data.
We do finetuning for 1000 epochs.
The number of epochs can be varied based on the computation resources available.

We have to reload the saved surrogate at every optimization step to use the ranking loss surrogate as a transfer HPO method. This is similar to how FSBO is used.
However, if we initialize it randomly instead of loading the saved surrogate, the surrogate can be used as a non-transfer HPO method.
Nevertheless, there is a requirement to start the finetuning process afresh. In the next section, we discuss why this restart is required.
Subsequently, we talk about the cosine annealing used during finetuning.


\subsubsection{Requirement of restarting training}\label{sec:restart}

During the implementation of deep ensembles, we found that the model performs much better in the HPO evaluation cycle when we train it from scratch at every optimization step (acquisition step). This is shown in Section~\ref{sec:baselineResults}.
This result is counterintuitive because an already trained model should quickly converge to a local optimum.
One of the reasons for this performance anomaly is that the model gets biased towards the points observed in the starting steps of the optimization cycle.
Lets say we have 2 models 
\begin{itemize}
\item $\textrm{m}_{\textrm{restart}}$ which always restarts training at every acquisition step.
\item $\textrm{m}_{\textrm{reuse}}$ which continues to train the model trained in the previous optimization steps.
\end{itemize}
Let the models be finetuned for 100 epochs at each step.
Let there be just one seen HP configuration at the beginning of the optimization.
The figures~\ref{fig:bias25},~\ref{fig:bias50},  and ~\ref{fig:bias100} show the number of times each observed HP configuration is used for training the model at the 25th, 50th and 100th optimization step by the $\textrm{m}_{\textrm{reuse}}$ model.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/bias25}
\caption{Bias at $25^{th}$ optimization cycle.}
    \label{fig:bias25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/bias50}
\caption{Bias at $50^{th}$ optimization cycle.}
    \label{fig:bias50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/bias100}
\caption{Bias at $100^{th}$ optimization cycle.}
    \label{fig:bias100}
\end{figure}

We in figures~\ref{fig:bias25},~\ref{fig:bias50},  and ~\ref{fig:bias100} that the number of times a data point is used during the training is biased with the first data point being used much more than the last.
Generally, all the data is used during finetuning due to data scarcity.
Hence,  in our example, the number of times an HP configuration is used scales with the number of epochs trained.
The usage bias is not intended because all observed HP configurations should be treated equally in any training or finetuning step.
When we use $\textrm{m}_{\textrm{restart}}$ every known HP configuration is used only 100 times for finetuning at every evaluation step.
Hence it is better to restart the model before finetuning.

The second reason for restarting is that the surrogate model may get stuck at a stubborn local minimum at any finetuning step $n$, where $1 <= n <= 100$ in our case.
Coming out of this local minimum may require the response surface to change drastically.
The response surface will change drastically only when the training data distribution changes significantly.
This is not possible in the sequential process of SMBO because, at every step, only one new HP configuration is added to the known HP configurations.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/localMinima}
    \caption{Figure showing changes in response surfaces and a stubborn local minimum.}
    \label{fig:localMinima}
\end{figure}

Figure~\ref{fig:localMinima} depicts this issue for a simple 1 dimensional case.
Consider the red curve.
It represents the HP response surface for $k$ known data points.
When we add $(k+1)^{th}$ data point,  most of the response surface remains the same.
Only part of it changes.
The blue curve represents the response surface for $(k+1)$ data points.
If our model is already trained for the red curve,  it becomes challenging for it to come out of the local minimum because of the minimal changes in the response surface.
We call this such a minimum, a stubborn local minimum.
The brown dot in the figure depicts the stubborn local minimum.

If we reload and retrain our model afresh, the probability of reaching a good local minimum is higher. For example, during training for the blue curve, the surrogate model can start from a value $x > 10$. This would enable it to reach a better local minimum than a model that does not restart finetuning.
The number of ways to reach a good local minimum would increase exponentially with the increase in the dimensions of the HP search space.
Hence, restarting the training (from the saved surrogate model in our case) should improve our finetuned surrogate model.

Because of these reasons,  we use the restart mechanism during the finetuning of our ranking loss surrogate (and baseline implementations).
This makes the surrogate more robust.

\subsubsection{Using cosine annealing}

When we plotted the finetuning loss curves of our model, we found that these loss curves were very jittery.
We noticed the same behavior when finetuning the FSBO model.
Figure~\ref{fig:jitteryFTLoss} shows the finetuning loss for one of the search spaces.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/jitteryFTLoss}
    \caption{Figure showing jittery loss function curve.}
    \label{fig:jitteryFTLoss}
\end{figure}

We hypothesize that these jittery losses are because the learning rate is unsuitable for the response curve's curvature at the targeted local minima.
One solution to this is to use a lower learning rate.
However,  finetuning using a lower learning rate will take a long time to converge.
The solution we proposed and utilized was using cosine annealing.

There are a couple of advantages of using cosine annealing.
First, the initial learning rate is kept high to give the optimizer time to get to an area close to the local minima if it has not done so.
After that, the learning rate declines quickly to a small value.
This helps the model get deep into the local minimum; hence, the probability of the optimization jumping out of the local minimum is very low.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/goodFTLoss}
    \caption{Figure showing a loss curve obtained by using cosine annealing.}
    \label{fig:goodFTLoss}
\end{figure}

Figure~\ref{fig:goodFTLoss} shows a finetuning loss curve obtained using cosine annealing.
This strategy reduces the problems that could prop us when using a constant learning rate. However, we realized at the later stage of the thesis that cosine annealing could not be used extensively while finetuning the proposed model due to the issue of negative transfer learning.
During training, on the other hand, we use a constant learning rate as we can use the validation loss as a reference for overfitting or underfitting.

\section{Ranking Surrogate Model}

After explaining the learning mechanism, we now turn to the last critical component that is used in the proposed method.
This is the addition of Deep Sets to the architecture to make our model context-aware.
Context-aware means that the surrogate model conditions its outputs based on the available HP configurations (or support configurations).
This helps the surrogate distinguish between different tasks.
We first discuss the surrogate architecture after the addition of Deep Sets, and after that, we explain how meta-training is done for this surrogate.

\subsection{Using Deep Sets to build context aware models}\label{sec:DeepSetWithModel}

We have already discussed the fundamental ideas of using deep sets in  ~\ref{sec:DeepSets}.
The basic idea is to precondition the surrogate function $s_{\theta}$ on known evaluations of the target HP response surface.
These known evaluations act as a support for the scoring function.
Hence they are subscripted as "s" in the equation.
We then query the surrogate to get the relevant ranks of new $\textbf{X}$ which are subscripted as "q" in the equation.
The surrogate represents a function of the form:
$$
s_{\theta}(\textbf{X}_{q} | \textbf{X}_{s}, \textbf{y}_{s})
$$

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]          (Xs)                                                     {$\textbf{X}_s$, $\textbf{y}_s$};
\node[roundnode]          (Xq)                        [below=of Xs]     {$\textbf{X}_q$};
\node[squarednode]      (DeepSet)              [right=of Xs]        {Deep Set};
\node[squarednode]      (Concatenation)             [below=of DeepSet]        {Concatenation};
\node[squarednode]      (DNN2)                      [right=of Concatenation]        {DNN$_2$};
\node[squarednode]      (DNN1)                      [above=of DNN2]                    {DNN$_1$};
\node[squarednode]      (DNN3)                      [below=of DNN2]                    {DNN$_3$};
\node[roundnodey]        (s1)                             [right=of DNN1] {$\textbf{s}_1$};
\node[roundnodey]        (s2)                             [right=of DNN2] {$\textbf{s}_2$};
\node[roundnodey]        (s3)                             [right=of DNN3] {$\textbf{s}_3$};
\node[squarednode]      (rank2)                             [right=of s2] {Rank$_2$};
\node[squarednode]      (rank3)                             [below=of rank2] {Rank$_3$};
\node[squarednode]      (rank1)                             [above=of rank2] {Rank$_1$};
\node[squarednode]      (uncertainty)                             [right=of rank2] {Uncertainty};
\node[roundnodey]        (mus)                             [right=of uncertainty] {$\mu_r$};
\node[roundnodey]        (sigmas)                             [below=of mus] {$\sigma^2_r$};

%Lines
\draw[->] (Xs.east) -- (DeepSet.west);
\draw[->] (DeepSet.south) -- (Concatenation.north) node[midway] {Latent Output};
\draw[->] (Xq.east) -- (Concatenation.west);
\draw[->] (Concatenation.east) -- (DNN1.west);
\draw[->] (Concatenation.east) -- (DNN2.west);
\draw[->] (Concatenation.east) -- (DNN3.west);
\draw[->] (DNN1.east) -- (s1.west);
\draw[->] (DNN2.east) -- (s2.west);
\draw[->] (DNN3.east) -- (s3.west);
\draw[->] (s1.east) -- (rank1.west);
\draw[->] (s2.east) -- (rank2.west);
\draw[->] (s3.east) -- (rank3.west);
\draw[->] (rank1.east) -- (uncertainty.west);
\draw[->] (rank2.east) -- (uncertainty.west);
\draw[->] (rank3.east) -- (uncertainty.west);
\draw[->] (uncertainty.east) -- (mus.west);
\draw[->] (uncertainty.east) -- (sigmas.west);

\end{tikzpicture}
\caption{Skeleton of the proposed model with Deep Sets.}
\label{fig:proposeModelDeepSets}
\end{figure}

The  architecture of the surrogate $s_{\theta}$ with deep sets is given in Figure~\ref{fig:proposeModelDeepSets}.
For the complete architecture of the Deep Set node, please refer~\ref{fig:DeepSetArchitecture}.
We keep the node abstract for simplicity.

To rank a batch of query HP configurations $\textbf{X}_q$, first, the data points in the support set $\textbf{X}_s$, $\textbf{y}_s$ are passed through the deep-set.
Each $X_s^i$, $y_s^i$ are concatenated to get a single vector for input into the deep set to obtain a vector $X_s^i : y_s^i$.
This concatenation is passed through the Deep-set, we obtain the latent output $O_l$.
Please note that there is a single latent output from the deep set and all the support HP configuration points need to be input to it in one shot.

It can be argued that we could use the concatenation $X_s^i : rank(y_s^i)$ instead of directly using the actual value of the objective function. This is a way to concatenate a value from the ranking space instead of the output space. However there are two significant issues with this approach. First, the size of the support set is undefined; hence the range of the ranks can vary drastically. Second, we already know that values in the ranking space are relative. So even if we use the same number of support set elements, the same HP configuration can have different ranks in different support batches. Hence we believe a deeper study of the ranking space is required before it can be employed in the concatenation.

After obtaining the latent output $O_l$ from the deep-set,  it is concatenated with the query data points.
The concatenated result is passed through a Deep Neural Network to get their respective scores and, subsequently, their ranks.
For example if there is a batch of queries given by $\{X_{q_1},  X_{q_2},  X_{q_3} ...\}$,  and the latent output is given by $O_l$ then the concatenation yields
$$
\{O_l:X_{q_1},  O_l:X_{q_2},  O_l:X_{q_3} ...\}
$$
The whole batch of these concatenated vectors is then ranked using the rest of the architecture.

\subsubsection{Meta training}

Using the architecture with a deep-set during the HP optimization is very straightforward.
We can use the known HP configurations as a support set, and the queried HP configurations as the query set.
During meta-training,  however,  we have to divide the data into support and query sets.

In the meta-training, we randomly select 20\% of the batch size as a support set. The sampling is done without replacement.
For example, if the batch size is 100, we select 20 HP configurations as a support set.
Afterward,  we select the query points from the remaining choices (again without replacement). The number of query points depends on the batch size. If the batch size is 100, we will select 80 HP configurations randomly without replacement.
We do not simultaneously sample the support and query points from all the given meta datasets.
As already discussed in Section~\ref{sec:rlmetatraining}, we do a double sampling where we first select one meta-data in a search space. From the selected meta-data, we sample the support and query HP configurations.
We sample data from all the training and validation tasks as separate batches to calculate the average training and validation loss.



\iffalse
\chapter{Research Question}
The format of this is the same as that of experiments and Results.
Also known as Hypothesis.
\fi

\chapter{Experiments and Results}
\label{chap:HPOBExplanation}

In this chapter, we present the experiments we conducted and the results we obtained to some of the research questions that arose during the thesis.
We first understand the structure of the metadata used for meta-training,  meta-validation,  and meta-testing.
In the subsequent sections, we present the results obtained in detail.

\subsubsection{Meta-Data}
To compare our proposed ranking loss surrogate with other HPO surrogates, we would have to run the Bayesian optimization (BO) on a set of different machine learning models.
The BO would have to be run with the ranking loss surrogate and other surrogates.
Moreover, the optimum within the HP search space is different when the ML model is trained on different data sets.
In addition to this, due to the stochastic nature of ML models and the surrogate models, we would have to run the HPO multiple times for each dataset.

As one can see,  this evaluation, if done right from the training of the ML models, is not feasible.
To overcome this challenge, we use a meta dataset proposed by Pineda
 et al. called HPO-B~\cite{DBLP:journals/corr/abs-2106-06257} to benchmark our results in the thesis.
Using this benchmark, we do not need to train our ML models from scratch, as the metadata contains evaluations of multiple HP configurations for different ML models and datasets.
In the rest of the section, we discuss the organization of the benchmarking metadata.

HPO-B is a benchmark that is used for doing black box HPO.
Both transfer and non-transfer HPO surrogates can be studied using this benchmark.
The meta data in HPO-B comes in 3 versions namely \textbf{HPO-B-v1},  \textbf{HPO-B-v2}, and \textbf{HPO-B-v3}.
Of this \textbf{HPO-B-v3} contains distilled search spaces that have the most datasets. \textbf{HPO-B-v3} can be split into meta-training, meta-validation, and meta-test sets. 
Using the meta-training dataset, one can meta-learn an HPO surrogate.
Then the surrogate is evaluated and tested using the meta-validation and meta-test data, respectively.
The metadata in any split (training, testing, or validation) is organized in a JSON format. Figure~\ref{fig:metadataorganization} shows an illustration of this format.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
% roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[squarednode]      (ss2)                             {Search Space 2};
\node[squarednode]      (main)                   [left=of ss2]           {MetaData};
\node[squarednode]      (ss1)                    [above=of ss2]          {Search Space 1};
\node[squarednode]      (ss_)                       [below=of ss2]        {...};
\node[squarednode]      (ds2)                       [right=of ss1]        {Data Set 2};
\node[squarednode]      (ds1)                       [above=of ds2]        {Data Set 1};
\node[squarednode]      (ds_)                       [below=of ds2]        {...};
\node[squarednode]      (Xy)                       [right=of ds2]        {\{\textbf{X},  \textbf{y}\},  \textrm{Seeds}};

%Lines
\draw[->] (main.east) -- (ss2.west);
\draw[->] (main.east) -- (ss1.west);
\draw[->] (main.east) -- (ss_.west);
\draw[->] (ss1.east) -- (ds2.west);
\draw[->] (ss1.east) -- (ds1.west);
\draw[->] (ss1.east) -- (ds_.west);
\draw[->] (ds2.east) -- (Xy.west);
\end{tikzpicture}
\caption{Structure of the metadata in the HPO-B benchmark.}
\label{fig:metadataorganization}
\end{figure}

As shown in Figure~\ref{fig:metadataorganization}, the metadata consists of a list of HP search space ids. An HP search space id corresponds to a single ML model. Each search space further has multiple dataset ids.
These dataset ids correspond to different training data used to train ML models. Each dataset id contains an \textbf{X}, \textbf{y} pair and 5 seeds. \textbf{X} represents a set of HP configurations and
\textbf{y} represents their respective true evaluations. The Bayesian optimization used in our thesis starts with different initial known HP configurations. One initial configuration is called a seed. A seed gives a set of values (or indices in implementation) from the $\textbf{X}$,  $\textbf{y}$ pair.

We compare the results of ranking loss surrogates with transfer HPO surrogates and non-transfer HPO.
HPO-B can be used for analyzing both types of surrogates.
However, to cross-compare transfer and non-transfer HPO,  we only compare against \textbf{HPO-B-v3} test split as recommended by Pineda
 et al. ~\cite{DBLP:journals/corr/abs-2106-06257}.


\iffalse
\subsection{HPO-B Dataset}
To use the HPO-B code,  one must write function observe\_and\_suggest and observe\_and\_suggest\_continous to deal with discrete and continous optimisation case respectively.
Due to out of scope nature of the continous case,  in our implementation we assume that we are dealing only with the discrete case only.

The implementation of observe\_and\_suggest functions differs based on the surrogate model used by our problem.
\fi

\subsubsection{Experimental Protocol}
To test the performance of ranking loss surrogates at different stages of development, we used four main baseline methods - Random Search,  SMBO using a Gaussian surrogate, SMBO using Deep Ensembles, and FSBO.
We reused the Gaussian and Random surrogate implementations of HPO-B in the thesis.
However, the Deep Ensemble and FSBO surrogates were implemented from scratch.

In the given meta data if we take the set of all combinations in \textbf{HPO-B-v3} test split i.e \{Search Spaces $\times$ Metadata sets $\times$ Seeds\}, we get 430 HP optimizations.
We start with five initially known HP configurations given by the HPO-B seed in each optimization and run the evaluation cycle for 100 iterations.
This evaluation cycle is discussed in Algorithm~\ref{alg:deepEnsembleFinetuning} albeit for deep ensembles.

One optimization creates one array of incumbent best (highest) HP evaluations.
This incumbent array for different models is compared against each other to create a rank array for each surrogate.
For example, if at the evaluation step 23, the incumbent of surrogate $a$ has a higher value than of surrogate $b$, then the rank of model $a$ at step 23 is lower than model $b$ (if we consider lower rank to be better).
We get a list of rank arrays for every surrogate if we do this for all optimizations.
This rank array list is averaged to get a single averaged rank array.
Plotting this averaged rank array gives us a rank graph of a surrogate.
We use this \textbf{rank graph} to compare the different surrogates in this thesis.
For example, Figure~\ref{fig:RsGpRankGraph} compares the rank graph of 2 baselines.

\section{Baseline Results}
\label{sec:baselineResults}
The results of Deep Ensemble and FSBO baselines are discussed in this section.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/RsGpRankGraph}
    \caption{Sample rank graphs of Random and Gaussian surrogates.}
    \label{fig:RsGpRankGraph}
\end{figure}

\subsubsection{Deep Ensembles}

The Deep Ensembles (DE for short) we used contained five neural networks by default. Each neural network had two fully connected neural layers with 32 neurons each. All neural networks used the same architecture. 
We considered only the non-transfer case for DE. Hence we did not do any meta-training for them.
The training was done using the Adam optimizer with a full batch gradient. This is because there are very few data points during any optimization step. We train each neural network for 1000 epochs with a learning rate of 0.02 at every evaluation step in an HPO optimization.
We do not use adversarial examples as proposed by Lakshminarayanan
 et al. ~\cite{DeepEnsemblePaper} as the input HP search space may not be continuous.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/DEPerformance}
    \caption{Graphs showing performance of Deep Ensembles.}
    \label{fig:DEPerformance}
\end{figure}

Figure~\ref{fig:DEPerformance} shows the performance of various Deep Ensemble models.
We used the best results of Random and Gaussian surrogate baselines to compare how Deep ensembles fair in the HPO-B benchmark.
In addition to plotting the rank graph,  we also plot the average regret for every model.
In the average regret graph, the y-axis represents the regret on a log scale.
In Figure~\ref{fig:DEPerformance},  DE-M5 stands for Deep Ensembles with five neural networks, whereas DE-M10 stands for Deep Ensembles with ten neural networks.
An "RS" suffix in the legend denotes that the surrogates are reinitialized before finetuning at every evaluation step.

In this thesis, we studied the following main models of Deep Ensembles
\begin{itemize}
\item Deep Ensembles that finetune with no restarting.
\item Deep Ensembles that finetune with restart at every evaluation step.
\item Deep Ensembles with 5 and 10 neural networks.
\end{itemize}

We observed in our study that Deep Ensembles with restarts (at every evaluation step) give us better results than Deep Ensembles that do not do a restart.
This observation proves our hypothesis of the requirement of restarts as discussed in Section~\ref{sec:restart}.
Furthermore,  we observe that using deep ensembles with restarts gives us results comparable with Gaussian surrogates.
Increasing the number of neural network ensembles does not improve performance significantly.
We see this illustrated both in the ranking graph and the average regret graph.
We also use critical rank graphs developed by Pineda et al. ~\cite{pineda2021hpob}.
Critical graphs represent ranks at particular evaluation steps averaged across all HPO optimizations. 
Figure~\ref{fig:DERank100} illustrates a critical rank graph for the Deep Ensemble study.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank5}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank25}
\end{minipage}\par
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank50}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank100}
\end{minipage}
% \vskip\floatsep% normal separation between figures
    \caption{Critical rank graphs of Deep Ensembles at the $5^{th}$, $25^{th}$, $50^{th}$, and $100^{th}$ evaluation step.}
   \label{fig:DERank100}
\end{figure}

After the DE results analysis, several advantages and disadvantages of using the DE surrogate were found.
First, being a non-transfer HPO surrogate, DE can be applied to any continuous HPO problem.
Second,  as each neural network is independent of the others,  the neural networks can be trained in parallel.
Hence, DE can be scaled quickly based on the available computing power.
On the negative side,  since no meta-training is done, we have to fit the DE on the very few data points available during the HPO optimization.
This training on very few data points leads to overfitting DE surrogates because of their enormous representative capacity.
In addition, DE is incapable of modeling objective functions with discrete or semi-discrete domains.
The input data points to DE are assumed to lie in a continuous space.
Hence DE will not be able to model discrete and semi-discrete HP search space trivially.


\subsubsection{Few Shot Bayesian Optimization (FSBO)}

FSBO was selected as a baseline because it was the state-of-the-art baseline in the transfer HPO problem domain.
During our implementation of FSBO, we found that an FSBO model with four neural network layers of 32 neurons each gave us the best performance.
This architecture is similar to the Deep Ensemble architecture.
The difference between the architectures is that FSBO has four network layers instead of 2.
We used early stopping to get better training loss, as mentioned in Section~\ref{sec:FSBOBackground}.
The learning rate used for meta-training was 0.0001, and that used for finetuning was 0.03.
We finetuned our model for 500 epochs.
The learning rates of the kernel parameters and the neural network parameters were kept identical.
We used Adam optimizer for both training and finetuning.
However, during the finetuning phase, we used a cosine annealing schedular. The reason for using cosine annealing has been discussed in Section~\ref{sec:rlfinetune}.
Finally, we also do restarts for the FSBO domain with the restart mechanism discussed in Section~\ref{sec:FSBOBackground}.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.5]{images/FSBOPerformance}
    \caption{Rank graph showing performance of self implemented FSBO.}
    \label{fig:FSBOPerformance}
\end{figure}

Figure~\ref{fig:FSBOPerformance} shows the rank graph of FSBO compared to the best performing models of random search, GP and DE surrogates.
There is a slight difference in ranking graphs of previous surrogates because a few HPO optimizations were unsuccessful for the FSBO due to non-positive definite Matrix exceptions obtained during meta-training and finetuning (The GPytorch implementation threw this).
As we can see,  our implementation could not give us state-of-the-art results for FSBO.
Since these results are a correct yardstick to measure the ranking loss surrogate model, we used the results obtained by the FSBO implementation provided by Pineda et al. ~\cite{pineda2021hpob} in the rest of the thesis.


Figure~\ref{fig:FSBOBestPerformance} compares the best performing FSBO surrogate against the performance of other baseline surrogates.
As one can see from this figure, FSBO is an HPO model that outperforms the other HPO models both in the rank graph and the regret graph.
Moreover,  FSBO always has a better rank at every evaluation step.
This can be seen in the critical rank graphs illustrated in Figure~\ref{fig:FSBOBestPerformanceRank100}.
Using these baseline results as a reference, we discuss how the ranking loss surrogate model compares to these methods in detail in the following sections.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/FSBOBestPerformance}
    \caption{Rank and regret graph of best performing of FSBO.}
    \label{fig:FSBOBestPerformance}
\end{figure}


\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/FSBOBestPerformanceRank25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/FSBOBestPerformanceRank50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/FSBOBestPerformanceRank100}
    \caption{Critical rank graphs of baselines at different evaluation steps.}
   \label{fig:FSBOBestPerformanceRank100}
\end{figure}

\section{Ranking Loss model results: Research question results}

As discussed in Chapter~\ref{chap:ProposedIdea},  our proposed idea has two components - a ranking loss and a surrogate model.
In both these components, several research questions need to be studied.
This section discusses the results or answers to these research questions in different sub-sections separately.
In each section, we present an ablation study to highlight the essential components of our proposed idea.

For uniformity, we use the following strategy - We will use the same number of neural networks and the same neural network architecture in our study.
We also try to keep the learning rate similar for similar components.
For different components, however, we use different learning rates.
For example, learning rates during meta-training and finetuning are different.
Similarly, each loss function may require a different learning rate to perform the best as the HP response surfaces change with the used loss function.
Like in the best case of Deep Ensembles, we use ten neural networks in our ensemble. Each neural network has two 32-neuron layers.

\subsection{Meta training results}
In this section, we discuss the observations we made during the meta-training of the proposed model. The effect of finetuning on the optimization results is discussed in detail in Section~\ref{sec:EffectOfFineTuningResults}. Please note that this meta-training is required only for the transfer HPO case. For the non-transfer case, we directly do a finetuning with the evaluated HP configurations during the HP optimization.
The two main architectures we study in this thesis are the Basic Scoring model and the Basic Scoring model with Deep Sets. 

The training of the Basic Scoring Model (without Deep sets) obtained relatively smooth loss curves. This is illustrated using the loss curves of the search space with id "4796" in Figure~\ref{fig:loss4796BasicScoringModel}. The "input dim = 3" in the image title specifies that the dimensionality of this HP search space is 3. After adding deep sets to the model, the loss curves become jumpy. This is illustrated in Figure~\ref{fig:loss4796DeepSets} using the same search space. These jumpy loss curves are expected because adding deep sets increases the architecture's representation capacity (and the parameter space). A bigger parameter space results in more noisy gradients during the stochastic gradient descent. 

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/loss4796BasicScoringModel}
\caption{Loss curves for the basic scoring model.}
    \label{fig:loss4796BasicScoringModel}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/loss4796DeepSets}
\caption{Loss curves for a model with deep sets.}
    \label{fig:loss4796DeepSets}
\end{minipage}
\end{figure}

The second thing we observed is that sometimes the validation loss gets significantly worse during the training. Figure~\ref{fig:NegativeTransferLearning} illustrates this sort of behavior. One reason for this behavior could be that the validation task is not similar to the training task.
There is some negative knowledge transfer from the meta-training phase to the meta-validation phase. This type of learning is called Negative transfer learning and is discussed in detail in Section~\ref{sec:NegativeTransferLearning}.
Due to the negative transfer learning issue, we could not employ an early stopping mechanism like in the case of FSBO baseline implementation for the ranking loss surrogates.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.40]{images/NegativeTransferLearning}
    \caption{An instance of negative transfer learning.}
    \label{fig:NegativeTransferLearning}
\end{figure}



\iffalse
\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/GoodLossCurve4796}
\caption{Good training loss curve (search space ID 4796)}
    \label{fig:GoodLossCurve4796}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/NegativeLearning5860}
\caption{Bad training loss curve (search space ID 5896)}
    \label{fig:NegativeLearning5860}
\end{minipage}
\end{figure}


\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/RLEvaluationBasicScoring}
    \caption{Benchmarking basic scoring model trained using ListMLE}
    \label{fig:RLEvaluationBasicScoring}
\end{figure}
\fi

\subsection{Which loss functions are better?}

In this section, we discuss the following question - are ranking losses better for HPO than regression ones? Secondly, which ranking loss is better for HPO: pointwise,  pairwise, or listwise loss?
We use the Deep Ensemble baseline implementation proposed by Lakshminarayanan et al. ~\cite{DeepEnsemblePaper} as a reference. We use subset regression,  RankNet, and ListMLE loss functions for pointwise, pairwise, and listwise ranking losses, respectively. 

\subsubsection{Non-transfer HPO}
We first test the non-transfer HPO case where the surrogate models are not meta-trained. We only do finetune (or training) during the evaluation phase. We also reinitialize the models before every evaluation step. We have discussed the reason for this in-depth in Section~\ref{sec:rlfinetune}.
We train each neural network for 1000 epochs at every evaluation step. The learning rate is 0.02, and we do not use cosine annealing.
Using cosine annealing here did not make sense since there is no guarantee that the randomly initialized model is near a local optimum.
Hence, we need a bigger learning rate to find the optima quickly.
We try to keep all the parameters the same as the best case of DE results (Our reference method for the non-transfer HPO case).

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q1AblationNonTransfer}
    \caption{Comparison of Ranking losses with regression losses (Non-transfer HPO case).}
    \label{fig:Q1AblationNonTransfer}
\end{figure}

Firstly, we see that the average regret for both the regression losses is similar. This result is expected because the only significant difference between the two models is how uncertainty is modeled. Secondly,  we see from Figure~\ref{fig:Q1AblationNonTransfer} that the models trained with ranking losses are better at the beginning of the optimization steps cycle than the regression losses. This can be seen in the critical rank graphs of Figure~\ref{fig:Q1AblationNonTransferRank100} as well. However, the pointwise loss function very quickly deteriorates in performance in the long run. The pointwise loss function tries to learn the rank directly from the input. The learned ranks also depend on the other HP configurations in a given batch. Hence this loss function does not train the model to learn relative differences in the HP configurations.
\textbf{In conclusion, we find that listwise ranking losses are better than other losses in the initial steps. However, their performance deteriorates at the later end of the optimization}.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/Q1AblationNonTransferRank@25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/Q1AblationNonTransferRank@50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/Q1AblationNonTransferRank@100}
    \caption{Critical rank graph of the non-transfer HPO methods.}
   \label{fig:Q1AblationNonTransferRank100}
\end{figure}


\subsubsection{Transfer HPO}
In the transfer HPO case,  we first train the models with the meta-training data given by the HPO-B data set and then store our models on persistent storage.
We train the models for 5000 epochs with a fixed learning rate of 0.001.
We used a batch size of 100 for all models. We used a list size of 100 for the listwise loss functions.
However, we use cosine annealing during finetuning for the reasons discussed in Section~\ref{sec:rlfinetune}.
It is because there is a very high chance that the learned model is close to any one of the local optima (the exception to this is the negative learning rate case discussed later).

The selection of batches of HP configuration for training is not trivial.
For any given search space,  we have data on multiple tasks.
As the distribution of HP configurations may differ for different tasks, we need to learn to rank the HP configurations against other configurations within the same task.
Hence, for each step within an epoch (i.e., for the number of batches within the epoch),  we randomly select a task (i.e., meta-dataset) and then select the HP configurations within this task.
This sampling mechanism is the double sampling mechanism we discussed in Section~\ref{sec:rlmetatraining}.
Double sampling is not required during finetuning as we are finetuning only for a particular task.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q1AblationTransfer}
    \caption{Comparison of Ranking losses with regression losses (Transfer case).}
    \label{fig:Q1AblationTransfer}
\end{figure}

Figure~\ref{fig:Q1AblationTransfer} compares the average regret and the ranking graph for the regression and ranking losses for the transfer case.
The "-T" suffix in the legends signifies that the corresponding plot is for a transfer HPO case.
As we can see here,  the listwise ranking losses are far superior to other losses.
It is also interesting to see that the average regret of the listwise losses is similar to the regression losses in the transfer case.
To check the overall performance of the listwise loss models, we compare this model with the worst,  medium, and best performing surrogates as sentinels for the study.
The random selection strategy is the worst performing,  whereas FSBO is the best performing SMBO surrogate. We use a GP surrogate to represent a surrogate with a medium performance.
Figure~\ref{fig:Q1FinalAblation} shows this ablation.
The listwise loss function functions better than the state-of-the-art transfer HPO model "FSBO" in the first 15 to 20 evaluation steps.
\textbf{We conclude that listwise ranking losses are superior to both the regression and the other ranking losses in the transfer HPO case.  However, the performance of the listwise loss surrogate is not on par with state-of-the-art transfer HPO methods like FSBO in the long run.}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q1FinalAblation}
    \caption{Comparison of listwise (transfer) loss with baseline HPO surrogates.}
    \label{fig:Q1FinalAblation}
\end{figure}


\subsection{Does adding weighting to listwise loss improve the results?}

In Section~\ref{sec:positionEnhancedRanking} we discussed the requirement for weighting for our problem.
This section presents a short ablation to study its effect on our models.
We do not separate the transfer and non-transfer cases here.
We use inverse log weighting for the weighting in our case.
From Figure~\ref{fig:Q2Ablation}, we see that both in the case of transfer HPO and non-transfer HPO, the weighting positively impacted the performance. Moreover, the models trained with weighted listwise losses are better at every optimization cycle step.

Even though the difference in the average regret is not significant in the 4 cases,  the ranking graph differences are pretty significant. One thing to note here is that when we use listwise loss,  we train and finetune with the same weighted loss. We do not use a different weighting (like the inverse linear weighting for training and inverse log weighting for the meta training). Moreover, we use the double sampling mechanism discussed above during meta-training.
\textbf{To conclude,  adding weighting functionality to listwise losses improves the performance of the learned surrogate both in the transfer and non-transfer HPO case.}


\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q2Ablation}
    \caption{Comparison of listwise losses with theg weighting strategy.}
    \label{fig:Q2Ablation}
\end{figure}


\subsection{Does adding deep sets into the architecture improve performance?}

We found in our research that our model could be improved by making it context-aware. We used an architecture with deep sets described in Section~\ref{sec:DeepSetWithModel}. As previously discussed, the latent result of the deep-set is used as a context for the scoring model. To meta-train this architecture,  we needed to change the training and sampling mechanisms.

For sampling, we used a support set of data points, i.e., \{$X_s$, $y_s$\} that is 20\% of the actual batch size. The concept of double sampling is also used in this case. For each double sampling, we sample first the task.
Then the support points are sampled without replacement from this task.
Subsequently, we sample the query points $X_q$ based on the batch size and list size (By default, 100 for both).
During meta tuning, we must also have a support and query set for training.
We again use 20\% of the points as support points and the rest as query points during the finetuning process.

One major issue when training with deep sets is that we cannot train the neural networks separately.
It is because of the common backbone in ensemble architecture.
In order to keep the training as separate as possible, we initialize the neural networks separately. After that, we calculate the loss of each neural network separately with the same batch data. Then we aggregate the losses and back-propagate the aggregated loss through the whole architecture. Note that we do not aggregate the scores of the neural networks but the ranking losses obtained from each scoring neural network.

Figure~\ref{fig:Q3Ablation} shows the results of this study. In the non-transfer case,  the model becomes terrible. The primary reason for this is that the complexity of the architecture is very high, and it needs a considerable amount of data to train. This data is absent during the HPO optimization cycle. However,  in the case of transfer learning,  we see an enormous improvement when we use deep sets. It makes intuitive sense because each task is different from the other. The embedding of this information, or rather the difference in information, helps the model distinguish the tasks and pre-condition the learning process. 

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q3Ablation}
    \caption{Comparing the effect of adding Deep Sets to the proposed model.}
    \label{fig:Q3Ablation}
\end{figure}

We compare the best results obtained so far with the sentinel performances of GP,  FSBO,  and Random search. Figure~\ref{fig:Q3AblationFinal} illustrates this.  Two interesting observations can be made from our results. First,  our model performs significantly well in the first evaluation steps, and the performance later deteriorates w.r.t to state-of-the-art results. Hence,  this model can be used in any transfer HPO case with limited evaluation steps due to the computation budget. Secondly,   the average regret is also significantly better in the initial phase. Hence, we can unequivocally state that our proposed model is a good candidate to consider in any transfer HPO problem. 
We also present in Figure~\ref{fig:Q3AblationFinalRank100} the critical rank graphs for reference.

\textbf{To conclude, after integrating deep sets into the surrogate architecture,  the surrogate improves the performance of the transfer HPO case very significantly. However, the performance deteriorates in the non-transfer case. Hence we recommend using deep sets with meta-training to get the best results.}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/Q3AblationFinal}
    \caption{Comparison of the best proposed model with other baseline HPO surrogates.}
    \label{fig:Q3AblationFinal}
\end{figure}



\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/Q3AblationFinalRank@25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/Q3AblationFinalRank@50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/Q3AblationFinalRank@100}
    \caption{Critical rank graph of the listwise weighted transfer learning with deep sets with sentinel performances}
   \label{fig:Q3AblationFinalRank100}
\end{figure}


\iffalse

This is because less relearning happens in finetuning due to the addition of context to our model.

During the training however,  we found that the loss curves were very jumpy.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DeepSetLoss4796}
\caption{Loss curve for model with deep set (SSID: 4796)}
    \label{fig:DeepSetLoss4796}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DeepSetLoss5527}
\caption{Loss curve for model with deep set (SSID: 5527)}
    \label{fig:DeepSetLoss5527}
\end{minipage}
\end{figure}

Figure~\ref{fig:DeepSetLoss4796} and Figure~\ref{fig:DeepSetLoss5527} shows this clearly.
We think that this is because of very high representation capacity which is a consequence of using the deep set architecture.

The results of this training and finetuning our model with deep sets are shown in Figure~\ref{fig:RLDeepSetevaluation}.
In the figure "raw" means that there was no finetuning performed and the trained model was used as is in the the evaluation/optimization cycle.
One advantage we see with this model is that whether finetuning is done or not,  the first few evaluations are better than the state of the art results.
This can be seen in the Rank@5 graph critical graph given in Figure~\ref{fig:DeepSetRank5}.
In the later evaluation steps, naturally,  using finetuning makes gives better performance.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.35]{images/DeepSetRank5}
    \caption{Critical rank graph for Model with Deep Set.}
    \label{fig:DeepSetRank5}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/RLDeepSetevaluation}
    \caption{Benchmarking for the Deep set evaluation data}
    \label{fig:RLDeepSetevaluation}
\end{figure}
\fi

\iffalse
The finetuned results is better than all the baselines except FSBO.
In fact,  it is better than FSBO in the first few evaluation steps.
This can be seen in Figure~\ref{fig:RLDeepSetWeightedRank25}.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/RLDeepSetWeighted}
    \caption{Ablation for weighted loss}
    \label{fig:RLDeepSetWeighted}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.35]{images/RLDeepSetWeightedRank25}
    \caption{Critical rank graph for weighting ablation.}
    \label{fig:RLDeepSetWeightedRank25}
\end{figure}

In this section, we evaluate our built model chronologically, first using the basic scorer model,  then making the model context-aware, and then adding uncertainty capability to the model.
\fi

\subsection{Does using finetuning for the transfer learning surrogate improve performance?}
\label{sec:EffectOfFineTuningResults}
There are two ways of using the meta-trained ranking loss surrogate model. One way is to use it directly without changing it in the optimization process. The other is to finetune it at every evaluation step of the optimization process. We have always used the latter in our ablation studies (or answers to research questions) until now.
In this section, we will see how the performance changes when no finetuning is done during the optimization process.
We take the best model so far to study how finetuning affects performance. Then we run the optimization process with and without finetuning and plot the ranking and regret graphs.

If we do not use any finetuning, selecting the HP configurations for the evaluation process boils down to ranking the set of all unknown HP configurations. Then we select the ranked HP configurations one by one from this ranked list. Finetuning can be understood as a more sophisticated form of the optimization process in which the ranking loss surrogate tries to learn/understand the context from the known HP configurations. Figure~\ref{fig:FineTuningAblation} shows the graphs of models with and without finetuning.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/FineTuningAblation}
    \caption{Effect of finetuning on surrogate performance.}
    \label{fig:FineTuningAblation}
\end{figure}

From Figure~\ref{fig:FineTuningAblation} one can see that finetuning does help make the model get better results in the long run.
The "-Raw" suffix signifies that no finetuning is performed using the trained model.
One fascinating result we see is that without any finetuning, the ranking loss surrogate shows good performance in the first evaluation steps.
This is because the surrogate model learns the best HP configuration found in the training metadata during meta-training.
Hence it selects these HP configurations in the first evaluation steps.
It makes intuitive sense because this behavior is shown by experts who try out the best HP configuration seen in their experience in the first evaluations. On average, this strategy works very well; hence the first evaluations were good for the non-finetuned surrogate model.
However, as the optimization steps proceed, the surrogate can condition its selection on more data. This conditioning is only done when the model is finetuned. 

\textbf{To conclude, we find that finetuning plays a significant role in the improvement of transfer HPO performance. Even though not using finetuning may perform well in the first few steps of the optimization,  the finetuned surrogate works better in the long run.}

\subsection{How does the proposed model compare to SOTA surrogates?}
After analyzing our proposed model, one natural question is how it performs against the state-of-the-art HPO surrogates. We compare our results with the best-performing transfer and non-transfer HPO surrogates to answer this question.

Figure~\ref{fig:NonTransferSOTA} compares the results with some prominent state-of-the-art non-transfer HPO surrogates. This figure shows that the non-transfer version of the proposed model shows comparable performance to the Gaussian Process surrogate. This is the case both in the rank graph and the average regret. We also see that the proposed model performs very well in the initial few optimization steps. This can be seen in the critical graphs shown in figure~\ref{fig:SOTANonTransferCriticalRankedGraph}. This property is also seen in its transfer variant.

\iffalse
Interestingly, the average regret of the proposed model at the end of optimizations is not very good compared to non-transfer surrogates.
Since the average regret gives a biased estimation of the results and the rank graph is a more accurate picture,  we conclude here that the proposed ranking loss surrogate model is better than the non-transfer surrogates.
\fi

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/NonTransferSOAT}
    \caption{Comparing our results against SOTA non-transfer HPO surrogates.}
    \label{fig:NonTransferSOTA}
\end{figure}

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/NonTransferSOTARank@25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/NonTransferSOTARank@50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/NonTransferSOTARank@100}
    \caption{Critical rank graphs of the proposed model with SOTA non-transfer HPO surrogates.}
   \label{fig:SOTANonTransferCriticalRankedGraph}
\end{figure}


Figure~\ref{fig:TransferSOTA} compares the results with other state-of-the-art transfer HPO surrogates. As we see from this figure, the strength of using the proposed model is that it outperforms the state-of-the-art results in the first evaluations in the optimization procedure. Moreover, it outperforms RGPE, which also uses the concept of ranking making our model unequivocally better performing than other ranking methods. This can also be seen in the critical rank graphs shown in Figure~\ref{fig:SOTACriticalRankedGraph}.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/TransferSOAT}
    \caption{Comparing results against SOTA transfer HPO surrogates.}
    \label{fig:TransferSOTA}
\end{figure}

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/TransferSOATRank@25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/TransferSOATRank@50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/TransferSOATRank@100}
    \caption{Comparing critical rank graphs against SOTA transfer HPO surrogates.}
   \label{fig:SOTACriticalRankedGraph}
\end{figure}

\textbf{To conclude, using a listwise weighted surrogate with deep sets gives results on par with state-of-the-art HPO methods both in the transfer and non-transfer case in the initial optimization steps. Its performance, however, degrades in the later steps.}

\subsection{Final ablation}
In this section, we show the ablation of all the different ranking loss surrogates that were used in the previous ablations. Figure~\ref{fig:FinalAblation} shows this ablation. In this figure, we can see that making the model a transfer model, weighting the ranking loss, and adding deep sets all significantly improved the proposed model's performance. 
The biggest strength of the proposed model is that it is excellent in the initial 30-40 steps of the optimization process (compared to state-of-the-art HPO surrogates). This is crucial for a sequential process like SMBO because selecting a good HP configuration in a particular step affects the selection of subsequent HP configurations.
The second strength of the model is that there is a possibility of reducing negative learning from occurring because of the context encoded using the deep sets.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.22]{images/FinalAblation}
    \caption{Ablation of all ranking loss surrogate models studied in this thesis.}
    \label{fig:FinalAblation}
\end{figure}

\subsection{UCB and EI in the ranking space}
In the previous experiments, we used average rank as the acquisition function. This section briefly discusses the results of 2 more sophisticated acquisition functions applied to the ranking space - Upper Confidence Bound (UCB) and Expected Improvement (EI).

In the case of UCB, the integration is simple. We use the ranks obtained from the ensemble of rankers to calculate the mean and the standard deviation rank of each HP configuration. Using this mean and the standard deviation, we can calculate the UCB score. However, when we use the EI as an acquisition function, we must pass the incumbent HP configuration in both the support and query sets. It is important to note that the effect of passing the same configuration in the support and query set has not been studied in this thesis. A detailed analysis of acquisition functions in the ranking space is out of the scope for this thesis, as mentioned in section~\ref{sec:AcquisitionFunctionInRankingSpace}.

We only use the state-of-the-art transfer HPO method (i.e., FSBO) for this ablation study as a reference. We compare the learned surrogates that use the Average (rank), UCB, and EI as acquisition functions.
Figure~\ref{fig:acqusitionRanking} shows the ranked graph comparing these models.
All models in this figure~\ref{fig:acqusitionRanking} are meta-trained with weighted listwise ranking loss. Hence they are transfer HPO methods. They also use deep sets in their architecture.
\textbf{We conclude that both UCB and EI acquisition functions are superior even in the ranking space. Further, the proposed surrogate model with EI  acquisition performs better than FSBO in all optimization steps.}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.22]{images/acqusitionRanking}
    \caption{Ablation of different acqusition functions in the ranking space.}
    \label{fig:acqusitionRanking}
\end{figure}

\iffalse
Data used in the experiment
Infrastructure used
(That is the protocol)
Results should be structured like hypothesis.

experiment and results
protocol
data split…

First show the resutls of implementation of GP (M = 5 and M =10 )
Benchmark this...

Next with DKT (Benchmark this)

Next show the best results obtained by architecture.
\fi



\chapter{Conclusion}

In this thesis, we studied how to use the concept of ranking with Sequential Model-Based Optimization (SMBO) to improve hyperparameter optimization. The HPO method we researched was a transfer learning method with two canonical parts: ranking loss functions and context-aware HPO surrogates. Firstly, we formulated the problem of selecting HP configurations within the SMBO algorithm as a ranking problem. Then, we researched the effects of using various ranking losses to train and finetune the HPO surrogate. We also studied the effect of adding weighting to the ranking loss. Finally, we integrated deep sets into the surrogate model architecture to make it context-aware.

We found in our study that the loss function used to learn an HPO surrogate significantly affects its performance. Among the ranking loss functions, listwise ranking loss showed the best performance during the HPO optimizations. After adding weighting to the listwise ranking loss function, the proposed model's performance further improved. Adding deep-sets to the surrogate made its performance on par with the performance of state-of-the-art transfer HPO methods. However, the results were the best only when we used the ranking loss surrogate in the transfer learning scenario.

One of the key takeaways of this thesis is that it is crucial to differentiate the ranking space from the output space. The ranking loss surrogates performed well because they learn and function in the ranking space rather than the output space. Hence we also conclude that working in the ranking space is better than working in the output space. Moreover, working in the ranking space is helpful in a problem domain like the HPO. The output ranges of the evaluated HPO objective functions may differ for different tasks. By working in the ranking space, we avoid normalizing meta-data values usually done by other transfer HPO methods.

We conclude that a context-aware model trained with a weighted listwise loss function has the potential to give excellent hyperparameter optimization results. We encourage the reader to use this model for any HPO. We also believe that doing more research on listwise losses in the HPO domain could give promising results. The concepts developed in this thesis and any further research on listwise losses could also aid other areas like Information Retrieval, which heavily uses ranking losses. In the rest of this chapter, we discuss the proposed model's advantages and limitations. Finally, we end this report by suggesting some interesting future research directions.

\section{Advantages}
\label{sec:advantages}

First and foremost, one of the main advantages of the proposed surrogate model is that it is a transfer HPO surrogate. Training the surrogate on previously available metadata makes the surrogate perform significantly better than other non-transfer HPO surrogates.

Secondly, we also found that the sampling mechanism required for the listwise loss function had the upper hand compared to other loss functions. Because a set has a massive number of possible subsets, the number of data instances sampled for the listwise loss is very high. 
For example, if we have a metadata set of 100 HP observations, i.e., 100 ($\textbf{x}$,$y$) pairs, and a training list size of 15, then a total of ${100 \choose 15}$ training lists can be sampled from the metadata.
This is a significant advantage because, during an HPO optimization, the known HP configurations and their evaluations are very scarce.
We can also increase the surrogate model's complexity without worrying much about the available data.

Another significant advantage of the proposed method is that training using ranking losses inherently dampens any uncertainty of the HPO objective function's output.
This is because ranking losses work in the ranking space. The range of the actual scores in the metadata is irrelevant to the training procedure.
As the order of the known evaluations is the only relevant thing, minor variations in the objective function's output do not affect the loss's training. This also allows us to manipulate the range of our surrogate's scores based on our requirements.


\section{Limitations}
\label{sec:Limitations}
\label{sec:NegativeTransferLearning}
Even though the proposed method gives state-of-the-art results, it has some limitations that need further study. In this section, we critically analyze the limitations of the proposed model.

We found in our research that for some search spaces, the ranking loss surrogate's validation errors did not reduce during its training. In fact, they increased.
Figure~\ref{fig:NegativeLearning} illustrates this for the search space id 5527.
This deterioration of validation loss signifies that negative transfer learning is occuring~\cite{Weiss2016}.
Negative transfer learning occurs when the source tasks set differs from the target tasks. In this example, the meta-training task is different from the meta-validation task.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/NegativeLearning}
    \caption{Training and validation loss curves obtained for the training of search space 5527.}
    \label{fig:NegativeLearning}
\end{figure}

Out of the 16 search spaces in HPO-B, the search spaces with ID 5527, 5889, 5906, 6766, and 7609 showed negative transfer learning curves during the training of our proposed model. 
We conclude that it is inevitable for any transfer learning method to give bad results for these search spaces.

\iffalse
\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.20]{images/NegativeTransferLearingRankWise}
    \caption{Rank graphs per search space for all the SOTA transfer learning methods.}
    \label{fig:NegativeTransferLearingRankWise}
\end{figure}
\fi

This problem should occur only in the cases where the model is context-free.
Even though our ranking loss surrogate is made context-aware by integrating deep sets into its architecture, we still have the problem of negative transfer learning.
One remedy for getting around this problem is to finetune the model with more epochs.
Nevertheless, we cannot guarantee that the finetuning will re-learn from the small number of observed HP configurations during the optimization cycle.
Since the testing/evaluation task may be similar to the training task, we could also not do any early stopping during meta-training.

Another disadvantage of the proposed model is that we ignore the sorting functionality necessary to complete the ranking process during our surrogate training. This is because sorting is non-differentiable.
Due to this issue, the complete ranking process is not learned with the listwise loss. This means that we do not solve the complete ranking problem at hand conceptually.

Our loss function uses the exponentiation function as the strictly increasing positive function.
The usage of the exponentiation function puts a practical restriction on the range of the relevance scores. This is because standard floating-point implementations in computer languages only have a fixed range of values they can represent.
These floating-point implementations may not have the range to map the actual HPO objective function's range completely.
Hence, more research is required to find out better strictly increasing functions.

Another problem with our model is that the meta-trained model may be biased towards one task due to the larger availability of data from this task.
Double sampling helps us eliminate this issue by sampling all tasks with equal probability.
However, the consequence of double sampling is that the meta-training overfits the surrogate to the tasks with lesser data because their data is overused during training. More studies must be conducted to find a mechanism for fair metadata sampling across tasks.

\iffalse
\begin{itemize}
\item The quality requirement of the uncertainty estimation and need for it is not always the same.
The search spaces that have less data need more uncertainty and vis versa.
 The learnt model is extremely sensitive to the learning rate and the number of epochs.
 \item The range controller function we use in our model is $\tanh$ because of the function's smoothness.
However,  if the output relevance scores of 2 HP configurations are either at the positive extreme or negative extreme,  then it may be difficult to distinguish between them.
This can be dampenned by tuning the $k$ and $\alpha$ parameters as discussed in Section~\ref{sec:BasicScoringModelDNN},  but this does have practical limits due to the floating point implementations in computer languages.
\item It is during the meta training that all the meta training tasks / meta datasets have the same output range for simplicity.
This is the case of all outputs HPO-B.
If there is any other meta data used,  the outputs of the different meta data (tasks) should be normalized before being used in the training process.
\end{itemize}
\fi
    

\iffalse
\section{Evaluation}
\subsection{Testing}
explain how a ranking graph works ar implemented
Explain the regret rank@ some location.

\subsection{Ablation}

Result tabulation of case study: sorting:
1.  Within range
2.   Outside range 
mean of 3 times should be written.

show the results of raw without deep set.

Next show different strategies used for building the ranking loss model one step at a time.
First with only scorer.
then with deep set.
Then with raw deep set
finetuning and deep set
adding uncertainty

Checking the early stop and hypothesing why is was wrong.

box plot variation of each of the scorers... for 1 or more data sets?

show results of independent training and training with output restriction

what about training independently,   this requires normalization. 
as explained by sebastian.
\fi

\section{Further research work}

During our thesis, we found various topics that could be taken up for future research. For example, in section~\ref{sec:AcquisitionFunctionInRankingSpace} we proposed that further research on acquisition functions is necessary due to the intricacies of the ranking space. Similarly, we already know from section~\ref{sec:Limitations} that the listwise loss function does not optimize the sorting function.
Interestingly, the inclusion of sorting functionality in a loss function has been proposed and studied in Swezey et al. ~\cite{PiRank}.
One research direction would be to incorporate the ideas of PiRank in order to see if there is any improvement in the proposed model.

Lakshminarayanan et al.~\cite{DeepEnsemblePaper} propose a new loss function that inherently learns an output's mean and variance. Our idea, on the other hand, predicts definite scores. After that, it predicts the mean and variance in the ranks induced by the scores. One exciting idea we could not explore due to the scope of the thesis is the integration of the loss function proposed by Lakshminarayanan et al. and ranking losses. Doing a theoretical and empirical analysis of such an integration could be one research direction.
It would be interesting to see the effect of this integration on HPO problems with less metadata because Deep Ensembles by themselves perform relatively well in a silo.

This thesis has primarily focused on how to select HP configurations from a set of discrete HP configurations.
Hence the model does not deal with continuous search spaces directly.
For working with continuous HP search spaces,  we can divide the space  (in the Euclidean sense) into hyper-volumes and use 1 HP configuration to represent the whole hyper-volume.
Then we may select the best region, subdivide the space, and continue the process until we reach a particular granularity.
This method is just a workaround to the main problem, and we also cannot guarantee that an optimum will be found.
Hence, further research must be done on formulating the ranking loss function for continuous search spaces.

Lastly, novel HPO surrogates that this thesis has not explored can be implemented. It would not only further the research empirically, but also ideas used in other baselines could be incorporated to improve the model.





\iffalse
\subsubsection{Training with mean and restricted output}
Implementation yet to be done
\fi

\iffalse
Paper: https://arxiv.org/pdf/2012.06731.pdf
       (Impt Ref) https://arxiv.org/pdf/1903.08850.pdf

Key Idea:
    Use permutation matrices to represent sorting. Learn the matrix with the loss function.
    [0 1 0] [x]    [y]
    |1 0 0| |y|  = |x|
    [0 0 1] [z]    [z]
    Permutation Matrices are not smooth in the input space Hence relaxation is necessary for differentiability
    Relaxation is done by using a double schocastic matrices i.e all row and column matrices sum to 1.

    Unimodal
    Double schochastic

The idea of Permutation Matrices is taken from Neural sort - Neural sort (Impt ref)

PLACKETT-LUCE DISTRIBUTIONS -> Very important to explain the rationale about using score as a probability
    measure for our scoring function (Check section 2.1 of paper 2 (Neural Sort))
\fi 


\bibliography{references}
\bibliographystyle{plain}


\appendix
\newpage
\pagenumbering{roman}
\chapter{Controlling the output range}
\label{chap:OutputRangeControl}

Lets take the listwise loss function $L_{mle}$ which is nothing but maximum log-likelihood estimation. 
In our surrogate models, we use the strictly positive increasing function $\exp$ as proposed by Fen et. al.~\cite{listmlepaper}.
Hence, our $L_{mle}$ becomes 
\begin{equation}\label{eq:overflowissue}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\exp(s(\pi^*_j))}{ \sum\limits_{t=j}^k \exp(s(\pi^*_k))}
\end{equation}

Even though theoretically, the score range does not need to be restricted, we have practical limitations due to implementation constraints of floating-point numbers.
Due to the exponentiation in our loss function, if the score range is too high,  the values in equation~\ref{eq:overflowissue} may overflow.
On the other hand, if it is too small, the values may underflow.
In both these cases, we are bound to get NAN (Not A Number) exceptions when executing our implementations.
We would also get similar issues for the pairwise loss function because we use exponentiation for calculating the probability, as shown in equation~\ref{eq:pairwiseprobability}.
One way to eliminate this issue is to use the log-sum-exp trick. This trick is used in the ListMLE implementation given by Pobrotyn
 et. al. ~\cite{Pobrotyn2020ContextAwareLT} and we use this implementation in our thesis.

However, if the output score is used anywhere other than ranking, we would need to control its range.
We can do this by passing the output of the deep neural network through a $\tanh$ function.
If we would like to strictly limit the range of our function between $[-k,  k]$,  then we can pass the output score through the following function
\begin{equation}\label{eq:tanhEquation}
k * \tanh(\alpha * s(\textbf{X}))
\end{equation}

Here $\alpha$ is the smoothness factor which is inversely proportional to the smoothness of the $\tanh$ graph~\cite{tanhstackoverflowanswer}.
Figure~\ref{fig:tanhGraph} shows how to vary the smoothness and range of the output using $\alpha$ and $k$ respectively.
\iffalse
We used $k=2$ and $\alpha=0.01$ for our scorer.
\fi

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/tanhGraph}
    \caption{Effect of varying $k$ and $\alpha$ in equation~\ref{eq:tanhEquation}.}
    \label{fig:tanhGraph}
\end{figure}


\chapter{Case study: Inverse mapping}
\label{chap:caseStudy}

This case study examines how well an ML model performs if trained using the ListMLE loss function. For this, we consider a toy problem of sorting a list of real numbers in descending order. The main problem that we try to study here is - Is it possible to train a model using the ListMLE loss function such that it learns inverse mapping of points on a number line.

Consider numbers sampled from the range $[k,  p]$ where $k,p > 0$ and  $k,p \in \mathbb{R}$.
If we take 2 numbers $x_1, x_2 \in [k,  p]$,  we need to learn a mapping $s: x \mapsto \mathbb{R}$ such that $s(x_1) \leq s(x_2)$ when $x_1 \geq x_2$.
Consequently, we could sort the numbers based on the output score of the model to obtain a descending sorted order.

Consider a list $l = \{x_1, x_2, ..., x_n\}$ where $x_i \in [1, 100]$.
Let $s(x \mid \theta) \mapsto \mathbb{R}$ be our scoring function parametrised by $\theta$.
Here, one list contains $n$ data points sampled from $[1, 100]$.
Then the loss function we would use to learn our scorer is
\begin{equation}
\underset{\theta}{\rm argmin} \quad L_{mle}(s(l \mid \theta),  - k * l)
\end{equation}
where $k \in \mathbb{R}$.
Note that the second parameter of listwise loss function is scale-invariant; hence scaling the list does not affect the loss output (Section~\ref{sec:listMLE}).
To keep the output score in a defined range, we utilized the range controlling mechanism discussed in Appendex~\ref{chap:OutputRangeControl}.

We sample the testing data from the following ranges to evaluate our learned model:
\begin{itemize}
\item \textbf{In range}: The Same range as seen during training i.e $[1, 100]$.
\item \textbf{Out of range}: A completely different range as seen during training i.e $[-100, -1]$.
\item \textbf{Hybrid range}: A range that is partially seen and partially unseen i.e $[-50, 50]$.
\end{itemize} 
 
During training, a batch size of 100 lists was used. The training was done with values sampled inside the $[1, 100]$ range.
For testing, we sample data from inside, outside, and hybrid ranges.
A batch of 1000 lists was sampled for testing.
We sort these lists according to their respective scores output by our learned model.
After that, we check the percentage of the correctly sorted lists.
This accuracy of our model is the fraction of 1000 lists that were sorted.
We report the average of 5 runs in Table~\ref{table:caseStudyResults}.

\begin{table} [h!]
\centering
\resizebox{\linewidth}{!} {
\begin{tabular}{ | c | c | c | c | c | c | }
\hline
\textbf{Training Epochs} & \textbf{List size} & \textbf{In range Acc.} & \textbf{Out of range Acc.} & \textbf{Hybrid range Acc.} \\ [0.5 ex]
\hline \hline
1000 & 3 & 0.99 & 0.27 & 0.40\\
100 & 3  & 0.77 & 0.29 & 0.11\\
100 & 30  & 0.80 & 0.79 & 0.46\\
100 & 100  & 0.99 & 0.0 & 0.39\\
1000 & 100  & 1.0 & 0.71 & 0.48\\
\hline
\end{tabular}
}
\caption{Learned model Accuracies.}
\label {table:caseStudyResults}
\end{table}

We find from the tabulated results that training the model by running a higher number of epochs is vital.
Moreover, bigger list size is crucial to comprehensively learning the objective function.
The larger list size will make our model work well when the input is from the same distribution seen during training.
Moreover,  it also performs well when the input comes from a hybrid range.
These are good properties to have in any machine learning model.

The loss function is not weighted in this toy example because sorting requires each data point to be in its correct location.
Therefore,  our problem is not a direct generalization of this toy example.
Moreover, the toy example's input domain is relatively simple compared to real-world problems.
Nevertheless,  we take these results into account to decide on the list size for training our proposed ranking surrogate model.


%    #       Result -> Sorting is possible provided the output range of the model is not restricted
%    #       For now, we sample train/Val data from the same distribution.
  %  #   List of numbers to test with = {-1 to -100}
%    #       Result -> This is not possible because the model will map this to extreme values as it
 %   #                 is not distribution dependent.
 %   #   Check the percentage of the lists in the correct sorted order.

\iffalse
% \chapter*{Appendices}
\chapter{More information}
This thesis was completed in the representation learning lab of Albert-Ludwig-Universität Freiburg.  (Figure~\ref{fig:UniLogo})


\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.35]{images/logo}
    \caption{Logo: Albert-Ludwig-Universität Freiburg}
    \label{fig:UniLogo}
\end{figure}

\fi

\end{document}
