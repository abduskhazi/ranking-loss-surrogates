%\documentclass[11pt]{report}
\documentclass[12pt, twoside, ngerman]{report} 
%\documentclass[12pt,twoside]{report}
% \documentclass{article}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage{tikz}
\usetikzlibrary{positioning}
% \usetikzlibrary{graphdrawing.trees}

\usepackage{booktabs}

\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

% Commenting out another way to write the title.
% \title{MSc Thesis - Ranking Loss Surrogates}

\iffalse    
\author{
        Abdus Salam Khazi [
        \href{mailto:abdus.khazi@students.uni-freiburg.de}
                {Email} ]\\ \\
        \href{https://github.com/abduskhazi/ranking-loss-surrogates.git}
                {Github Repository} \cite{github_repository} \\ \\
        Supervisors:
        \begin{tabular}{ll}
             JProf. Josif Grabocka \&
			Sebastian Pineda
		\end{tabular}
       }
\fi


\begin{document}

\newgeometry{
    top=1.5in,
    bottom=1.5in,
    outer=1.75in,
    inner=1.25in,
    }

\title{
{\LARGE Master's Thesis}
\vspace{20pt}
\hrule
\vspace{20pt}
\textbf{Hyperparameter Optimization using Ranking Loss Surrogates}
\vspace{20pt}
\hrule
\vspace{20pt}
\author{{\LARGE Abdus Salam Khazi}}
\date{\LARGE \today}
\begin{figure}[h]
    \centering
    \includegraphics[width=50mm]{images/Logo.png}
\end{figure}
\large{Albert-Ludwigs-University Freiburg
\\Representation Learning Department}
}
    
\pagenumbering{gobble}

\maketitle
    

% \date{}

\chapter*{Declaration \thispagestyle{empty}}


I hereby declare, that I am the sole author and composer of my thesis and that no other sources or learning aids, other than those listed, have been used. Furthermore, I declare that I have acknowledged the work of others by providing detailed references of said work. 

I hereby also declare, that my Thesis has not been prepared for another examination or assignment, either wholly or excerpts thereof

\vspace{100pt}

\begin{minipage}{2in}
\textbf{\underline{\hspace{100pt}}} \\
Place, Date
\end{minipage}
\hfill
\begin{minipage}{1.3in}
\textbf{\underline{\hspace{100pt}}} \\
Signature
\end{minipage}

\newpage
\pagenumbering{roman}
\begin{abstract}

Abstract goes here

\end{abstract}

\newpage

\tableofcontents
\newpage
\newpage

\pagenumbering{arabic}

\chapter{Introduction}

The performance of any machine learning model is sensitive to the hyper-parameters used during the model training. 
Instead of using a new model type, it is more helpful to tune the hyper-parameters of an existing model to improve its performance.
Learning the best hyper-parameter for an ML model is called, Hyperparameter optimization (HPO in short).

The true evaluation of a hyperparameter optimisation objective function is computationally very expensive. 
Researchers have tried to get around this problem by proposing model based HPO algorithms.
In these methods,  the true objective function is modelled by a cheap surrogate function of high representational capacity.
For example,  the Sequential Model Based Optimization (SMBO)~\cite{NIPS2011_86e8f7ab} algorithm is a very important algorithm that iterates between learning a model given a few HP configuration evaluations and using the model to propose the next candidate to evaluate.

This thesis studies various existing approaches to HPO and proposes a new idea for the same using the concept of ranking.
The proposed idea in this thesis is called \textbf{Hyperparamter Optimization using Ranking Loss Surrogates}. 
The results obtained using this model are compared against the state-of-the-art results obtained using models like FSBO,  RGPE,  TAF, and others. 

\label{ProblemOverviewlabel}
\section{Objective}
The aim is to study ranking loss surrogates in the context of Hyperparameter optimization.

\section{Overview}
This sections contains the overview of the paper and how the thesis report is organised.


\chapter{Related Work}\label{chap:relatedWork}

In this chapter,  the hyper-parameter optimization problem is first defined concretely.
Then, the various approaches already used to do the HP optimization are discussed.
Subsequently,  some important concepts that the thesis uses to build the proposed model are also discussed.

\section{Hyper Parameter Optimization}
To find out the best hyper-parameter for any machine learning model $m$, we must first quantify a given hyper-parameter configuration $\textbf{x}$ by a real-valued number $v \in \mathbb{R}$.
If we define that
$$
\textbf{x}_1 \succ  \textbf{x}_2 \iff v_{\textbf{x}_1} < v_{\textbf{x}_2}
$$
then HPO can be defined mathematically by an abstract function, say,  $f(\textbf{x}) \mapsto \mathbb{R}$ as
$$
     \underset{\rm \textbf{x}}{\rm argmin}  f(\textbf{x}) \;\;\;  \forall \textbf{x} \in \mathbb{S}
$$
where $\mathbb{S}$ is the hyper-parameter search space.

This function $f(\textbf{x}) \mapsto \mathbb{R}$ is evaluated in the following chronological steps:
\begin{enumerate}
\item Using a given hyper-parameter configuration $\textbf{x}$,  we train our model $m$ to obtain the model $m^{trained}_\textbf{x}$. It consists of learning the parameters of our model, E.g. learning the weights and biases of a Deep Neural Network.
We use the training data to learn this model.
\item The validation data is passed through $m^{trained}_\textbf{x}$ to obtain the required results.
These results are evaluated based on an evaluation criterion '$\textrm{eval}$'. This criterion is different for different problems, e.g. Regression, Classification, etc.
The result of this evaluation is a real-value that gives a score for the configuration $\textbf{x}$.
\end{enumerate}

Hence the function $f(\textbf{x}) \mapsto \mathbb{R}$ can be written as 
$$
f(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}
$$

Finally, the HPO problem can be defined using the following equation:
\begin{equation}\label{eq:hpoobjectivefunc}
\underset{\rm \textbf{x}}{\rm argmin} \;\; f(m^{trained}_\textbf{x} (\textrm{Data}_\textrm{val})) \mapsto \mathbb{R}   \;\;\;  \forall \textbf{x} \in \mathbb{S}
\end{equation}

One way to view this objective function non mathematically is that we are trying to select a hyper parameter setting of the given model to obtain the best (lowest) validation error~\cite{fsbopaper}.

\subsubsection{HPO Constraints}\label{sec:hpoConstraints}

Hyperparameter optimization is different from other optimization methods because it has different constraints~\cite{bayesianOptimizationTutorial}.
It is because of the peculiar properties of the hyper-parameter search spaces.
Finding out the correct hyper-parameter setting is generally not feasible using a brute-force approach (trying out all possible combinations of hyper-parameters) because the search space itself has many dimensions, and the search space may be continuous.
More specifically,  some of the important constraints of this optimization problem are:

\begin{enumerate}
\item The evaluation of a given HPO configuration is computationally expensive.
\item It is a non-convex optimization problem.
\item The process of getting $m^{trained}_\textbf{x}$ from $m$ is stochastic hence the value $v_{\textbf{x}}$ is noisy.
\item Some dimensions have conditional constraints. The values of some dimensions may depend on the values of others. For example, the number of neurons in layer 3 only makes sense if we have 3 or more layers in a neural network.
\item The search space is hybrid in terms of continuity. Some of the dimensions (variables) may be continuous, while others may be discrete.
Using a gradient method is hence not trivial.
\end{enumerate}

To deal with the constraints of HPO problems, researchers have used different strategies for developing HPO algorithms and models.
Some of the more prominent approaches are Black-Box HPO, 
Multi-fidelity HPO and Online HPO.

\subsection{Black-box HPO}
In this approach the HPO objective function $f$ in equation~\ref{eq:hpoobjectivefunc}
is treated as a blackbox.
The problem is thus generalised to finding a global optima of $f$.
Some of the straightforward black-box HPO methods include Manual Search,  Grid Search and Random Search.
The optimization technique called Bayesian optimization gives us a more sophisticated mechanism to deal with this problem. 

Manual Search in the HPO search space is feasible when we have expert knowledge of the problem and the given model. 
The idea is to select configurations step by step by observing the results obtained
so that we do not waste computation time evaluating similar configurations through intuition.
This approach may be helpful for small models with lesser constraints.
However, as the HPO search space becomes very large or conditional constraints become too complex, the selection of configurations becomes more and more difficult.
Hence a more systematic and automated approach is more practical.

Grid search is a more systematic approach in which we divide the search space into grid points similar to ones in a Euclidean graph.
Let there be $m$ dimensions in the search space $\mathbb{S}$. Let the selected number of values for each dimension be $n_1, n_2, ... n_m$. In the case of a discrete variable, the selected values will be a subset of the possible values, whereas, in the case of a continuous variable, we need to select the values based on a selected granularity.
The cross-product of the selected subsets gives us the configurations to be evaluated. Hence, the number of configurations to evaluate will be $n_1 * n_2 * ... n_m$.
The number of configurations we need to evaluate in this approach becomes a big issue for this method as the dimensions of the search spaces increase.
Hence this approach becomes intractability for large search spaces.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.8]{images/rsgsexample}
    \caption{Illustrates Grid search and Random search in the case where 2 parameters are not equally important.  Adpated from~\cite{rshpoarticle}.}
    \label{fig:rshpofig}
\end{figure}

One issue with the Grid Search approach is that we assume that all dimensions in the HPO search space are equally important. It is not the case in many HPO problems.  The Grid layout in Figure~\ref{fig:rshpofig}(left) shows illustrates this.  For example, the learning rate in deep neural networks is much more important than many other parameters. If dimension $p$ is the most important in the search space, then it makes sense to evaluate more values of $p$. Random Search helps us solve this problem. 
The Random layout in Figure~\ref{fig:rshpofig}(right) illustrates this. 
Hence Random Search can be used as a trivial baseline for comparing other HPO models.

One advantage of these methods is that there are no restrictions on the HPO search spaces. Hence, they are suitable for any HPO problem at hand.
On the other hand, these methods are non-probabilistic.
Hence they cannot deal with noisy evaluations of the HPO configuration well.
Moreover, these methods are computationally expensive. The reason is that they do not use surrogate evaluators and hence train and evaluate the whole model.
Also, these search methods give us optimal HPO configurations only by chance.
\subsubsection{Bayesian Optimization}

Bayesian optimization tries to solve both computational costs and noisy evaluations of our objective function $f$.
It does this by building a model of the HPO objective function. This model is called a surrogate function.
Bayesian optimization uses known evaluations as its data to build the surrogate model. The data is of the form \{x, f(x)\} pairs.
The surrogate model is a probabilistic model. Hence, it also learns about the noise in the evaluations of the objective function.

The core procedure of the optimization process is the following:
\begin{itemize}
\item From known data $D = {(x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3)) .... }$, build a probabilistic model that learns the mean and variance of the objective function
\item Use the surrogate to sample the next best HPO configuration x' using a function known as acquisition function. Evaluate f(x').
\item Append (x', f(x')) to D and repeat the process.
\end{itemize}

The above process repeats till the computational resources are finished (here time) or we find an acceptable HPO configuration.
This procedure is also called SMBO (Sequential model-based Optimization).
The procedure alternates between collecting data and fitting the model with the 
collected data~\cite{SMBOPaper}.

Hence, there are two essential components of Bayesian optimization:

\begin{itemize}
\item Probabilistic surrogate model of the objective function.  Some surrogates are discussed in detail in the subsequent sections in this chapter.
\item The acquisition function
\end{itemize}

\subsubsection{Acquisition functions}
The acquisition functions used in the bayesian optimization need to do balance exploitation of information from the known/observed data points and exploration of unknown data points in the domain.
The following functions are some of the most prominent acquisition functions found in the literature~\cite{GPTutorial}
\begin{itemize}
\item \textbf{Upper Confidence Bound (UCB)}: It returns the best possible hyperparameter configuration using a linear combination of the mean and the standard deviation.
\item \textbf{Probability of Improvement}: It gives the probability with which we can get a better hyperparameter configuration than the incumbent best configuration.
\item \textbf{Expected Improvement}: Given a Gaussian distribution at a new input point, it finds the expectation of improvement i.e ($f(x) - f_{max}$) over the part of normal that is greater than $f_{max}$.
\end{itemize}

 \textbf{Expected Improvement} acquisition function is used all around the thesis in order to do a fair comparison of the models and algorithms.

\subsection{Online HPO}
Traditionally to evaluate and select a new HP configuration,  the objective function $f$ is fully evaluated.
In the most general sense this can be applied to both discrete and continuous HP search spaces.
Some advanced methods have proposed even gradient based HP optimization.

For example,  Maclaurin et al.~\cite{hypergradient} proposed a relatively cheap method to obtain hyper gradients i.e gradients of hyper parameters with respect to the whole objective function $f$.
Further,  Franceschi et al.~\cite{HPOAsBilevelOptimization} formulated the whole HPO problem as a bi level optimization problem~\cite{hutterneuripstutorial}.

In all these methods as the hyper parameters and the parameters of the model are being learnt disjointly,  they can be referred to as offline HPO.
The idea of online HPO is that it tries to evaluate and update the HP configuration during the training of the model itself.

Sometimes only a single hyper parameter is learnt online.
For example,  Baydin et al.~\cite{onlineLearningRateUpdate},  proposes the online learning of the learning rate.  They target this hyper parameter because it is the single most important hyper parameter.
In other times all parameters may be learnt.
For example,  Luketina el at. ~\cite{gradientbasedHPOtuning} proposes to interleave the updating of training parameters and hyper parameters.


\subsection{Multi-fidelity HPO}
If we treat our HPO objective function $f$ as a black box function,  we would need to evaluate equation~\ref{eq:hpoobjectivefunc} fully.
This is prohibitively expensive as evaluating a single HP configuration may take days~\cite{hutter2019automated}.
Multi-fidelity hyper parameter optimization tries to solve this problem by evaluating the HP candidates on cheap functions $f_{approx}$ that approximate the objective function $f$.

Here, $f_{approx}$  is called a fidelity as it copies or reproduces the true objective function upto some degree.
For example,  a fidelity could be evaluation of the HP configuration on only a subset of data,  the evaluation of the HP on downscaled image data or learning only for a few epochs etc.
The idea is to trade off between the performance of the approximate function and its optimization accuracy such that we get best selection of HP using the least compute power.
Since the true evaluation of HP configuration using $f$ is not done,  it is an approximate optimization technique.

The technique does not belong to the black box HPO domain.
This can be understood very clearly if we take the example of the "few epochs" fidelity.
Clearly,  the optimization algorithm looks into the training process the learns the objective function to prematurely exit it if need be.
Thus getting a feedback from within the "black box" of the HPO objective function.

The reason this technique is called multi-fidelity is because it may use different fidelities within the optimization process to get the best HP configuration.
For example,  while using successive having technique~\cite{successivehalving} for HPO one can start with a given "budget fidelity" for example - defined number of epochs or defined amount of training time.
At each step of the optimization process,  the budget is doubled and the worst performing HP configurations are eliminated for the next step.
Hence it uses "multiple" fidelities during the optimization process.


\section{Transfer-learning for HPO}

The HPO methods discussed so far,  evaluate each HP configuration from scratch.
In addition to being computationally inefficient,  it is contrary to how humans learn.
Humans use prior experience that they have accumulated and condition their actions on this knowledge.
Any machine learning mechanism that uses this concept is called a transfer learning method~\cite{Weiss2016}.

This concept of transfer learning can be utilized to accelerate HPO.
For example,  Gomes et al. ~\cite{svmhpmetalearnt} propose to meta learn a set of good HP configurations for the SVM.
They propose that if we learn HP configurations that worked well previously,  then there is high chance of finding that a new SVM model also works with these parameters well.
This concept of warm starting the HP optimization is also proposed by Reif et al.~\cite{metalearningwarmstartpaper} albeit for genetic algorithms.

There are 2 broad ways to transfer knowledge for doing HP optimization - By learning surrogates or by doing warm starting of initial configurations.

\subsection{Warm starting}

Before running any HPO method,  normally experts study the data being used to train the given model.
Based on their study they suggest a few initial HP configurations that have worked well for similar datasets according to their experience.
These initial configurations are evaluated for the given model and the evaluated values act as a starting point (or hinge) for the HPO method.
This is essentially automated by the warm starting method~\cite{Feurer2014UsingMT}.

In the above mentioned papers ~\cite{svmhpmetalearnt},  ~\cite{metalearningwarmstartpaper} the authors use the meta learnt HP configurations as starting points for finding the optima in the HP response surface.
Additionally this idea was also proposed for the SMBO optimization algorithm by Feurer et al. ~\cite{Feurer2014UsingMT}.


\subsection{Meta-learning of surrogates}

HPO models that use surrogates like SMBO,  have an additional gateway through which previous knowledge can be injected.
The idea is to meta-learn the surrogate using previously stored meta-data from similar tasks.
This is used by Schilling et al.~\cite{Schilling2016ScalableHO} in which they learn a collection of Gaussian models for each previously similar dataset due to computational constraints.
They then use the collection of GPs as a surrogate for the SMBO procedure.
Another flavour of this approach was proposed  by Feurer et al.~\cite{Feurer2018ScalableMF} proposed the use of rank weighted GP Ensembles (RGPE) surrogates meta-learnt from previous meta data.

This meta learnt surrogate can be used in 2 different ways during the HPO optimization procedure.
For example in SMBO,  one could use it without any modification (aka fine tuning) to suggest the next best candidate to evaluate from a given HP configuration list.
On the other hand the surrogate can be further meta trained (aka fine tuned) using the available target task meta data. 

Using meta learning of surrogates,  this thesis proposes a new surrogate model.

\section{Types of Surrogates for BO in HPO}
Hyperparameter optimization using Sequential Model Based Optimization (SMBO)~\cite{NIPS2011_86e8f7ab} is a convenient method proposed in the literature.
However,  the performance of this method is heavily reliant on how well the surrogate (model in S\textbf{M}BO) models the true HPO objective.
In this section,  we discuss in details some of the powerful surrogates that may be used with SMBO.

\subsection{Gaussian Processes}

Gaussian processes~\cite{GPTutorial} are predictive machine learning models that work well with few data points (or data pairs). 
They are inherently capable of modeling uncertainty.
Hence, they are used widely in problems such as hyperparameter optimization, where uncertainty estimation is essential.
In this section, we briefly explain the Gaussian process regression intuitively.

Before we proceed,  we need to understand normal (Gaussian) distributions. 
Consider a scalar random variable $X$ that is distributed normally  (a.k.a Gaussian distribution) around a mean $\mu$ with a variance of $\sigma^2$.
The following equation defines the probability density function (PDF) of $X$: 
$$
P_X(x) = \frac{1}{\sqrt[2]{2\pi}\sigma}\exp\left(- \frac{(x - \mu)^2}{2\sigma^2}\right)
$$
Here, $X$ represents the random variable, and $x$ represents an instance of the variable~\cite{GPTutorial}.
In this case,  the mean $\mu$,  variance $\sigma^2$, and any sample $x$ are all scalars.

If the random variable $\textbf{X}$ is a vector in $\mathbb{R}^d$ where $d \in I^{+}$,  then each component of the vector can be considered as a random variable.
In this case the mean $\boldsymbol{\mu} \in \mathbb{R}^d$ whereas variance, represented by $\Sigma$, is in the $R^{d \times d}$ space.
It is because the variance of all components in any valued vector random variable $\textbf{X}$ should contain the following two types of variance
\begin{itemize}
\item Variance of a vector component w.r.t itself.
$d$ diagonal values of the matrix $\Sigma$ represent this variance.
\item Variance of each vector component w.r.t all other components. These variances are represented by the upper/lower triangular values in the matrix $\Sigma$.
\end{itemize}
The matrix $\Sigma$, also known as the Covariance matrix, thus has all values necessary to represent the variance of any vector-valued random variable.


The probability density function of a vector valued variable $\textbf{X} \in \mathbb{R}^d$ with a mean $\boldsymbol{\mu}$ and covariance matrix $\Sigma$ is given by~\cite{MITMLBook}:

$$
\mathcal{N}(\textbf{x} | \boldsymbol{\mu},  \Sigma) = 
\frac{1}{(2\pi)^{\frac{d}{2}} |\Sigma|^{\frac{d}{2}}}
\exp\left( - \frac{1}{2} (\textbf{x} - \boldsymbol{\mu})^T  \Sigma^{-1}   (\textbf{x} - \boldsymbol{\mu}) \right)
$$
This equation defines the PDF of a multivariate normal distribution.

The core idea used in the Gaussian processes is that functions can be considered as vectors of infinite dimensions.
Consider any function $f$ that has a domain $\mathbb{R}$.
If $f$ is considered to be a vector in $\mathbb{R}^{\infty}$,
then each point $i \in \mathbb{R}$  can be represented by a component $f_i$ of the function $f$.
A function,  hence,  is nothing but a sample from $\mathbb{R}^{\infty}$.
Unfortunately, functions sampled from $\mathbb{R}^{\infty}$ are too general and not useful by themselves.

The idea of Gaussian processes is to sample smooth functions from $\mathbb{R}^{\infty}$.
In any smooth function $f$, if any point $g$ is close to $x$ in the domain of $f$, then $f(g) \approx f(x)$.
It is mathematically represented by the following equation:
$$
\lim_{\delta x \to 0} f_{x + \delta x} \approx f^{+}_x  \;\; \textrm{and} \;\; 
\lim_{\delta x \to 0} f_{x - \delta x} \approx f^{-}_x 
$$
$$\;\; \textrm{where} \;\; \delta x > 0 \;\; \textrm{and} \;\; x, \delta x \in \mathbb{R}
$$
The above definition is nothing but the definition of a smooth function in terms of vector notation. 
Moreover, nearby components of $f$ "vary" similarly w.r.t each other.
These properties can be naturally encoded using a covariance matrix.
Hence, we obtain smooth functions if we sample them from a multivariate normal distribution with the required covariance matrix.
The Gaussian process restricts the function sample space to a multivariate normal distribution.

The similarity between 2 points in a domain is defined by a function called \textbf{kernel} in Gaussian processes.
Using this kernel function, the values in the required covariance matrix are populated.
The smoothness of the sampled function $f$ is controlled by the kernel in the GP process.
Formally kernel $k$ is defined as,
$$
k(\textbf{x}, \textbf{x'}) \mapsto \mathbb{R}
$$

Here, $\textbf{x}, \textbf{x'}$ belong to a domain in the most abstract sense.
For example,  when the input domain is a euclidean space,  $\textbf{x} \in \mathbb{R}^{\mathbb{I}^+}$.

Some well known kernels are:
\begin{itemize}
\item \textbf{Radial Basis Function Kernel}
\item \textbf{Matern Kernel}
\item \textbf{Periodic Kernel}
\end{itemize}

Finally,  a Gaussian Process specifies that any new observation $y^*$ for input $\textbf{x}^*$,  is jointly normally distributed with known observations $\textbf{y}$ (corresponding to the input $\textbf{X}$) such that
\begin{align}
    Pr\left( \begin{bmatrix}
           \textbf{y} \\
           y^*
         \end{bmatrix}
         \right)
         &=  \mathcal{N}\left(m(\textbf{X}), \mathbf{\Sigma}\right)
\end{align}

Here, $m(\textbf{X})$ is the mean of the vectors which is commonly taken as $\textbf{0}$.
$\mathbf{\Sigma}$ is the covariance matrix defined as
$$
\mathbf{\Sigma} = \begin{bmatrix}
           \textbf{K} & \textbf{K}_* \\
           \textbf{K}_*^T & \textbf{K}_{**}
         \end{bmatrix}
$$
  Where $\textbf{K} = k(\textbf{X}, \textbf{X})$,  $\textbf{K}_*  =  k(\textbf{X}, \textbf{x}_*)$ and $\textbf{K}_{**} = k(\textbf{x}_*,  \textbf{x}_*)$ for any given kernel $k$~\cite{GPTutorial}.
  Due to the robustness of the GP process, we use this as one of the baselines in our thesis.

\subsection{Random Regression Forest}
     The core idea of this model is to train a Random Regression Forest, using the known data as in any SMBO procedure~\cite{SMBOPaper}.
Random regression forests are an ensemble of regression trees. 
This property is used to our advantage to predict the mean and the variance. 
The mean of the prediction of all the trees is the mean of the surrogate model.
The variance in the prediction of all trees is the variance of the surrogate model.

   The advantages of this model are
   \begin{itemize}
   \item It can handle both continuous and discrete variables trivially without any modifications to the model.
The data splitting during training is done using any variable be it discrete or continuous.
	\item It can handle conditional variables, unlike Gaussian processes, by making sure that data is not split based on a variable till it is guaranteed that no conditionality is broken by the split.
   \end{itemize}

\subsection{Bayesian Neural Networks}
\subsection{Neural Networks}
they don't work great, because they cant model uncertainty). Uncertainty can be modeled directly, or through ensembles.

\section{Types of Losses}
\subsection{BO with GP uses negative log likelihood}
It is not clear whether pointwise losses (regression as in GP) are the correct way to model HPO responses, because we only care for the minima regions (best performing configurations), and not for estimating all observations accurately.

\subsection{Ranking loss could be the answer in modeling hp optima better}
\subsubsection{Pointwise}
\subsubsection{Pairwise}
\subsubsection{Listwise}

\section{Set-modeling with Neural Networks}

In the previous sections we discussed about various types of surrogates that can be used for transferring knowledge in transfer HPO methods.
Training on data from known tasks and then using this trained model for working on unknown tasks, however,  has a major shortcoming conceptually.
Humans do not pre-condition their actions on new tasks only on their previous experiences.
Their pre-conditioning also includes the knowledge of the current task (however little it may be).


To model this concept,  one has to learn to represent and pre-condition knowledge from known tasks in a model.
This makes the model context aware.
If we consider a HP configuration and its evaluation ($\textbf{X}$, $y$) as a single object,  then the group of all the known pairs can be represented as a set.
Here,  $\textbf{X}$ is a feature vector and $y$ is a scalar.
Now our problem is a 2 stage process which includes
\begin{itemize}
\item Representation of a Set
\item Conditioning of our model on the Set Representation.
\end{itemize}


We use Deep Neural networks to represent a set in this thesis because it is a model with  high representational capacity and is easy to train.
Deep Sets by Zaheer et.  al~\cite{deepSets} and Set Transformers by lee et. al~\cite{setTransformer} are two of the interesting researches that we found in the literature.
Since the sophisticated attention mechanisms used in Set transformers were unnecessary for us,  we chose to use the more simple architecture used in Deep Sets.
The next section discusses about these Deep Sets.

% \subsection{Set-transformers}
\subsection{Deep Sets}\label{sec:DeepSets}
Traditionally deep learning or machine learning models learn functions of the following format:
$$
f : \mathbb{R}^d \mapsto \mathbb{R}^k \quad d,k \in \mathbb{I}^+ \quad \textrm{For Regression}
$$
$$
f : \mathbb{R}^d \mapsto \{c_1, c_2, ... c_n\}  \quad \textrm{For Multi-Classification}
$$
which can be thought of transforming objects from the input space to the output space.

For our problem of set latent representation,  the input space changes to a space containing sets (each instance in the space is a set by itself).
The output space however remains similar to either the regression case for regression tasks and classification case for classification tasks.
Consider $\mathbb{X} = \{a,b,  c, ... \}$ be a set containing all possible elements within the sets of the input space.
Then the set representation problem can be defined as:
$$
g : 2^{\mathbb{X}} \mapsto R^k   \quad k \in \mathbb{I}^+ \quad \textrm{For Regression}
$$
$$
g : 2^{\mathbb{X}} \mapsto \{c_1, c_2, ... c_n\}  \quad \textrm{For Multi-Classification}
$$
Where $2^{\mathbb{X}}$ is the power set of $\mathbb{X}$.

2 very important constraints of this problem are
\begin{itemize}
\item Permutation-Invariance constraint: The permutation of the objects within an input set should be irrelevant for the model $g$.
\item Set cardinality invariance constraint: The cardinality of the set can be variable.
Hence our model $g$ should be invariant to the number of elements in the input set.
\end{itemize}

Permutation equivariance is another type of constraint that the deep set paper deals with.
In this constraint the latent space should be equivariant or symmetric to the permutation of the objects in the input set.
This creates issues for our problem because we don't care if our latent space is some symmetric mapping of the input space.
We want the whole set to be mapped definitively to a latent embedding.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X1)                             {$\textbf{X}_1, y_1$};
\node[roundnode]      (X2)                            [below=of X1]     {$\textbf{X}_2, y_2$};
\node[roundnode]      (X3)                            [below=of X2]     {$\textbf{X}_3, y_3$};
\node[squarednode]      (phi1)                      [right=of X1]        {DNN$_{\theta}$};
\node[squarednode]      (phi2)                     [right=of X2]        {DNN$_{\theta}$};
\node[squarednode]      (phi3)                     [right=of X3]        {DNN$_{\theta}$};
\node[squarednode]      (Pool)                     [right=of phi2]     {Pool};
\node[squarednode]      (pho)                      [right=of Pool]     {DNN$_{\phi}$};
\node[roundnodey]      (LatentOutput)         [right=of pho]     {Latent Output};

%Lines
\draw[->] (X1.east) -- (phi1.west);
\draw[->] (X2.east) -- (phi2.west);
\draw[->] (X3.east) -- (phi3.west);
\draw[->] (phi1.east) -- (Pool.west)    node[midway, right] {$I_1$};
\draw[->] (phi2.east) -- (Pool.west)   node[midway, above] {$I_2$};
\draw[->] (phi3.east) -- (Pool.west)   node[midway, right] {$I_3$};
\draw[->] (Pool.east) -- (pho.west);
\draw[->] (pho.east) -- (LatentOutput.west);
\end{tikzpicture}
\caption{Skeletal of the architecture proposed in deep sets}
\label{fig:DeepSetArchitecture}
\end{figure}

Figure~\ref{fig:DeepSetArchitecture} illustrates the architecture proposed by the deep set paper.
We use a set of only 3 objects in the Figure for simplicity.
The architecture passes all the 3 inputs through the a deep neural network independently to get intermediate outputs $I_1$, $I_2$, $I_3$.
To solve the permutation invariance constraint,  any mathematical operation that is commutative and associative is applied on the intermediate outputs.
As long as the pooling operator is permutation invariant, the proposed architecture is also permutation invariant.
Examples of such operators are sum, mean, max etc.

To solve the problem of cardinality constraint we can use the mean operator.
Hence the pooling operation becomes:
$$
\textrm{Pool}(I_1, I_2, I_3) = \frac{ I_1 + I_2 + I_3}{3}
$$

The output of the pool operation is then passed through a different Deep Neural Network to finally get the Latent output.

\iffalse
\section{RGPE}
Read the FSBO paper and write the summary like that or read  the RGPE paper
This section is necessary because our model performs similar to this

\section{TAG}
Read the TAG paper.
Some other model if also necessary and you mention this in your report.
\fi

% \section{Deep Kernel Learning}

\iffalse
Paper: https://arxiv.org/pdf/2101.07667.pdf

Problem Domain: Hyperparameter searching.
Keyword: Use Deep Kernel surrogates

Key points:
    1. Use transfer learning surrogates to get faster convergence
    2. Treat HPO as a few shot learning problem (In the context of transfer learning)
    3. Use kernel k(phi(x), phi(x')) where phi is a neural network that transforms x to a latent vector
    4. Learn parameters of phi(W) and kernel(theta) together based on historical meta data
    5. Fine tune both W and theta based on the given task. If possible use warm start for initialization.
    6. Learn output range variance by creating augumented tasks.

Conceptual points:
    1. The deep kernel learnt does not have any task dependent parameters.
       The task dependent parameters are marginalized out.
       Hence finetuning during test time (after pretraining) should be complete.

Quick note on expected improvement.
    First the mean and variance is calculated for the target Hyperparameter Setting.
    Considering the mean and variance of a particular HP, we calculate the following:
%        Remove y_max from the set of all values of f(x) and lowerbound it by 0.
 %       Multiply the above value with the probability of the gaussian curve N(predicted_mean, predicted_variance)
        For a continous curve we need to integrate.
    Then we get the Expected Improvement for this HP setting.


Gaussian Processes:
    The output is considered to be a random variable.
    When have more than 1 data point, the output becomes a multivariate normal
        The output is a joint gaussian distribution.

Personal points (Probability vs Likelihood)
    Probability and likelihood are reverse in nature.
    Probability starts with a given set of parameters and caculates the probability of a given outcome to occur
    Likelihood starts with a given outcome, and we would like to determine the parameters.

    Marginal likelihood (Likelihood after marginalization of some parameters)
        integrates the effects of parameters that are not of your interest.

Note:
    Warm start is not essential for us. (It is not the main contribution of the paper)

Main motive of deep kernel learning:
    Learn the kernel function in the gaussian process.

How to do implementation with FSBO with HPO-B Metadataset?
There are 16 search spaces (model optmizations here)
    The train test split is already done metatraindata has the training data and metatest data has the test data.
    What about validation?? Really required?
    The search space id can be used for identifying correct search space for test and training data by using:
%        hpobhander.meta_test_data[search_space_id][dataset_id]["X"]
  %      hpobhander.meta_train_data[search_space_id][dataset_id]["X"]

1 ML model optimization
    1 search space (with a unique searchspace id)
        Dataset 1 (with unique dataset id)
        Dataset 2
        Dataset 3

Pretrain the FSBO model with the train split of the dataset. M'
Fine tune it on 1 dataset and evaluate using Expected Improvement
    i.e run loop like the training with the given data.

Did not use strictGP --> Is it a problem?
\fi



\iffalse
This concept is heavily used in domains like document ranking,  web search results  (i.e information retrieval)(and other areas) to rank documents that were retrieved for a given query.
\fi

\iffalse
Main Idea:
    Given a set of objects, we need to rank the objects. (E.g. Ranking documents based on relevance to a query)
    

3 types of ranking functions:


Evalution of learnt RF = Ranking measures.
Relationship of Ranking measures and RLs is unkown.


Main AIM:
    Find relationship between Ranking Measuress and Pointwise/Listwise Ranking Losses.
    Pointsize relationship already clear
    Goal to do the same for pair wize and listwise case.

Proposed IDEA:
    Use an essential loss. (Need more reading)

Loss function understanding:
Pointwise - Try to get the label as per data
Pairwise - Try to separate the labels as much as possible. (Because of -z in all forms of phi).
           It is a classification of 2 objects with a boundary --- Hence the effort to separate things.
Listwise - The anology of this is that of 2 oppposing forces. One the rank of the object. Secondly
           the number of objects below it in the list.
           Loss ===> -rank + number of objects below it.
                If the rank is low and more number of objects are below it ==> Loss > 0 which is not desirable
                On the contrary, if the rank is high it can bear more objects below it as Loss would not be so high.
           Permutation invariance is obtained by using random valid (best case)
%     Doubt. We talk about permutations. It is only possible if #Lables << #DataPoints.
        ==> This is true as we are doing K-Layer classfication.


Ranking Losses:
    Point               Pair            List
    Subset regression   Ranking SVM     ListNET
    McRank              RankBoost       ListMLE
                        RankNet

Brief note on Pairwise approach: from the introduction of https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf
    The data is created by creating (x1, x2) -> label tuples for all possible x1, x2 in the ranked list. The label can be -1, 0, or 1
    ,for instance, if x1 is having lower, equal or higher rank to x2 respectively.

Ranking Measures
    NDCG = K level ratings
    MAP = 2 level ratings

    Read: https://faculty.cc.gatech.edu/~zha/CS8803WST/dcg.pdf
    Read: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/letor3.pdf (Page 15 for a clearer picture)
        CG = Cumulated gain (of information)
        DCG = Discounted cumulated gain (of information)
        NDCG = Normalized DCG i.e Divide each position of the DCG by Ideal DCG for the results.
               You get a list of values in [0,1] which lenght of list = number of rankings considered.
        NDCG@k = required real valued function of the ranking measure.

    Question: Only 2 level rankings necessary for our case?

Understanding listwise loss function:
    Queries       Ranking(f(Q, D))      Ground Truth scores
    q1        [d1, d2, d3 ... d10]      [y1, y2, y3 ... y10]
    q2        [d1, d2, d3 ... d15]      [y1, y2, y3 ... y15]
    q3        [d1, d2, d3 ... d7]       [y1, y2, y3 ... y7]

    D = {Set of Documents}
    Q = {Set of Queries}
    f : QxD -> R [Note: Here D is conditioned on Q]. The function is defined for 1 (query, document) pair.

    Point to note - Each feature input = a concatenation of the query vector and the document vector.

    One instance of our training data is (X, Y) where X = {Set of all documents returned by query} Y = {Set of the respective ground
    truth scores}. Basically 1 query is one instance. Hence our loss function has to take in vector of outputs from f.

    Loss = L(Xi , Yi) where Xi and Yi are refer to 1 query qi.
    Full Batch Loss = mean (L(Xi, Yi) for all elements i in the training objects)

    Here f(Q, D) itself is the scoring function that is the main model to be learnt in our framework.

    *** Complete explanation found in Section 3 and Section 4 of paper:
        *** https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf

    * ListNet
    * We need to make sure that f returns scores that are SIMILAR IN RELAVANCE/ORDER to the ground truth scores.
    * This would make the Ranking(f(Q, D)) equal to the ground truth and we would have learnt our ranking function.
    * Note that we do not need to get the exact ground truth scores. Which increases the space of acceptable functions in the
      function space F (Here f belongs to F) we are searching from 1 to INF. i.e It becomes easier to search if we only want a subspace and not the exact function.
    * RANKING is nothing but sorting the results based on their respective scores/relavance (decreasing order)
    * Since the sorting function is non differentiable, the loss function in question should not be composed of the RANKING function.
    * Moreover, leaving the sorting function makes our loss permutation independent by making use of the + permutation invariant operator
      [Check the final step for more clarity on this. Cross entropy uses + operator]
    * This leaves us with 2 lists -
        a. List of scores given by our ranking function f
        b. List of relavance scores given to us by ground truth
    * The loss function finds the distance of these 2 lists.
    * A probabilitic approach is taken so as to take into account for any uncertainities.
%    * The probability of selecting any document can be taken as score_of_document / sum(all document scores) since higher score means
      a more relavant document
    * However the score of the document can negative as well. Hence a strictly positive and increasing funciton phi is taken
%        which makes prob(d) = phi(d) / sum (phi(d_i) for i in Documentsof(QueryGiven))
    * The probability of a permutation is nothing but the probability of selecting one document after another without replacement
      (Reminder: Discrete probability calculation){Which itself becomes a probability distribution i.e sigma = 1}
    * One possible way to find the distance betwen the 2 lists a and b is to find the probabilities of all permutations for a and b
      and compare both the distriputions. 
    * Complexity of this O(n!) ==> Intractible
    * Instead take the probabilities of selecting every document first in any possible permutation. {Which also is a probability 
      distribution i.e sigma = 1}
    * The first selection probability (Top 1 probability in the research paper) for a and b are calculated separately using their
      respective scores.
    * The final Loss for the list = Cross entropy of probability distribution (a) w.r.t that of b.
    * This loss is backpropogated through the network to set the parameters.
%
    ListMLE: http://icml2008.cs.helsinki.fi/papers/167.pdf
    * Main difference is that the loss function used is different.
    * Loss function is intuitive in that they would want to raise the probability of getting the ground truth permutation.
    * They increase the probability of getting the exact (ground truth) permutation using the scores given by f.

    Both the papers use linear network model for some simplicity. But we can use the non-linearity due to available library
        implementations
    Very nice summary of both loss functions section 3.2.1 : https://arxiv.org/pdf/1707.05438.pdf 

Advantages:
    Can be used to select best n configs and then evaluated at random based on the ranks. This is not as trivial to get in other
    places. So there is an amount of parallelism that can be built it.

Best Model for training and ft giving auful resuts - Reason can be that we are selecting a very unfitted model due to too much variation in the validation losses.
Hence we are start only ft best model. Ih the hope that we will get better resutls.
only fine tune best fit is not giving good results. Investigation in progress. 

\fi


\chapter{Background}

In this chapter we discuss in detail the fundamental concepts one must grasp in order to understand the thesis work.
First we talk about the problem of ranking and ranking losses.
Thereafter, we discuss the modelling of uncertainty done using Deep Neural Network ensembles.

\section{Rank Learning}\label{sec:ranklearning}

Consider a set of objects $\mathbb{A} = \{\textbf{x}_1, \textbf{x}_2, \textbf{x}_3, ... \textbf{x}_n\}$ where each $\textbf{x}$ belongs to a domain $\mathbb{D}$.
The problem of ranking is defined as finding an ordered list of objects in $\mathbb{A}$ such that an object $\textbf{x}_i$ is ranked before $\textbf{x}_j$ if $\textbf{x}_i$  is more relevant/important than $\textbf{x}_j$.
To accomplish this objective,  a ranking model needs to be learnt.
In the most general case the cardinality of $\mathbb{A}$ is not fixed.
For this reason, the ranking model, say $f_r$, can be thought of as a process that is divided in the following steps~\cite{procedureforrankinginintro}
\begin{itemize}
\item Obtaining a relevance score of each object in set $\mathbb{A}$.
\item Sorting the objects based on their relevance score. 
\end{itemize}

We can learn the ranking model $f_r$ by optimizing a criteria on the output of the model i.e the sorted list of object.
This criteria in the jargon of machine learning is called a loss function.
Hence,  it can be referred to as a \textit{Ranking Loss}.

Since the step of sorting is non differentiable,  it cannot generally be learnt during the optimization of our Ranking Loss.
Hence the ranking model,  $f_r$,  boils down to a relevance scoring function.
After learning $f_r$,  one can use it to rank newly given sets of objects by first finding their relevant scores and then sorting them accordingly.

Various types of ranking losses can be used in our optimization to learn the scoring function.
These can be broadly classified into the following types~\cite{RankingLossFirstPaperRead}:
\begin{itemize}
\item  Point-wise ranking losses
\item  Pair-wise ranking losses
\item  List-wise ranking losses
\end{itemize}

In point-wise ranking loss,  the loss function views the problem of ranking as that of assigning a label to each of the input data points.
Hence,  for learning,  each instance is a single object $x_i$ within the set $\mathbb{A}$.
For example,  in the McRank paper~\cite{McRank},  the authors reformulate the ranking problem as a multilevel classification problem where each data point is classified independently.
They then calculate the score as the expected rank of the object based on its soft classification.
Therefore, the complete scoring function comprises of a multi-level classifier and an external expectation calculation.

In pair-wise ranking loss,  the loss function's input is a pair of objects.
This loss function learns to model pair-wise preferences.
The function tries to separate the input data points as much as possible in the output space by minimising the pair-wise classification error~\cite{pairwisepreferencespaper}.

In list-wise ranking losses,  the loss is defined on the complete set of objects.
The 2 most important list-wise loss functions are
\begin{itemize}
\item ListNet~\cite{listwisebetter}.
\item ListMLE~\cite{listmlepaper}.
\end{itemize}

The point wise and pair wise ranking models do not view the ranking problem as a problem to rank a set of 
objects.
This is quite intuitive and is a fundamental advantage as compared with other methods.
It has been shown in~\cite{listwisebetter} that list wise approaches are superior in performance to point wise and pair wise losses.
We hence use the list wise approach to ranking.
We discuss and analyse the loss functions ListNet and ListMLE in detail in
the next sections

\section{Loss functions: Definition}

\iffalse
\begin{table}[ht]
\centering
% spacing in table
% \ra{1.3}  % Commenting this could not get this function right
\begin{tabular}{@{}lr@{}}
  \toprule
  Epochs listsize & Accuracy(within range) Accuracy outside range \\ \midrule
  A    & 82.47 $\pm$ 3.21 \\
  B    & 78.47 $\pm$ 2.43 \\
  C    & 84.30 $\pm$ 2.35 \\
  D    & 86.81 $\pm$ 3.01 \\
  \bottomrule
\end{tabular}

    \caption[Table caption]{\textbf{Sorting accuracy.} Obtained after sorting the numbers based on the scorer's results\\}
    \label{tab:accuracy}
\end{table}
\fi

Consider data in the format shown in table~\ref{tab:dataformat} is given to us.

\begin{table} [ht]
\centering
\begin{tabular}{ | c | c | c | }
  \toprule
  Instance & Object Set & Ground Truth \\ \midrule
% \hline \hline
  1 & $\{a_1, a_2, a_3, ... , a_{10}\}$  & $\{y_1, y_2, y_3, ... , y_{10}\}$  \\
  2 & $\{a'_1, a'_2, a'_3, ... , a'_{15}\}$ & $\{y'_1, y'_2, y'_3, ... , y'_{15}\}$  \\
  3 & $\{a''_1, a''_2, a''_3, ... , a''_{7}\}$ & $\{y''_1, y''_2, y''_3, ... , y''_{7}\}$  \\
  ... & $\{...\}$ & $\{...\}$ \\
  \bottomrule
\end{tabular}
\caption{Data format used to train the scoring function using list wise ranking loss}
\label {tab:dataformat}
\end{table}
Here let each data point $a$ be a sample/element from the set $\mathbb{A}$.
Each $y$ represents the ground truth preference score of objects belonging to a set $\mathbb{Y}$.
These preference scores of objects are relative to the objects within the input set.
Let $s$ be the scoring function to be learnt.
Hence, the declaration of $s$ is given by
$$
s : \mathbb{A} \mapsto \mathbb{R}
$$

As we can see from the table,  one instance in our data consists of a set of objects as input and a set of corresponding ground truths to train from.
To learn the function $s$ we optimize our list wise loss function.
This loss function takes as input the whole set of objects and their ground truth as one instance.
If we take any set $\mathbb{P}$ such that $\mathbb{P} \subseteq \mathbb{A}$,  the declaration of the list wise loss $L$ is hence given by
\begin{equation}
L : s(\mathbb{P}) \times \mathbb{Y}^{|\mathbb{P}|} \mapsto \mathbb{R}
\end{equation}

Where the scoring function $s$ applied to the $\mathbb{P}$ gives us the set of corresponding scores of all objects in $\mathbb{P}$.
We will consider the ground truth values to be $\mathbb{R}$ for our analysis as this is type of value
we have in our data sets.

In the next 2 sections we analyse ListNet and ListMLE,  the prominent listwise loss functions in the literature.

\section{Loss function: ListNet}

In this section we try to intuitively explain the ListNet idea proposed in~\cite{listwisebetter}.
Our objective is to learn the scoring function $s$ such that it returns scores that are similar in relevance/order when compared to the ground truth scores.
That is to say
$$
y_3 < y_{12} < y_1 \implies s(a_3) < s(a_{12}) < s(a_1)
$$
This would make the ranking of the objects equal to the ranking obtained by using the ground truth values.
Note that we do not need to get the exact ground truth scores.
This increases the number of acceptable functions that can be learnt by increasing the target function space.
This makes it easier to learn the scoring function.

Ranking is obtained by sorting the objects based on their respective scores.
Note that the sort functionality is non differentiable hence it is not a 
part of the ranking loss function.
Our loss function needs to be constructed using the following 2 lists:
\begin{itemize}
\item List of scores given by scoring function $s$.
\item List of scores given to us by ground truth.
\end{itemize}

The loss function must find some sort of a distance between the 2 given lists.
It then can reduce the distance by changing the parameters of the scoring function.

In ListNet,  a probabilistic approach is taken so as to account for any uncertainties in the ground truth values.
Consider selecting an object from the input set with a probability
$$
P = \frac{s(a)}{\Sigma_i s(a_i)} \;\;\; \forall i \in \{1, 2, 3, ...,  |\mathbb{P}|\}
$$
This make intuitive sense because the probability of selecting an object should be higher if it more relenvant and vise-versa.
Note that the score of any object by the scoring function can negative as well.
Therefore the score is passed through a strictly positive and increasing function $\phi$.
This changes the probability to
\begin{equation}\label{eq:objselection}
P = \frac{\phi(s(a))}{\Sigma_i \phi(s(a_i))} \;\;\; \forall i \in \{1, 2, 3, ...,  |\mathbb{P}|\}
\end{equation}

Equation~\ref{eq:objselection} is also referred to as top 1 probability of an object in~\cite{listwisebetter}.
This is because this gives the probability of ranking the object first when we are calculating the permutation probability of given list.

The proposed way to find the distance between 2 lists in ListNet is
\begin{itemize}
\item Find the top 1 probabilities of each object using the scores given by the scoring function.
\item Using the ground truth values,  find similar top 1 probabilities.
\item The cross entropy between the 2 entities gives us the "distance" between the 2 lists.
\end{itemize}

Let $P_{s(a)}$ represent the top 1 probability of an object using the scores given by the scoring function.
Similarly,   let $P_y$ represent the top 1 probability using its ground truth value.
The cross entropy used as a loss in ListNet is given by
\begin{equation}
L(\textbf{y},  {s(\textbf{a})}) = - \Sigma_i P_{s(a_i)} \log P_{y_i}
\end{equation}
Where $\textbf{y}$ and $s(\textbf{a})$ represent the ground truth values and the scores given by the scoring function.

\iffalse

\fi

\section{Loss function: ListMLE}\label{sec:listMLE}

ListMLE loss stands for, "List Maximum Likelihood Estimation" loss.
It is another type of list loss function that is similar to listNet.
As in ListNet,  the probability of selecting an object from the list is taken the same as given in equation~\ref{eq:objselection}.
However,  the final loss used in MLE is not cross entropy.
Rather it maximizes a likelihood estimation as the name suggests.

Let $\pi$ define any permutation of a list.
The probability of a permutation is nothing but the probability of selecting one document after another without replacement.
In our case the permutation probability of selecting 1 permutation using the selection probabilities given by equation~\ref{eq:objselection}  is~\cite{listwisebetter}
\begin{equation}\label{eq:firstMLEequation}
P_{\pi} = \prod\limits_{j=1}^{k} \frac{\phi(s(\pi_j))}{ \sum\limits_{t=j}^k \phi(s(\pi_k))}
\end{equation}
where $\pi_i$ is the object at position $i$ in the permutation $\pi$.

Applying log to the above equation gives us
\begin{equation}
\log P_{\pi} = \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi_j))}{ \sum\limits_{t=j}^k \phi(s(\pi_k))}
\end{equation}

However,  the question remains which permutation to use?
The best permutation for the given set of objects would be according to the true scores of the objects.
More precisely,  it would be the objects ordered in the descending order of their relevance scores.
Let this permutation be represented by $\pi^*$.
Hence our probability equation becomes
\begin{equation}
\log P_{\pi^*} = \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}

ListMLE maximizes this probability.
Since most we generally minimize the objective function,  the loss function of ListMLE is given by
\begin{equation}
L_{mle} = - \log P_{\pi^*}
\end{equation}
Expanding the right hand side of the equation gives us the final loss function that has to be minimised by any algorithm that uses ListMLE

\begin{equation}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}


Notice that in the calculation of the loss, the true score values of the objects are unused.
Which means that the actual scores of the objects do not matter.
The only constraint is that the scores must have the values that give the same permutation.
Even uneven scaling of the actual scores does not affect the output as long as the constraint is maintained.

In the case of ListNet,  however,  the true scores do matter.
The probability of selecting objects according to their true scores is used.
Hence the result is invariant only to linear scaling.

This advantage makes listMLE loss function superior to the ListNet as it makes the target space of functions bigger and hence the convergence can be quicker.
Because of this advantage we use ListMLE as a list loss function in our thesis.

\section{Position Enhanced Ranking}\label{sec:positionEnhancedRanking}

In many problem domains that use the ranking concept,  it may not be important that each object be placed at exact location as induced by its relevance.
For example,  when a search engine ranks its search results, it is more important to find the most important results and rank them correctly than to order the least important results correctly.

This is also the case in the problem of ranking HP configurations when the ranking model is used as a surrogate in an SMBO process.
In this process,  the ranking surrogate is only needed to obtain the most important HP configuration at each step in the optimization cycle.

Lan et al.  discuss this problem in detail in their paper,  "Position-Aware ListMLE: A Sequential Learning Process for Ranking"~\cite{positionawarerankinglistmle}.
However,  they reformulate the problem as a sequential learning process.
A more accessible approach is to weight each object component in our ListMLE by any decrease function $c$~\cite{TRLWO}.
This is possible because the listMLE loss function is in the form of a summation.
Hence the weighted ListMLE function is given by:

\begin{equation}
L_{mle} = -  \sum\limits_{j=1}^{k} c(j) \log \frac{\phi(s(\pi^*_j))}{ \sum\limits_{t=j}^k \phi(s(\pi^*_k))}
\end{equation}
Where $c(j)$ gives the weight of the rank j in the ordered list.
This is approach used in our model to improve our ranking loss function.
The type of decreasing function to use is discussed in more detail in chapter 4.
Note that it is also possible for using the weighting in the ListNet case as ListNET and ListMLE have similar forms.

\section{Uncertainty modelling using Deep Ensembles}\label{sec:uncertaintyDeepEnsembles}

Deep Neural Networks (DNNs) are machine learning models with very high representational capacity~\cite{Goodfellow-et-al-2016}.
Due to this property,  one can use them as surrogates for HPO objective functions.
But the issue is that DNNs do not quantify uncertainty trivially.
In fact the results can be seen as overconfident.
If used in any HPO optimization algorithm as surrogate,  such overconfident wrong predictions cause a lot of computational overhead by predicting inefficient HP configurations.

We are discussing in detail this concept because we use deep neural networks as a scoring function in our proposed ranking loss surrogate model.
Uncertainty estimation qualities of a surrogate have high importances especially if they are used in techniques like SBMO.
As the proposed model is studied as a surrogate in the SMBO technique,  we need to study how to model uncertainty efficiently using the underlying DNN architecture.

In the current literature,  uncertainty quantification methods using deep neural networks can be broadly classified into the following methods:
\begin{itemize}
\item Bayesian Neural networks~\cite{Goan-2020}.
\item Ensemble Approach using monte carlo drop out~\cite{JMLR:v15:srivastava14a}.
\item Ensemble approach using multiple neural networks.
\end{itemize}

In Bayesian neural network(BNN),  a prior over weights and biases is specified during the initialization of the BNN.
Given the data,  a posterior predictive distribution is calculated for all the parameters of the network (Weights and Biases).
One issue with this approach is that BNNs are very complex and difficult to train. 

Monte Carlo drop out is a regularization technique used during the training of neural networks.
With a certain probability,  connections between neurons are dropped.
Using this technique one obtains possibly $2^N$ neural networks where $N$ is the number of connections
in the artificial neural network.
We can get an ensemble of high capacity models for free.
It is normally only used during training to obtain regularization.

However,  if one uses Monte Carlo dropout during the evaluation,  we can get multiple results from the same input using this approach.
Given input $x$ and output $y = \textrm{NN}(x)$.  If we have $m$ neural networks obtained using Monte Carlo dropout,  we get $\{y_1, y_2... y_m\}$ outputs,  we can obtain the mean and variance of 
\begin{equation}\label{eq:simpleDNNensemble}
y_\textrm{mean} = \frac{\Sigma y}{m}  \;\;\;\;\;  y_\textrm{variance} =\frac{\Sigma(y - y_{mean})^2}{m-1}
\end{equation}
Please note that the $m-1$ in the denominator is due to Bessels Correction~\cite{besselcorrection} to reduce the bias in estimation.


Lakshminarayana et al.~\cite{DeepEnsemblePaper} propose another method to predict uncertainty using deep neural networks.
They propose that the uncertainty prediction can be done directly using a single neural network.
This is possible if we assume that the underlying uncertainty is a Gaussian distribution.
With this assumption, the neural network would have 2 outputs instead of one.
One for the mean of the prediction, say $\mu$, and the other for the variance,  say $\sigma^2$
 of the prediction.
One important point to note is that the variance cannot really be negative.
This is made sure by the authors to pass the output of the neural network through a strictly positive "softplus" function.
 
The authors propose to optimize the following loss function
$$
L_{de} = \frac{\log \sigma^2 }{2} + \frac{(y - \mu)^2}{2\sigma^2} + k
$$
Where both the outputs are some functions of the DNN parameters ($\theta$) and the input ($\textbf{x}$) i.e $\sigma^2 = f(\theta,  \textbf{x})$ and $\mu = g(\theta, \textbf{x})$.
Here, the back propagation finds and updates the parameters $\theta$ using the partial derivative:
$$
\frac{\partial L}{\partial \theta}
$$


We use this loss function to build deep ensemble surrogates for the HPO.
The prediction of uncertainty using ranking losses is however done using the simple ensemble approach given in equation~\ref{eq:simpleDNNensemble}.
This is because the integration of the loss function which learns both the mean and variance with the ranking loss functions is non trivial.

One simple approach is to simply use a combination of losses like
$$
L_{\textrm{total}} = L_{mse} + L_{de}
$$
However such loss functions would need a thorough theoretical analysis which is out of the scope of this thesis.
Hence we do not use this approach on our proposed model.

As there are are multiple neural networks, each predicting its own Gaussian distribution,
there needs to be a mechanism to integrate the results.
This is done using a mixture of Gaussian Distributions.
If there are $m$ neural networks in the ensemble,  the mean and variance are given by
$$
\mu_{final} =  \frac{\sum\limits_{i=1}^{m} \mu_i}{m} \quad \textrm{and} \quad \sigma_{final}^2 = \frac{\sum\limits_{i=1}^{m} (\sigma_i^2 + \mu_i^2) - \mu_{final}^2}{m}
$$

\section{Baselines}

In this thesis,  2 HPO techniques were implemented before studying the proposed model - Deep Ensembles and Few Shot Bayesian Optimization (FSBO).
Deep Ensembles were used as surrogates in the SMBO optimization.
They were studied with with 2 main objectives in mind.
First to study how uncertainty is estimated using Deep Neural Networks using the approach proposed by~\cite{DeepEnsemblePaper}.
This was a pre-requisite to implement the uncertainty in our proposed model as we use deep neural networks as a scorer in our model. 
Second to understand how a non transfer technique like GP would work for our given problem.
Note,  it is also possible to make the Deep Ensemble surrogate a transfer technique by meta training it before using it in the optimization cycle.

The second technique implemented was FSBO.
We choose this because as this gave the state of the art results on the HPO-B benchmark that we use.
(The benchmark is discussed in the later chapters).
Studying FSBO also gave us an idea on how to implement the transfer mechanism in our proposed model.
In addition to this we used Random search and GP as standard baselines for result comparison.

Both these FSBO and DE have built in capability for uncertainty estimation.
Hence we have to use an acquisition function during the optimization cycle of SMBO.
Expected improvement was used in all models that deal with uncertainty.
This is to maintain consistency in results across all the models.
Further more it also has advantages on other acquisition function~\cite{Jones1998}.


The following 2 sections discuss the implementation details of the baselines methods used in our thesis.

\subsection{Deep Ensemble}

As previously mentioned Deep Ensembles were implemented according~\cite{DeepEnsemblePaper} as a non transfer surrogate.
Hence,  there was no meta-training done for the Deep Ensembles.
Consequently the usage of DE as a surrogate was quite similar to that of Gaussian processes.
SMBO with deep ensemble was implemented as shown in algorithm~\ref{alg:deepEnsembleFinetuning}~\cite{pineda2021hpob}.

\begin{algorithm}[H]
\caption{SMBO with Deep Ensemble surrogate}
\label{alg:deepEnsembleFinetuning}
\begin{algorithmic}
    \State $X_{known},  Y_{known} \gets$ Initial HP configurations.
    \State $X_{pending} \gets$ HP configurations to evaluate.
    \State $k \gets$ Number of evaluation cycles
    \For{$i < k$}
        \State DE $\gets$ Randomly initialize Neural Networks. 
        \For {$nn \in$ DE}  \Comment{Can be trained in parallel}
            \State train($nn$) with $X_{known},  Y_{known}$
        \EndFor
        \State $EI_{scores} \gets$ EI( $X_{pending}$ ) \Comment{Expected Improvement scores}
        \State $x^* \gets $ best $(EI_{scores})$
        \State $y^* \gets f(x^*)$ \Comment{HP objective function evaluation}
        \State $X_{known} \gets X_{known} \cup x^*$
        \State $Y_{known} \gets Y_{known} \cup y^*$
        \State $X_{pending} \gets X_{pending} \setminus x^*$ 
        \State $i \gets i + 1$
    \EndFor
    
\end{algorithmic}
\end{algorithm}

We use similar procedures like Algorithm~\ref{alg:deepEnsembleFinetuning} for other models i.e FSOB implementation and the proposed Ranking Loss surrogate model.
The only difference is that the surrogate and its training step differ with different methods.

A few points are worth noting here.
First,  we see that the algorithm evaluates a set of discrete HP configurations in the HP search space.
This is the same approach we take when we apply our model because the ranking concept we use requires a set of defined objects.
The advantage of using this approach is that there is no restriction on the type of search space we optimize.
It may be discrete or continuous.
If it is continuous,  we only have to discretize it upto a required granularity based on our computational resources.

Secondly,  we see that at each evaluation cycle,  a new set of neural networks are trained.
Old trained neural networks are discarded.
This rationale behind this is discussed in chapter 4.
We also use this sort of initialization in other models.
Finally,  as the neural networks are independent of each other,  they can be trained in parallel.
This makes the usage of this model scalable.

One question that remains to be answered is what sort of architecture is used by our neural networks.
For this 2 architectures were analysed - undivided neural network as shown in Figure~\ref{fig:undividedarchitecture} and a neural network divided at its tail as shown in Figure~\ref{fig:dividedarchitecture}
\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$X$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCn)                      [right=of FCmiddle]        {$FC_n$};
\node[roundnodey]      (mean)                             [right=of FCn] {$\mu$};
\node[roundnodey]      (variance)                             [below=of mean] {$\sigma^2$};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCn.west);
\draw[->] (FCn.east) -- (mean.west);
\draw[->] (FCn.east) -- (variance.west);
\end{tikzpicture}
\caption{Example of an undivided Neural Network architecture}
\label{fig:undividedarchitecture}
\end{figure}

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$X$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCpenaltimate)             [right=of FCmiddle]        {$FC_{n-1}$};
\node[squarednode]      (FCn1)                      [right=of FCpenaltimate]        {$FC_{n_1}$};
\node[squarednode]      (FCn2)                      [below=of FCn1]        {$FC_{n_2}$};
\node[roundnodey]      (mean)                             [right=of FCn1] {$\mu$};
\node[roundnodey]      (variance)                             [right=of FCn2] {$\sigma^2$};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCpenaltimate.west);
\draw[->] (FCpenaltimate.east) -- (FCn1.west);
\draw[->] (FCpenaltimate.east) -- (FCn2.west);
\draw[->] (FCn1.east) -- (mean.west);
\draw[->] (FCn2.east) -- (variance.west);
\end{tikzpicture}
\caption{Example of a divided Neural Network architecture}
\label{fig:dividedarchitecture}
\end{figure}

In both these figures,  FC stands for fully connected layers.
As the undivided architecture is closer to the neural network used in the proposed ranking loss surrogate model,  we used it for the sake of consistent comparison.
3 Fully connected layers of 32 neurons each were used for each neural network.
All neural networks used the same architecture. 
The training was done using the the Adam optimizer with full batch gradients.
This is because the number of data points in the evaluation cycle are very few.
Each neural network was trained for a 1000 epochs with a learning rate of 0.02.
We do not use adversarial examples as proposed in the deep ensemble paper because it was giving bad results.

\subsection{FSBO}
Few Shot Bayesian Optimization (FSBO) is a transfer learning HPO method that utilizes a meta learnt surrogate for knowledge transfer mechanism.
It reformulates the problem into few shot learning task.
In the context of our problem,  this means meta learning thoroughly from the existing meta data and then adapting to the new task at evaluation cycle by fine tuning a few training epochs.
Therefore the following 2 steps are required to be done in a chronological order
\begin{itemize}
\item Meta training - For knowledge transfer.
\item Fine tuning - For few shot learning.
\end{itemize}

For the purpose of knowledge transfer, the authors make use of a deep kernel surrogate proposed in by wilson et al~\cite{pmlr-v51-wilson16}.
Here,  a neural network is used to transform points in the HP search space to a latent space.
Kernels are then applied to this latent space in a Guassinan process to obtain a probabilistic evaluation.
The deep kernel kernel can be represented as~\cite{fsbopaper}
$$
k(\phi(\textbf{x}, \textbf{w})  ,   \phi(\textbf{x'}, \textbf{w}) |  \mathbf{\theta})
$$
Where $\textbf{x}$ and $\textbf{x'}$ are HP inputs in the original search space,
$\mathbf{\theta}$ and $\textbf{w}$ are parameters of kernel $k$ and the neural network $\phi$.
We used the implementation of deep kernels provided by Patacchiola et al.~\cite{patacchiola2020bayesian} in this thesis.

During the meta training step, we train our surrogate by learning the parameters $\mathbf{\theta}$ and $\textbf{w}$.
A New FSBO model and consequently new surrogate parameters have to be learnt for every new search space.
This is the case even if the input dimensions are of the search space domain are the same.
This is because every HP search space represents different machine learning model and hence has a different HP response surface.
During training we first used an RBF kernel and then used a matern $\frac{5}{2}$ kernel.
A fully connected neural network was used to obtain the latent space representation.

If an assumption is made that the knowledge from the meta data is enough to predict the best HP configuration across all future dataset,  the fine tuning step may be skipped.
However,  this is rarely the case as there are always variations in new dataset (Even though the model being optimised is the same).
Therefore,  the hyper parameter response surface would also be different.

The optimization algorithm in the case of FSBO is similar to Algorithm~\ref{alg:deepEnsembleFinetuning} with a minor change.
The acquisition function used in this model is also expected improvement.
The concept of restarting the fine tuning each time is also proposed in the FSBO paper~\cite{fsbopaper}.
The difference here is that at each evaluation step,  the stored model is loaded anew and fine tuned before using it for predicting the best model.
The restart happens from the meta trained FSBO model and not from a randomly initialized model as in the case of Deep Ensembles.

In our implementation,  Adam optimizer was used in both the training and fine tuning steps.
Note,  however,  that the learning rate of the kernel parameters and the neural network parameters were identical.
The learning rate used for meta-training was 0.0001 and that used for fine tuning was 0.03 different.
We utilized cosine annealing during the fine tuning cycle.
The rationale for using this is discussed in detailed in chapter 4.

We used early stopping mechanism for meta training because we wanted to avoid huge computation costs and over fitting of the model during training.
We took advantage of the split of meta-validation data to do the early stopping.
We saved the best model in our implementation and if the validation loss went greater than lowest validation loss.
The training was stopped if the training error was consistently higher than the validation error for a set number of epochs.
In our case it was 10 epochs but this is easily configurable.

\iffalse
\subsubsection{Implementation issues}
After completing the own implementation,  we found that the results obtained were not in par with the paper.
For this reason, the results are used only for the first stage.
The results already present in the benchmarking data of~\cite{pineda2021hpob} were used for comparing this result with our implementation.
\fi

% \section{Evaluation Strategy}



\chapter{Method}\label{chap:ProposedIdea}

% \section{Proposed Idea: : Ranking Loss Surrogates}

The choice of surrogate to use in model based optimization is very crucial.
Does the surrogate have enough representational capacity?
Does it have the capability of representing uncertainty?
How is the surrogate learnt?
These are some of the questions that need to be answered before selecting a surrogate model.
The selection of a surrogate model has a direct impact on the performance of the model based optimization algorithm.

In the quest to improve HPO surrogates,  we propose and analyse a new type of surrogate model that is based on the concept of ranking.
There are 2 components of our proposed idea

\begin{itemize}
\item The learning mechanism of the surrogate model.
\item The surrogate model itself.
\end{itemize}

This chapter discusses in detail both these components.
In this chapter,  we first assume that a good surrogate model of sufficient representational capacity already exists.
We use a simple Deep Neural network for this purpose.
We then analyse and implement the proposed learning algorithm that uses the concept of ranking.
We then do a simple case study of inverse mapping using the learnt model.
Finally we build a ranking model surrogate to improve the performance of any model based HPO algorithm.
We use SMBO as a reference for our study.

\section{Basic scoring model : Deep Neural Network}

In order to discuss the implementation details of ranking loss functions,  we need to first have a reference scoring model.
Since we also want representational capacity to be high a fully connected neural network is used as a scorer to begin with.
This is depicted in Figure~\ref{fig:basicScoringModel}.
\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]      (X)                             {$X$};
\node[squarednode]      (FC1)                      [right=of X]        {$FC_1$};
\node[squarednode]      (FCmiddle)             [right=of FC1]        {...};
\node[squarednode]      (FCn)                      [right=of FCmiddle]        {$FC_n$};
\node[squarednode]      (RangeController)                      [right=of FCn]        {Range Controller};
\node[roundnodey]      (y)                             [right=of RangeController] {$y$};

%Lines
\draw[->] (X.east) -- (FC1.west);
\draw[->] (FC1.east) -- (FCmiddle.west);
\draw[->] (FCmiddle.east) -- (FCn.west);
\draw[->] (FCn.east) -- (RangeController.west);
\draw[->] (RangeController.east) -- (y.west);
\end{tikzpicture}
\caption{Basic scoring model}
\label{fig:basicScoringModel}
\end{figure}

As we can see this model is almost similar to the model used in the Deep Ensemble baseline implementation (Figure~\ref{fig:undividedarchitecture}).
The difference between the 2 is that Figure~\ref{fig:undividedarchitecture} had 2 outputs whereas Figure~\ref{fig:basicScoringModel} has one output.
In addition to this there is a very crucial "Range Controller" component added to our scoring model.

Lets say \{\textbf{X}, \textbf{y}\} be the training data used for training a generic machine learning model $m$.
Most of the loss functions utilize the values present in \textbf{y} as a reference to train the model.
After the training completes,  the range of the model $m$ is not vastly different from the the range of the observed ouputs \textbf{y}.
This is not guaranteed in the loss function $L_{mle}$.
As mentioned in section~\ref{sec:listMLE} the only thing that matters is that their corresponding ground truth relevant scores always should yield the same order.
Hence the scoring function $s$ learnt using our ranking loss function can have an arbitrary range.

In our models,  we use the strictly positive increasing function $\exp$ as proposed in~\cite{listmlepaper}.
Hence our $L_{mle}$ becomes 
\begin{equation}\label{eq:overflowissue}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\exp(s(\pi^*_j))}{ \sum\limits_{t=j}^k \exp(s(\pi^*_k))}
\end{equation}
Even though theoretically the range of our scoring model $s$ does not need to be restricted,  we do have practical limitations due implementation constraints of floating point numbers used in computer software.
Due to the exponentiation in our loss function,  if the range of $s$ is too high,  the values in equation~\ref{eq:overflowissue} may overflow.
If the range is too small,  the values may underflow.
In any case,  we are bound to get nan (Not A Number) exceptions in our implementations.

For this reason we control the range of our scorer by passing the output of the deep neural network through a $\tanh$ function.
If we would like to strictly limit the range of our function between $[-k,  k]$,  then we can pass the output of our scorer through the following function
\begin{equation}\label{eq:tanhEquation}
k * \tanh(\alpha * s(\textbf{X}))
\end{equation}

Where $\alpha$ is the smoothness factor which is inversely proportional to the smoothness of the $\tanh$ graph~\cite{tanhstackoverflowanswer}.
Figure~\ref{fig:tanhGraph} shows how the to vary the smoothness and range of the output using $\alpha$ and $k$ respectively.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/tanhGraph}
    \caption{Effect of varying $k$ and $\alpha$ in equation~\ref{eq:tanhEquation}}
    \label{fig:tanhGraph}
\end{figure}

We used $k=2$ and $\alpha=0.01$ for our scorer.

\subsection{ListMLE Implementation}

To train the scorer the loss function ListMLE was implemented in python using the PyTorch~\cite{PyTorch} deep learning library.
Algorithm~\ref{alg:listMLEAlgorithm} shows the steps used to obtain the loss scalar before back propagating it using the autograd functionality of PyTorch.
Please note that the actual implementation is a little more sophisticated due to the use of multi dimensional tensors.

For numerical safety and accuracy the values of the predicted score reduced by a constant factor.  Here the constant factor is the biggest value of the list itself.
This is possible because we are using $\exp$ as our strictly increasing function:
$$
\frac{e^{a_1}}{\sum\limits_{i} e^{a_i}} = \frac{e^{a_1 + k}}{\sum\limits_{i} e^{a_i + k}} 
$$

\begin{algorithm}[h]
\caption{Loss ListMLE Algorithm}
\label{alg:listMLEAlgorithm}
\hspace*{\algorithmicindent} \textbf{Input} : $l_{predicted} \in \mathbb{R}^k$ \Comment{Relevance scores predicted by the scorer}\\
\hspace*{\algorithmicindent} \textbf{Input} : $l_{actual} \in \mathbb{R}^k$ \Comment{Actual relevance scores}\\
\hspace*{\algorithmicindent} \textbf{Output} : Loss $\in \mathbb{R}$
\begin{algorithmic}[1]
    \Procedure{ListMLE}{$l_{predicted}$,  $l_{actual}$}
    \State $l_{predicted} \gets l_{predicted} - \max(l_{predicted})$
    \State $l_{predicted} \gets \exp(l_{predicted})$
    \State $sum \gets 0$
    \For{$i < k$} \Comment{$k$ is list size here}
        \State $sum \gets sum$ + \textsc{Top1LogProb}($l_{predicted}$,  $l_{actual}$)
        \State $l_{predicted},  l_{actual} \gets $ \textsc{RemoveTop1}($l_{predicted}$,  $l_{actual}$)
        \State $i \gets i + 1$
    \EndFor
    \State  Return $-1 * sum$
    \EndProcedure
     \Procedure{Top1LogProb}{$l_{predicted}$,  $l_{actual}$}
    \State $j \gets \textrm{argmax}(l_{actual})$
    \State $prob \gets \frac{l_{predicted}[j]}{\sum l_{predicted} }$
    \State  Return $\log(prob)$
    \EndProcedure
         \Procedure{RemoveTop1}{$l_{predicted}$,  $l_{actual}$}
    \State $j \gets \textrm{argmax}(l_{actual})$
    \State $l_{predicted} \gets$ \textsc{RemoveElementAtIndex}($l_{predicted}$, $j$)
    \State $l_{actual} \gets$ \textsc{RemoveElementAtIndex}($l_{actual}$, $j$)
    \State  Return $l_{predicted}$,  $l_{predicted}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

It is very evident from the implementation of Algorithm~\ref{alg:listMLEAlgorithm} that it calculates the permutation probability of the objects in the list as described in Section~\ref{sec:listMLE}.
However,  the algorithm is not efficient.
This is because it modifies and removes elements in the lists.
if lists are implemented as contigous elements (like arrays) the time complexity of running would be $O(n^2)$ where $n$ is the list size.

One way to get around this problem is to sort the lists before calculating the permutation probability.
This implementation is done in the paper~\cite{Pobrotyn2020ContextAwareLT}.
This makes the time complexity $O(n \log n)$
We use this implementation due to its efficiency.
Algorithm~\ref{alg:listMLESorted} depicts this.
More precisely the ListMLE is first expanded to the following equation.
\begin{equation}\label{eq:sortedListMLEequation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log \sum\limits_{t=j}^k \exp(s(\pi^*_k)) - \log \exp(s(\pi^*_j)) \right)
\end{equation}
Since the scores are sorted in the correct order we can re-write the equation as 
\begin{equation}\label{eq:sortedListMLEequation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log \sum\limits_{t=j}^k \exp(s^*(k)) - \log \exp(s^*(j)) \right)
\end{equation}

Now $\sum\limits_{t=j}^k \exp(s^*(k))$ is nothing but the reverse cumulative sum where the first element is the sum of all elements and the next element is the sum of all elements starting from position 1 and so on.
Let this be represented by $Q$.
$\log \exp(s^*(j))$ can be directly written as $s^*(j)$ if natural logarithm is taken.
Hence,  the equation that is implemented in Algorithm~\ref{alg:listMLESorted} is~\cite{Pobrotyn2020ContextAwareLT}.
\begin{equation}\label{eq:sortedListMLEequation}
L_{mle} = \sum\limits_{j=1}^{k} \left( \log Q(j) - s^*(j) \right)
\end{equation}

\begin{algorithm}[h]
\caption{Loss ListMLE Algorithm (sorted)}
\label{alg:listMLESorted}
\hspace*{\algorithmicindent} \textbf{Input} : $l_{predicted} \in \mathbb{R}^k$ \Comment{Relevance scores predicted by the scorer}\\
\hspace*{\algorithmicindent} \textbf{Input} : $l_{actual} \in \mathbb{R}^k$ \Comment{Actual relevance scores}\\
\hspace*{\algorithmicindent} \textbf{Output} : Loss $\in \mathbb{R}$
\begin{algorithmic}[1]
\Procedure{ListMLESorted}{$l_{predicted}$,  $l_{actual}$}
\State $l_{actual}, $ IndexOrder $\gets$ \textsc{Sort}($l_{actual}$)
\State $l_{predicted} \gets$ \textsc{SortWithIndexOrder}($l_{predicted}$,  IndexOrder)
\State $l_{predicted} \gets l_{predicted} - \max(l_{predicted})$ \Comment{Numerical Stability}
\State $prob \gets 0$    
    \For{$i < k$}
        \State $prob \gets prob + \log Q[i] - l_{predicted}[j]$
        \State $i \gets i + 1$
    \EndFor
\State Return $prob$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Weighted Loss}
As discussed in Section~\ref{sec:positionEnhancedRanking},  the ranking problem we are have in HPO is not the general ranking problem.
It is more important for our ranking function to order the top part of the list than the bottom part.
This is evident from Algorithm~\ref{alg:deepEnsembleFinetuning} where at every optimization cycle we are only concerned about selecting the best available configuration for true evaluation.
Keeping this constraint in mind,  we need to have a weighting strategy such that
$$
c(j) \propto \frac{1}{j}
$$
where $j$ is the ranking of the object and $c(i)$ specifies its weight.

In this section we discuss about some strategies to do this weighting for our problem.
The concept of biased ranking loss is discussed in the paper, "Top-Rank Enhanced List wise Optimization for Statistical Machine Translation" by  Chen et. al~\cite{TRLWO}.
Here Chen et. al proposes a position based weighting of the ranking function such that:
\begin{equation}
w_j = \frac{k - j + 1}{\Sigma_{t=1}^k t}
\end{equation}
where $w_j$ represents weight of the object at position $j$ in the ordered list.
This strategy decreases the weights linearly across the ranks.
The issue with this weighting is that it takes the weight of an object in the middle of the list to be $0.5$ times the weight of the object at the top of the list.
This however is not what we want for our case.
We need a sharper decrease in weights across the ranks.

We could use 2 simple strategies of weighting for our problem
\begin{itemize}
\item Inverse linear weighting given by $w_j = \frac{1}{j}$
\item Inverse logarithmic weighting given by $w_j = \frac{1}{\log (j+1)}$
\end{itemize}

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.65]{images/weightingfunctions}
    \caption{Different Weighting Functions}
    \label{fig:weightingfunctions}
\end{figure}

Figure~\ref{fig:weightingfunctions} shows all 3 weighting strategies.
In the case of inverse linear strategy,  we see that the weight becomes extremely small after only a few ranks in the list.
This has an unwanted consequence for our problem.
Because the weights of objects at the end of the list are too small,  the ranking function may not learn to rank optimally.

The inverse log weighting was found to be most suitable because it neither completely ignores the objects at the end of the list nor does it given them too high an importance.
In fact,  the weights of the rest of list are very similar which is what we want - all objects at the tail of the list are equally "unimportant" in our case.

Another advantage of using inverse weighting is that we can employ parallelism to enhance our HPO during the optimization cycle.
For example,  at every step the top $n$ configurations can be selected,  and all of them can be tried for optimization.
This overcomes any uncertainties that may occur while ranking the top part of the list. 
The parallelism is less effective when using the weighting proposed by Chen et al.


\iffalse
Applying this to HPOB... (With or without query)
    First learning from first search space.
    Even with this the loss curves are very smooth if we use 2 * tanh(0.01 * x)
\fi
    
\section{Case study with inverse mapping}

In this case study,  we study how the learnt ranking function behaves when we use different parameters to train.
For this a toy example of sorting in the descending order is considered.
    
The main problem that we try to study here is - Is it possible to train a ranking function using the ListMLE loss function such that it learns inverse mapping of points on a number line.
Consider numbers sampled from the range $[k,  p]$ where $k,p > 0$ and  $k,p \in \mathbb{R}$.
If we take 2 numbers $x_1, x_2 \in [k,  p]$,  we need to learn a mapping $s: x \mapsto \mathbb{R}$ such that $s(x_1) \leq s(x_2)$ when $x_1 \geq x_2$.
Consequently we could sort the numbers based on the output of the scorer to obtain a descending sorted order.

Consider a list $l = \{x_1, x_2, ...x_n\}$ where $x_i \in [1, 100]$.
Let $s(x \mid \theta) \mapsto \mathbb{R}$ be our scoring function parametrised by $\theta$.
Here, one list contains $n$ data points sampled from $[1, 100]$.
Then the loss function we would use to learn our scorer is
\begin{equation}
\underset{\theta}{\rm argmin} \quad L_{mle}(s(l \mid \theta),  - k * l)
\end{equation}
where $k \in \mathbb{R}$.
Note that second parameter of list wise loss function is scale invariant hence scaling the list has no effect on the loss output (Section~\ref{sec:listMLE}).

%    #       Result -> Sorting possible provided the output range of the model not restricted
%    #       For now we sample train/val data from the same distribution.
  %  #   List of numbers to test with = {-1 to -100}
%    #       Result -> This is not possible because the model will map this to extreme values as it
 %   #                 is not distribution dependent.
 %   #   Check the percentage of the lists in the correct sorted order.

The validation data taken from 3 different ranges
\begin{itemize}
\item Same range as the training data $[1, 100]$
\item Completely different range as seen by the scorer during training i.e $[-100, -1]$
\item Hybrid range i.e $[-50, 50]$
\end{itemize} 

To evaluate our scorer,  we first sample the validation data from the above ranges,
We then check the percentage of the lists that are correctly sorted during our testing time.
We used the same values of $k$ and $\alpha$ as used in the basic scoring model.
During training a batch size of 100 lists was used.
We try sort 1000 lists according to the learnt scorer's results.
The accuracy gives the fraction of the lists that were sorted in 1000 lists.
We report the average of 5 runs in the Table~\ref{table:caseStudyResults}.

\begin{table} [h!]
\centering
\resizebox{\linewidth}{!} {
\begin{tabular}{ | c | c | c | c | c | c | }
\hline
\textbf{Training Epochs} & \textbf{List size} & \textbf{Learning Rate} & \textbf{In-range Acc.} & \textbf{Out-range Acc.} & \textbf{Hybrid Acc.} \\ [0.5 ex]
\hline \hline
1000 & 3 & 0.0001 & 0.99 & 0.27 & 0.40\\
100 & 3 & 0.0001 & 0.77 & 0.29 & 0.11\\
100 & 30 & 0.0001 & 0.80 & 0.79 & 0.46\\
100 & 100 & 0.0001 & 0.99 & 0.0 & 0.39\\
1000 & 100 & 0.0001 & 1.0 & 0.71 & 0.48\\
\hline
\end{tabular}
}
\caption{Sorting Accuracies at test time}
\label {table:caseStudyResults}
\end{table}

We find from the tabulated results that it is important to completely learn the function by running a higher number of epochs.
To comprehensively learn the scoring function,  it is crucial to have a larger list size.
This will make our scorer model work well when the input is from the same distribution seen during training.
Moreover,  it also gives reasonable when the input comes from the distribution edge (i.e from a location close to the distribution).
These are good properties to have in any machine learning model.

The loss function is not weighted in this toy example because sorting requires that each data point be on its correct location.
Therefore,  our problem is not a direct generalization of this toy example.
Moreover the input domain in the toy example is quite simple as compared to the real world problems.
Nevertheless,  we do take these results into account to decide on the list size for training our proposed ranking surrogate model.

\section{Method training and optimization}

The proposed method in this thesis, "Ranking loss surrogate" is a transfer HPO model.
For this reason its working principle is similar to that of FSBO model.
Like FSBO we use surrogate learning as a knowledge transfer mechanism.
Hence the 2 main parts of using our optimization process are
\begin{itemize}
\item Meta learning the ranking loss surrogate
\item Using the trained surrogate in the evaluation cycle (with or without fine tuning).
\end{itemize}

\subsection{Meta training the surrogate}\label{sec:rlmetatraining}

The space of hyper parameter configurations in which we are trying to get an optimum during any HPO is called HP search space.
This search space is different for different machine learning models.
Hence we need to train different ranking loss surrogate models for different search spaces.

The same machine learning model,  however,  can be used to fit different data sets or tasks.
This leads to different HP response surfaces in the same search space.
Hence,  we obtain different meta data when the same model is optimized on different datasets (or tasks).
The goal of training our model is to use different meta-data sets (or tasks) from a single search space to train our model.
The idea is to learn all the common characteristics of the tasks and transfer this knowledge to the new task.

We use stochastic gradient descent to train our model.
For this we need to sample a batch of data from the given meta data.
There are 2 ways to do this:
\begin{itemize}
\item Double sampling: First sample the task then sample the meta data within the task.
\item Sample meta data from all tasks.
\end{itemize}

We use the first method of sampling our data.
This is because due to higher level of sampling,  we have obtain good regularization,
faster training and a possibility to use a lower learning rate.
We use Adam optimizer with a learning rate of 0.001 for our model.
We do our training for 5000 epochs and in each epoch we take 100 steps in which each step does 1 double sampling.
While sampling data points from within a meta data set, we sample without replacement.
We use the number of lists (batch size) as 100 and the size of each list also 100.
Algorithm~\ref{alg:RLSMetaTraining} illustrates us a brief skeleton of our meta training procedure.
After the training,  every model is saved on to a persistent location (e.g. hard disk) so that it can be load when necessary.

\begin{algorithm}[h]
\caption{Ranking Loss surrogate meta training}
\label{alg:RLSMetaTraining}
\hspace*{\algorithmicindent} \textbf{Input} : $epochs \in \mathbb{I}$ \\
 \hspace*{\algorithmicindent} \textbf{Input} : $X_{train}$,  $y_{train}$  \Comment{Meta data used to train} \\
\hspace*{\algorithmicindent} \textbf{Input} : $s_{\theta}$ \Comment{Model to train}
\begin{algorithmic}[1]
\Procedure{MetaTrain}{$s_{\theta}$,  $X_{train}$,  $y_{train}$, $epochs$}
    \For{$i < epochs$}
            \For{$j < 100$}
                \State $B_{X}, B_{y} \gets$ \textsc{DoubleSample}($X_{train}$) \Comment{Get the training batch}
                \State $y_{pred} \gets s_{\theta}(B_{X})$
                \State $\textrm{loss} \gets L_{mle}(y_{pred},  B_{y}) $
                \State $g \gets \frac{\partial \textrm{loss}}{\partial \theta}$ \Comment{$g$ stands for gradient}
                \State Use $g$ to update $\theta$ using the Adam optimizer
            \EndFor
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Fine tuning}\label{sec:rlfinetune}
Fine tuning is the second major part of the optimization process.
This may be considered optional for transfer HPO models like ours.
However,  we found that it improved the HPO evaluation cycle performance.

We take the SMBO optimization of Deep Ensembles in Algorithm~\ref{alg:deepEnsembleFinetuning} as an example to understand the fine tuning process.
In this algorithm,  at every evaluation cycle the deep ensembles are retrained with the seen evaluations of the true HPO objective function.
We need to modify Algorithm~\ref{alg:deepEnsembleFinetuning} by replacing Deep Ensemble training with the fine tuning of our model.
The fine tuning  process is exactly similar to the \textsc{MetaTrain} procedure in Algorithm~\ref{alg:RLSMetaTraining}.
The only difference is that we use seen evaluations instead of meta training data for training.
To avoid duplication,  we do not re-write the complete algorithm again.
We do fine tuning for 300 epochs.
The number of epochs can be varied based on the computation resources available.

Like FSBO,  at every evaluation cycle,  the model is reloaded from the saved state before being fine tuned.
In the next section we discuss why this restart is required.
Subsequently we talk about the use of cosine annealing used during fine tuning.

\subsubsection{Requirement of restarting training}\label{sec:restart}

During the implementation of deep ensembles,  we found that the model performs much better in the HPO evaluation cycle when we train it from scratch at every step (acquisition step).
This is counter intuitive because an already trained model should converge to a local optima quickly.
One of the reasons for this performance anomaly is that the model gets biased towards the points that are observed in the starting steps of the optimisation cycle.
Lets say we have 2 models 
\begin{itemize}
\item $\textrm{m}_{\textrm{restart}}$ which always restarts training at every acquisition step.
\item $\textrm{m}_{\textrm{reuse}}$ which trains the model trained in the previous optimization steps.
\end{itemize}

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/bias25}
\caption{Bias at 25th optimization cycle}
    \label{fig:bias25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/bias50}
\caption{Bias at 50th optimization cycle}
    \label{fig:bias50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/bias100}
\caption{Bias at 100th optimization cycle}
    \label{fig:bias100}
\end{figure}


Let the models be fine tuned for 100 epochs at each step.
Let there be just 1 seen HP configuration to start with.
The figures~\ref{fig:bias25},~\ref{fig:bias50},  and ~\ref{fig:bias100} show the number of times each observed HP configuration is used for training the model at the 25th, 50th and 100th optimization cycle step by the $\textrm{m}_{\textrm{reuse}}$ model.

Generally all the data is used during fine tuning due to data scarcity.
Hence,  in our example the number of times an HP configuration is used scales with the number of epochs trained.
The heavy bias that is present in the figures is actually not intended.
This is because all observations should be treated equally in any training/fine-tuning step.
When we use $\textrm{m}_{\textrm{restart}}$ every known HP configuration is used only 100 times at every evaluation step.
Hence it is better to restart the model before fine tuning.

The second reason for restarting is that the model may get stuck at a stubborn local minima at any fine tuning step $n$ where $1 <= n <= 100$ in our case.
Coming out of this local minima may require the response surface to change very drastically.
The response will change like this only when the training data distribution significantly changes.
This is not possible in the sequential process of SMBO because at every step only 1 new HP configuration is added to the known HP configurations.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/localMinima}
    \caption{Figure showing changes in response surfaces and a stubborn local minima}
    \label{fig:localMinima}
\end{figure}

Figure~\ref{fig:localMinima} depicts this issue for a simple 1 dimensional case.
Consider the red curve.
It represents the HP response surface for $k$ known data points.
When we add $(k+1)^{th}$ data point,  most of the response surface remains the same.
Only part of it changes.
This is represented by the blue response surface.
If our model is already trained for the red curve,  it becomes extremely difficult for it to come out of local minima because of the minimal changes in the response surface.
We call this minima a stubborn local minima.
This is depicted by the brown dot in the figure.

If however,  we reload and retrain our model from the beginning there is more chance of it reaching the good local optima in the blue curve if it descends from the other direction.
The number of ways to reach a good local minima increases exponentially with the increase in the HP search space dimensions.
Hence, restarting the training (from the saved model in our case) should improve our fine tuned model.

Because of these reasons,  we use the restart mechanism during the fine tuning of our ranking loss model (and also baseline implementations).
This makes the model more robust.

\subsubsection{Using cosine annealing}

We plotted the fine tuning loss curves of our model
We found that these loss curves were very jittery.
We noticed the same behaviour when fine tuning FSBO model.
Figure~\ref{fig:jitteryFTLoss} shows the fine tuning loss for one of the search spaces.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/jitteryFTLoss}
    \caption{Figure showing jittery loss function curve}
    \label{fig:jitteryFTLoss}
\end{figure}

We hypothesise that these jittery losses are because the learning rate is not suitable with the response curve's curvature at the targeted local minima.
One solution to this is to use a smaller learning rate.
However,  the fine tuning using smaller learning rate will take a long time.
The solution we proposed and utilized was using cosine annealing.

There are couple of advantages of using cosine annealing.
First the initial learning rate is kept high in order to give the optimizer time to get to an area close to the local minima if it has not done so.
Thereafter,  the learning rate declines quickly to the target small learning rate.
This helps the model to get deep into the local minima and hence the possibility of the optimization to jump out of the local minima is very less.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.45]{images/goodFTLoss}
    \caption{Figure showing a loss curve obtained by using cosine annealing}
    \label{fig:goodFTLoss}
\end{figure}

Figure~\ref{fig:goodFTLoss} shows a fine tuning loss curve obtained using cosine annealing.
Even though this strategy may not be a perfect one,  it reduces the possible problems that could prop us when using a constant learning rate.
During training,  however,  we use a constant learning rate as can use the validation loss as a reference for over fitting or under fitting.

\section{Ranking Surrogate Model}

After explaining the learning mechanism,  we now turn to 2 important components that are used in our method.
The first is Deep Set to make our model context aware.
The second is the use of uncertainty for improving the output of the model.

\subsection{Using Deep Sets to build context aware models}\label{sec:DeepSetWithModel}

We have already discussed the fundamental ideas of using deep sets in  ~\ref{sec:DeepSets}.
In this section we discuss about how to add deep sets in the ranking loss surrogate model architecture and then how to train the same.

The basic idea is to precondition our scoring function $s_{\theta}$ on known evaluations of the target HP space.
These known evaluations act as a support for the scoring function.
Hence they are subscripted as "s" in the equation.
We then query the scoring function to get the relevant scores of new $\textbf{X}$ which are subscripted as "q" in the equation.
The scoring function hence becomes:
$$
s_{\theta}(\textbf{X}_{q} | \textbf{X}_{s}, \textbf{y}_{s})
$$

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]          (Xq)                                                     {$\textbf{X}_q$, $\textbf{y}_q$};
\node[roundnode]          (Xs)                        [below=of Xq]     {$\textbf{X}_s$};
\node[squarednode]      (DeepSet)              [right=of Xq]        {Deep Set};
\node[squarednode]      (Concatenation)             [below=of DeepSet]        {Concatenation};
\node[squarednode]      (DNN)                      [right=of Concatenation]        {Basic Scorer};
\node[roundnodey]        (ys)                             [right=of DNN] {$\textbf{y}_s$};

%Lines
\draw[->] (Xq.east) -- (DeepSet.west);
\draw[->] (DeepSet.south) -- (Concatenation.north) node[midway, right] {Latent Output};
\draw[->] (Xs.east) -- (Concatenation.west);
\draw[->] (Concatenation.east) -- (DNN.west);
\draw[->] (DNN.east) -- (ys.west);
\end{tikzpicture}
\caption{Skeleton of the proposed model with Deep Sets.}
\label{fig:proposeModelDeepSets}
\end{figure}

The  architecture of the scoring function $s_{\theta}$ with deep sets is given in Figure~\ref{fig:proposeModelDeepSets}.
For the complete architecture of the Deep Set node please refer~\ref{fig:DeepSetArchitecture}.
We keep the node abstract for simplicity.\

First the data points in the query set $\textbf{X}_q$, $\textbf{y}_q$ are passed through the deep set to get the Latent Output.
Each $X$, $y$ are concatenated to get a single vector for input into the deep set.
We obtain a latent output from the deep set.
This latent output is then concatenated with the query data points.
The concatenated result is passed through a Deep Neural Network to finally get the desired relevance scores.
For example if there is a batch of queries given by $\{X_{q_1},  X_{q_2},  X_{q_3} ...\}$,  and the latent output is given by $O_l$ then the concatenation yields
$$
\{O_l:X_{q_1},  O_l:X_{q_2},  O_l:X_{q_3} ...\}
$$
where ":" represents concatenation of 2 vectors.

\subsubsection{Meta training}

Using the above given scoring function during the evaluation cycle is very straightforward.
We use the known data points as a support set and the target data points to evaluate as the query set.
During meta training,  however,  we have to divide the training data into support set and query set.

In our data division we first select 20 ($X$, $y$) data points as a support set randomly without replacement.
Thereafter,  we select the query points from the remaining choices (again without replacement). The number of query points depends on the batch size (100 by default).
During the meta training we do not sample the support and query points from all the given meta datasets.
At each step within an epoch we select one meta data to sample these points as discussed in Section~\ref{sec:rlmetatraining}.
The training of all the components in the model are carried out together (i.e. in the same back propagation step when using Pytorch).
For the purpose of analysing the training accurately,  we however sample data from all the training and validation task to report the training and validation loss respectively.

\subsection{Uncertainty implementation}\label{sec:UncertaintyImplementation}

We know from Section~\ref{sec:hpoConstraints} that the evaluation of the target objective function in HPO is noisy.
Hence it is important for any surrogate that models this objective to have the capability of estimating uncertainty.
The model proposed till now has no such capability.
Hence,  in this section we propose the extension of the model to incorporate uncertainty.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
roundnode/.style={circle, draw=brown!60, fill=black!5, very thick, minimum size=7mm},
roundnodey/.style={circle, draw=green!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]          (Xq)                                                     {$\textbf{X}_q$, $\textbf{y}_q$};
\node[roundnode]          (Xs)                        [below=of Xq]     {$\textbf{X}_s$};
\node[squarednode]      (DeepSet)              [right=of Xq]        {Deep Set};
\node[squarednode]      (Concatenation)             [below=of DeepSet]        {Concatenation};
\node[squarednode]      (DNN2)                      [right=of Concatenation]        {Basic Scorer$_2$};
\node[squarednode]      (DNN1)                      [above=of DNN2]                    {Basic Scorer$_1$};
\node[squarednode]      (DNN3)                      [below=of DNN]                    {Basic Scorer$_3$};
\node[roundnodey]        (y1)                             [right=of DNN1] {$\textbf{y}_1$};
\node[roundnodey]        (y2)                             [right=of DNN2] {$\textbf{y}_2$};
\node[roundnodey]        (y3)                             [right=of DNN3] {$\textbf{y}_3$};
\node[squarednode]      (mean)                             [right=of y2] {$\mu$, $\sigma^2$};
\node[roundnodey]        (mus)                             [right=of mean] {$\mu_s$};
\node[roundnodey]        (sigmas)                             [below=of mus] {$\sigma^2_s$};

%Lines
\draw[->] (Xq.east) -- (DeepSet.west);
\draw[->] (DeepSet.south) -- (Concatenation.north) node[midway] {Latent Output};
\draw[->] (Xs.east) -- (Concatenation.west);
\draw[->] (Concatenation.east) -- (DNN1.west);
\draw[->] (Concatenation.east) -- (DNN2.west);
\draw[->] (Concatenation.east) -- (DNN3.west);
\draw[->] (DNN1.east) -- (y1.west);
\draw[->] (DNN2.east) -- (y2.west);
\draw[->] (DNN3.east) -- (y3.west);
\draw[->] (y1.east) -- (mean.west);
\draw[->] (y2.east) -- (mean.west);
\draw[->] (y3.east) -- (mean.west);
\draw[->] (mean.east) -- (mus.west);
\draw[->] (mean.east) -- (sigmas.west);

\end{tikzpicture}
\caption{Full proposed scoring model with uncertainty}
\label{fig:proposeModelDeepSetsUncertainty}
\end{figure}

Figure~\ref{fig:proposeModelDeepSetsUncertainty} shows the architecture of the model with uncertainty.
The only addition to the model is the usage of multiple deep neural networks.
As discussed in Section~\ref{sec:uncertaintyDeepEnsembles},  we use multiple neural networks to calculate the mean and the variance instead of using a single neural network to do so.
Given the support set and a single query,  the scoring function modelled in Figure~\ref{fig:proposeModelDeepSetsUncertainty}  gives a mean and a variance assuming that the distribution of the possible relevance scores is a Gaussian.

We train all the neural networks together using the given meta data.
For this,  the loss function requires a definite value of the predicted output $y$.
We use the mean $\mu_s$ for this purpose.
Due to this combined training method the training is more efficient.
Hence for example if we are using ListMLE,  our loss function would be given by
\begin{equation}\label{eq:overflowissue}
L_{mle} = -  \sum\limits_{j=1}^{k} \log \frac{\exp(\mu(\pi^*_j))}{ \sum\limits_{t=j}^k \exp(\mu(\pi^*_k))}
\end{equation}

One problem that may come up during the training of these DNNs together is that the ranges of the DNNs may be different.
This is taken care by the fact that we use a range controller in our scoring function as shown in Figure~\ref{fig:basicScoringModel}.
Hence, the ranges of all the DNNs are the same. 
The variance within the ensemble is still present due to the random initialization of the neural networks.
By using the range controlling mechanism we both make the model numerically stable and make the training more efficient.

\section{Different training mechanisms.}
\subsubsection{Independent training}
Implementation yet to be done

\subsubsection{Training with mean and restricted output}
Implementation yet to be done


\chapter{Research Question}
The format of this is the same as that of experiments and Results.
Also known as Hypothesis.

\chapter{Experiments and Results}

In this chapter we present the experiments that we conducted and the results we obtained in order to answer the research questions already posed.
We first understand the structure of the (meta)data used for meta-training,  meta-validation,  and meta testing.
In the subsequent sections we present the results obtained in detail.

\subsubsection{Meta-Data}

For comparing our proposed model with other HPO models we would have to run the Bayesian optimization used in our case (with our surrogate and with other surrogates) on a set of different machine learning models.
Moreover,  the optimum within the HP search space may be different when the ML model is trained on different data sets.
Hence, we further need to do multiple HP optimizations of a model each time using a different dataset.
In addition to this, due to the stochastic nature of ML models as well as the surrogate models, 
we would have to run the HPO multiple times for each dataset.

As one can see,  this evaluation if done right from the training of the ML models is not feasible.
To overcome this challenge,  we use the HPO-B~\cite{DBLP:journals/corr/abs-2106-06257} benchmarking in this thesis.
Using this benchmark,  we do not need to train our ML models from scratch as the meta data contains evaluations of multiple HP configurations for different ML models and datasets.
In the rest of the section we discuss about the organisation of the benchmarking meta data.


HPO-B is a benchmark that can be used for doing black box HPO.
It can be used for both transfer models and non transfer models.
The meta data consists of a list of (hyper-parameter) search spaces.
These are hyper parameter search spaces of single models.
It is organised in a json format with the structure illustrated in Figure~\ref{fig:metadataorganization}.

\begin{figure}[htb]
\centering
\begin{tikzpicture}[scale=0.75, transform shape,
% roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm},
]
%Nodes
\node[squarednode]      (ss2)                             {Search Space 2};
\node[squarednode]      (main)                   [left=of ss2]           {MetaData};
\node[squarednode]      (ss1)                    [above=of ss2]          {Search Space 1};
\node[squarednode]      (ss_)                       [below=of ss2]        {...};
\node[squarednode]      (ds2)                       [right=of ss1]        {Data Set 2};
\node[squarednode]      (ds1)                       [above=of ds2]        {Data Set 1};
\node[squarednode]      (ds_)                       [below=of ds2]        {...};
\node[squarednode]      (Xy)                       [right=of ds2]        {\{\textbf{X},  \textbf{y}\},  \textrm{Seeds}};

%Lines
\draw[->] (main.east) -- (ss2.west);
\draw[->] (main.east) -- (ss1.west);
\draw[->] (main.east) -- (ss_.west);
\draw[->] (ss1.east) -- (ds2.west);
\draw[->] (ss1.east) -- (ds1.west);
\draw[->] (ss1.east) -- (ds_.west);
\draw[->] (ds2.east) -- (Xy.west);
\end{tikzpicture}
\caption{Structure of the meta data in the HPO-B benchmark}
\label{fig:metadataorganization}
\end{figure}
Where \textbf{X} represents the set of configurations for a evaluated for a model in a particular data set and
\textbf{y} represents the evaluation results.
The bayesian optimization used in our thesis can be started with different initial known HP configurations.
One initial configuration, called a seed in the meta data, is provided by a set of values (or indices) in $\textbf{X}$ and $\textbf{y}$.
There are a total of 5 seeds provided for a search space and dataset combination.

The meta data in HPO-B comes in 3 versions namely \textbf{HPO-B-v1},  \textbf{HPO-B-v2}, and \textbf{HPO-B-v3}.
Of this \textbf{HPO-B-v3} contains distlilled the search spaces that have the most datasets and can be split is split into train,  validation and test sets. 

The meta data set in HPO-B is divided into test and train data.
Using meta dataset,  one can learn meta learn a model using the training split.
Then the model is evaluated in the testing split.
This data splitting approach is used both in the baselines and proposed idea models.

There are 2 types of HP optimization surrogates that are studied in this thesis - transfer learning surrogates
and non-transfer learning surrogates.
This benchmark can be used for analysing both types of models.
However,  in order cross compare transfer and non-transfer techniques,  we only compare against \textbf{HPO-B-v3} test split as recommended in the HPO-B paper.

\iffalse
\subsection{HPO-B Dataset}
To use the HPO-B code,  one must write function observe\_and\_suggest and observe\_and\_suggest\_continous to deal with discrete and continous optimisation case respectively.
Due to out of scope nature of the continous case,  in our implementation we assume that we are dealing only with the discrete case only.

The implementation of observe\_and\_suggest functions differs based on the surrogate model used by our problem.
\fi

\section{Baseline Results}

To test our proposed model,  we used 4 main baseline methods - Random Search,  SMBO using Gaussian surrogates,  SMBO using Deep Ensembles and FSBO.
Library implementation of Random Search and GP surrogates were taken.
Whereas the Deep Ensemble and FSBO papers were implemented in this thesis.
The results of these baselines are discussed in this section.

In the given meta data if we take the set of all combinations in \textbf{HPO-B-v3} test split i.e \{Search Spaces $\times$ Metadata sets $\times$ Seeds\},  we get 403 HP optimizations.
In each optimization,  we start with 5 initial seeds given by HPO-B and run the evaluation cycle for 100 iterations.
This evaluation cycle is discussed in Algorithm~\ref{alg:deepEnsembleFinetuning} albeit for deep ensembles.

In the whole evaluation cycle,  an array of incumbent best (highest) HP evaluation based on the Metadata set is created.
The incumbent array for every optimization cycle for a model is compared against other models to create a rank array for each model.
For example,  if at step 23 the incumbent of model $a$ has a higher value than of model $b$,  then rank of model $a$ at step 23 is lower than model $b$ (When we consider lower rank to be better).
The rank arrays of every optimization cycle is averaged to get the rank graph of a particular model.
We present this \textbf{rank graph} as a primary form of comparison between the models in question.

For example Figure~\ref{fig:RsGpRankGraph} compares the rank graph of 2 baselines.

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/RsGpRankGraph}
    \caption{Rank Graph of RS and SMBO with GP surrogate}
    \label{fig:RsGpRankGraph}
\end{figure}

\subsubsection{Deep Ensembles}

The deep ensembles we used contained 5 neural networks by default.  Each neural network contained 2 fully connected neural layers with 32 neurons each. 
We considered the non-transfer case for deep ensembles first.
Hence we did not do any meta training for this model.
Each neural network was trained for a 1000 epochs with a learning rate of 0.02 at evaluation cycle.
We used Adam as our optimizer.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/DEPerformance}
    \caption{Graphs showing performance of Deep Ensembles}
    \label{fig:DEPerformance}
\end{figure}

Figure~\ref{fig:DEPerformance} shows the performance of various Deep Ensemble models.
We use the best results obtained from the previous baselines for comparing how Deep ensembles fair in the HPO-B benchmark.
In addition to plotting the rank graph,  we also plot the  average regret that is obtain if we use these models.
The average regret on y-axis is plotted in a log scale.

\textbf{Ablation Required for the raw part missing.}

In our study,  we studied the following main models of DE
\begin{itemize}
% \item Fine tuning deep ensembles with no restart.
\item DE using restart at every evaluation cycle.
\item Increasing the number of neural networks in DE to 10.
\end{itemize}

We observe that using deep ensembles does give us results which are comparable with GP.
Increasing the number of neural network ensembles does not seem to give very high performance improvement.
This is the case for both the ranking graph and the average regret.
Since this ranking would be cluttered for a large ablation study,  we also make use of critical rank graphs~\cite{pineda2021hpob}.
This is illustrated in Figure~\ref{fig:DERank100}.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DERank50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/DERank100}
    \caption{DE Ranking at various evaluation cycle steps}
   \label{fig:DERank100}
\end{figure}

After our analysis,  we a couple of advantages of the DE model.
First,  being a non transfer HPO model we found that it can be applied to any continuous HPO problem.
Second,  as each neural network is independent of the others,  the neural networks can be trained in parallel.
Hence we can scale up our ensemble based on the compute power available to us.

On the negative side,  since we do not do any meta training we are bound to (and have to) over fit to the very less data available during the optimization cycle.
In addition to this,  it does not model discrete or semi discrete spaces correctly. 
This is because during the learning,  the input is assumed to be continuous.


\subsubsection{Few Shot Bayesian Optimization (FSBO)}

The reason for the selection of FSBO as a baseline was that it was the state of the art in transfer HPO.
During our implementation of FSBO,  due to the computational resources required for the optimization cycle,  we tried to use this model with only single search spaces.
We found that using a model with 4 neural network layers with the same width of 32 as used in the Deep Ensemble is the best for the performance of the model.
We used early stopping for doing better training as mentioned before.
During the fine tuning we ran our model for 500 epochs with a learning rate of 0.03.
We used Adam optimiser for both training and fine tuning.
However,  we used cosine annealing on in the fine tuning phase.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.5]{images/FSBOPerformance}
    \caption{Rank graph of self implemented FSBO}
    \label{fig:FSBOPerformance}
\end{figure}

We can see in Figure~\ref{fig:FSBOPerformance},  our implementation could not give us the state of the art results.
Hence we used the FSBO results stored in the HPO-B benchmark instead for the comparison with the rest of the baselines.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/FSBOBestPerformance}
    \caption{Graphs showing best performance of FSBO}
    \label{fig:FSBOBestPerformance}
\end{figure}


\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/FSBOBestPerformanceRank25}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/FSBOBestPerformanceRank50}
\end{minipage}\par
\vskip\floatsep% normal separation between figures
\includegraphics[width=0.45\textwidth]{images/FSBOBestPerformanceRank100}
    \caption{Ranking of all baselines at various evaluation steps}
   \label{fig:FSBOBestPerformanceRank100}
\end{figure}

From Figure~\ref{fig:FSBOBestPerformance} it is evident that FSBO as a model vastly outperforms the other models both in the rank graph and the regret graph.
Moreover,  FSBO is always in a better rank at every evaluation step.
This can be seen in Figure~\ref{fig:FSBOBestPerformanceRank100}.
We use the DE with 10 neural network ensemble because it was giving best results for DE.
With the performance of the baselines set,  we now move to discuss in detail how our model compares to these methods.


\section{Ranking Loss model results}

In this section we evaluate our built model chronologically first using basic scorer model,  then making the model context aware and thereafter adding the capability of uncertainty to the model.

\subsubsection{ListMLE with Basic scoring model}
Training of the basic scoring model with the meta data obtained relatively smooth loss curves.
Figure~\ref{fig:GoodLossCurve4796} shows one such loss curve obtained for the search space ID 4796 in the HPO-B meta dataset.
However,  we did find search spaces that had bad loss curves.
Figure~\ref{fig:NegativeLearning5860} shows example of one such loss curves.
We can deduce in this figure that the validation task is different from the training task.
Since the testing/evaluation task may be similar to the training task,  we did not do any early stopping of the model (during training) based on the validation losses.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/GoodLossCurve4796}
\caption{Good training loss curve (search space ID 4796)}
    \label{fig:GoodLossCurve4796}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/NegativeLearning5860}
\caption{Bad training loss curve (search space ID 5896)}
    \label{fig:NegativeLearning5860}
\end{minipage}
\end{figure}

With the trained basic scoring model,  the optimization runs can be done without any fine tuning.
This is nothing but obtaining the relevance scores of the pending HP configurations using our basic scorer.
Then evaluating the true HP objective function in the order given by their respective relevance scores.
However,  a more robust mechanism is to fine tune the scoring model based on the known HP evaluations.
Fine tuning for every evaluation step is restarted as discussed in Section~\ref{sec:restart}.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.25]{images/RLEvaluationBasicScoring}
    \caption{Benchmarking basic scoring model trained using ListMLE}
    \label{fig:RLEvaluationBasicScoring}
\end{figure}

From Figure~\ref{fig:RLEvaluationBasicScoring} one can see that the hypothesis regarding fine tune is correct.
Fine tuning does help make the model get better results in the long run.
One very interesting result we see is that without any fine tuning,  the basic scoring model trained with listMLE is very good in the first 5 evaluation steps.
We this more closely in the critical graph in Figure~\ref{fig:RLEvaluationBasicScoringRank5}.

The reason for this is that the scorer model learns to rank the best HP configuration found in the meta data.
This corresponds to a person trying out the best configuration seen in his experience.
On average this works very well hence the first configurations were good for the non fine tuned model.
In the long run however,  there is more observed data that the model can rely on.
This is only done when the model is fine tuned.
This is the reason why the fine tuning scorer works better in the long run as compared to the non fine tuned model.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.35]{images/RLEvaluationBasicScoringRank5}
    \caption{Critical rank graph at evaluation step 5}
    \label{fig:RLEvaluationBasicScoringRank5}
\end{figure}

\subsubsection{Deep Sets: Context aware scoring model}

We found in our research that our model could be improved by making it context aware.
For doing this we used an architecture with deep sets described Section~\ref{sec:DeepSetWithModel}.
As previously discussed the latent result of the deep set can be used as a context for the scoring model.
In order to meta-train this architecture,  we needed to change the way we sample the data.
We used a support set of 20 data points i.e \{$X_s$, $y_s$\} pairs.
For each double sampling we sample first the task.
Then the support points are sampled without replacement.
Then we sample the query points $X_q$ based on the batch size and list size (By default 100 for both).

During the training however,  we found that the loss curves were very jumpy.

\begin{figure}[h]% [H] is so declass\'e!
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DeepSetLoss4796}
\caption{Loss curve for model with deep set (SSID: 4796)}
    \label{fig:DeepSetLoss4796}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{images/DeepSetLoss5527}
\caption{Loss curve for model with deep set (SSID: 5527)}
    \label{fig:DeepSetLoss5527}
\end{minipage}
\end{figure}

Figure~\ref{fig:DeepSetLoss4796} and Figure~\ref{fig:DeepSetLoss5527} shows this clearly.
We think that this is because of very high representation capacity which is a consequence of using the deep set architecture.

During meta tuning again we have to have a support set and query set for training.
We cannot fix the support data points because the evaluation data is very less.
For this reason,  we use 20\% of the points as support points and the rest as query points during the fine tuning process.

The results of this training and fine tuning our model with deep sets are shown in Figure~\ref{fig:RLDeepSetevaluation}.
In the figure "raw" means that there was no fine tuning performed and the trained model was used as is in the the evaluation/optimization cycle.
One advantage we see with this model is that whether fine tuning is done or not,  the first few evaluations are better than the state of the art results.
This can be seen in the Rank@5 graph critical graph given in Figure~\ref{fig:DeepSetRank5}.
This is because there is less relearning that happens in fine tuning due to addition of context to our model.
In the later evaluation steps, naturally,  using fine tuning makes gives better performance.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.35]{images/DeepSetRank5}
    \caption{Critical rank graph for Model with Deep Set.}
    \label{fig:DeepSetRank5}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/RLDeepSetevaluation}
    \caption{Benchmarking for the Deep set evaluation data}
    \label{fig:RLDeepSetevaluation}
\end{figure}

\subsubsection{Using weighted Loss Function}
We discussed that using a weighting loss function makes more sense for us as discussed in Section~\ref{sec:positionEnhancedRanking}.
The ablation of using inverse log weighting with and without fine tuning is shown in Figure~\ref{fig:RLDeepSetWeighted}.
Here we see that there is a big improvement to our evaluation results.
This proves the hypothesis that weighting is very essential for getting good results.

The fine tuned results is better than all the baselines except FSBO.
In fact,  it is better than FSBO in the first few evaluation steps.
This can be seen in Figure~\ref{fig:RLDeepSetWeightedRank25}.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/RLDeepSetWeighted}
    \caption{Ablation for weighted loss}
    \label{fig:RLDeepSetWeighted}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.35]{images/RLDeepSetWeightedRank25}
    \caption{Critical rank graph for weighting ablation.}
    \label{fig:RLDeepSetWeightedRank25}
\end{figure}


\subsubsection{Uncertainty}
Uncertainty estimation is very important for every surrogate model that is used in SMBO.
This is implemented using the ensemble method described in Section~\ref{sec:UncertaintyImplementation}.
We see from Figure~\ref{fig:RLUncertaintybenchmark} that there is massive improvement of performance in our model after uncertainty is implemented in it.
We can also see that it is on par with the state of the art FSBO results.

In the first steps of the optimization cycle,  using uncertainty is very important.  This is because we do not have enough data to build or fine tune our surrogate model.
Using a surrogate without uncertainty in the first steps may lead to an overconfident model that gives bad results.
This especially not good because the evaluation cycle is a sequential process.
The selection of the next point depends on the surrogate which has been fine-tuned with previously explored points.
This is what makes the model with uncertainty (if estimated correctly) superior to other models.
 
\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/RLUncertaintybenchmark}
    \caption{Ablation for uncertainty implementation}
    \label{fig:RLUncertaintybenchmark}
\end{figure}

One thing interesting is that the average regret of the best model we propose is not very good as compared to even the Deep Ensemble baseline.
Since the average regret gives a biased estimation of the results and the rank graph is a more accurate picture of the results,  we conclude here that the proposed ranking loss surrogate model is better than the Deep Ensemble baseline.

Finally, in Figure~\ref{fig:FinalAblation} we show how each of models built chronologically compare against each other.
In this figure we see that the 2 main factors that improved our model were the use of a weighted loss and the use of uncertainty.
One anomoly we see is that the addition of deep set itself seemed to give bad results.
Nevertheless, we accept it to be better than just using the basic scoring model.
Firstly because addition of the deep set improved the performance of the model in the first approximately 30 steps.
This is crucial for a sequential process.
Secondly,  because there is a possibility to reduce the probability of negative learning to occur because of the context encoded using  the deep sets.
Also one can see that one standard deviation of both the basic scoring model and the deep set model are overlapping. 
This signifies that their performance difference in the last ranks is not very huge.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.20]{images/FinalAblation}
    \caption{Ablation of all RL models}
    \label{fig:FinalAblation}
\end{figure}


\iffalse
Data used in the experiment
Infrastructure used
(That is the protocol)
Results should be structured like hypothesis.

experiment and results
protocol
data split

First show the resutls of implementation of GP (M = 5 and M =10 )
Benchmark this...

Next with DKT (Benchmark this)

Next show the best results obtained by architecture.
\fi

\section{Advantages and Limitations}

Compare this loss function and method with other base lines..
Advantages of the proposed Idea:
    The amount of data instances for training is exponential in number. Which is very good for a deep learning model
    For example if we have 100 observation set and we use a list size of 15 to train our model, we will have 100C15
    unique instances to train.

Observed disadvantages:
\begin{itemize}
\item In our model,  a scorer is first learnt and then using the scorer,  we rank the set of objects in question.
When optimising the scorer,  we ignore the sorting functionality necessary to complete the process of ranking.
This is because sorting is non differentiable. 
This means that the true evaluation of the ranked list is not optimized.
This needs to be improved which is done in Pi-Rank paper.
\item The learnt model is extremely sensitive to the learning rate and the number of epochs.
\item It is assumed that the target task and the training task have the same output range. 
They must be normalized in order to get the correct results if they are not in the same range.
\item Using $\tanh$ function restricts the output very much.
It may not have the latent space to completely the output.
\item Due to disadvantages  of exp increasing positive function - Perhaps using other increasing positive function helps? (More reading/research required on this topic
\item The search spaces that have less data need more uncertainty
The search spaces having more data need less uncertainty.
\end{itemize}
    
\subsubsection{Negative transfer learning}
For some search spaces,  the validation errors of the ranking loss model did not reduce at all during its training.
In fact the validation loss became worse.
Figure~\ref{fig:NegativeLearning} shows an illustration of this for the search space id 5527.
This is a classic example of negative transfer learning~\cite{Weiss2016}.
Negative transfer learning occurs when the set of source tasks (here,  training datasets) is very different from the target tasks (here, validation datasets).

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.5]{images/NegativeLearning}
    \caption{Training loss curve of ranking losses}
    \label{fig:NegativeLearning}
\end{figure}

This problem should occur only in the cases where the model is context free.
In our case,  the ranking loss model with deep set is context aware and results like these were unexpected.
Hence the issues with negative transfer learning remains unsolved when using our model.
One remedy for getting around this problem is to fine tune the model with a larger number of epochs.
Nevertheless,  we cannot guarantee that the fine tuning will re-learn from the small amount observed data points during the optimization cycle.

\section{Evaluation}
\subsection{Testing}
explain how a ranking graph works ar implemented
Explain the regret rank@ some location.

\subsection{Ablation}

Result tabulation of case study: sorting:
1.  Within range
2.   Outside range 
mean of 3 times should be written.

show the results of raw without deep set.

Next show different strategies used for building the ranking loss model one step at a time.
First with only scorer.
then with deep set.
Then with raw deep set
fine tuning and deep set
adding uncertainty

Checking the early stop and hypothesing why is was wrong.

box plot variation of each of the scorers... for 1 or more data sets?

show results of independent training and training with output restriction

what about training independently,   this requires normalization. 
as explained by sebastian.

\chapter{Conclusion}


\section{Further work}
Further study required with other baselines that deal with ranking loss.

In our methods we used the ensemble of DNNs which output a list of results.
We use this results to calculate the mean and the variance of our  prediction.
However,  the deep ensemble paper proposes a method to directly calculate the mean
and variance.
However,  the integration of this idea with ranking losses is non-trivial.
Hence the usage of this type of loss with the ranking loss function can be taken up in further research in order to check whether their is improvement in the results or not.

Working with continuous HP spaces.
How about dividing the space into areas and use 1 HP configuration as a representative
of the space (in the euclidean sense)
Then select the best region and subdivide the space and continue the process.
There are limitation,  cannot really guarantee the optima will be found like the gradient methods.
Hence this method is suboptimal to the gradient based HP methods for continuous search spaces.

\iffalse
Paper: https://arxiv.org/pdf/2012.06731.pdf
       (Impt Ref) https://arxiv.org/pdf/1903.08850.pdf

Key Idea:
    Use permutation matrices to represent sorting. Learn the matrix with the loss function.
    [0 1 0] [x]    [y]
    |1 0 0| |y|  = |x|
    [0 0 1] [z]    [z]
    Permutation Matrices are not smooth in the input space Hence relaxation is necessary for differentiability
    Relaxation is done by using a double schocastic matrices i.e all row and column matrices sum to 1.

    Unimodal
    Double schochastic

The idea of Permutation Matrices is taken from Neural sort - Neural sort (Impt ref)

PLACKETT-LUCE DISTRIBUTIONS -> Very important to explain the rationale about using score as a probability
    measure for our scoring function (Check section 2.1 of paper 2 (Neural Sort))
\fi 
 
\section{Conclusion}
This is the conclusion


\bibliography{references}
\bibliographystyle{plain}


\appendix
% \chapter*{Appendices}
\chapter{More information}
\pagenumbering{roman}
This thesis was completed in the representation learning lab of Albert-Ludwig-Universitt Freiburg.  (Figure~\ref{fig:UniLogo})

\begin{figure}[htb]
  \centering
    \includegraphics[scale=0.35]{images/logo}
    \caption{Logo: Albert-Ludwig-Universitt Freiburg}
    \label{fig:UniLogo}
\end{figure}

\end{document}

