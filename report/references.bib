@misc{github_repository,
  author = {Abdus Salam Khazi},
  howpublished = "\href{https://github.com/abduskhazi/ranking-loss-surrogates.git}{Code}",
  note = "[Online; accessed 06-Feb-2022]"
}

@misc{DeepEnsemblePaper,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{SMBOPaper,
      title={Sequential Model-Based Optimization for
General Algorithm Configuration},
      author={Frank Hutter, Holger H. Hoos and Kevin Leyton-Brown},
      howpublished = "\href{https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/11-LION5-SMAC.pdf}{Paper}"
}

@misc{GPTutorial,
  doi = {10.48550/ARXIV.2009.10862},
  url = {https://arxiv.org/abs/2009.10862},
  author = {Wang, Jie},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {An Intuitive Tutorial to Gaussian Processes Regression},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{MITMLBook,
      title={Machine Learning: A Probabilistic Perspective},
      author={K. P. Murphy},
      archivePrefix={The MIT Press},
      year = {2012}
}


@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{deeplearningarticle,
author = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
year = {2015},
month = {05},
pages = {436-44},
title = {Deep Learning},
volume = {521},
journal = {Nature},
doi = {10.1038/nature14539}
}

@incollection{Goan-2020,
	doi = {10.1007/978-3-030-42553-1_3},
  
	url = {https://doi.org/10.1007%2F978-3-030-42553-1_3},
  
	year = 2020,
	publisher = {Springer International Publishing},
  
	pages = {45--87},
  
	author = {Ethan Goan and Clinton Fookes},
  
	title = {Bayesian Neural Networks: An Introduction and Survey},
  
	booktitle = {Case Studies in Applied Bayesian Data Science}
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{DBLP:journals/corr/abs-2106-06257,
  author    = {Sebastian Pineda{-}Arango and
               Hadi S. Jomaa and
               Martin Wistuba and
               Josif Grabocka},
  title     = {{HPO-B:} {A} Large-Scale Reproducible Benchmark for Black-Box {HPO}
               based on OpenML},
  journal   = {CoRR},
  volume    = {abs/2106.06257},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.06257},
  eprinttype = {arXiv},
  eprint    = {2106.06257},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-06257.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pineda2021hpob,
  author    = {Sebastian Pineda{-}Arango and
               Hadi S. Jomaa and
               Martin Wistuba and
               Josif Grabocka},
  title     = {{HPO-B:} {A} Large-Scale Reproducible Benchmark for Black-Box {HPO}
               based on OpenML},
  journal   = {Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks},
  year      = {2021}
}

@article{rshpoarticle,
author = {Bergstra, James and Bengio, Yoshua},
title = {Random Search for Hyper-Parameter Optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
issn = {1532-4435},
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
journal = {J. Mach. Learn. Res.},
month = {feb},
pages = {281â€“305},
numpages = {25},
keywords = {global optimization, deep learning, response surface modeling, neural networks, model selection}
}
